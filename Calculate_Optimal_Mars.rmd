---
title: "Optimization Routines"
output:
  html_document:
    highlight: tango
    theme: spacelab
    toc: no
  pdf_document:
    toc: yes
---

# This is the Optimization Demonstration for MARS

## Approach
### Uncontrained Optimization

1. Use a combination of prediction and interpolation to predict all values oftime and accuracy under different hardware/software configurations.
2. Find the minimal combination of algorithm inputs that maximize accuracy. If there are ties, break them by using the point that requires the least data.
3. Find the costs associated with running the algorithm with those inputs on all different hardware configurations.
4. Find the combination of hardware that jointly minimizes cost and time.

### Data-contrained Optimization
1.  Use a combination of prediction and interpolation to predict all values oftime and accuracy under different hardware/software configurations.
2.  Subset the accuracy surface produced above to the amount of data available.  The maximizing point will fall in the upper right corner of the subsetted space.
3.  Find the costs associated with running the algorithms with the accuracy-maximizing point on all different hardwares.
4.  Find the combination of hardware that jointly minimizes time and cost.

### Cost-constrained Optimization
1.  Use a combination of prediction and interpolation to predict all values oftime and accuracy under different hardware/software configurations.
2.  Subset the space produced above to the amount of time and money able to be spent on modelling.  
3.  Working backwards now, find the accuracies that can be produced in the limited time.
4.  Using the subset of accuracy space, find the combination of algorithm inputs that maximizes accuracy.

```{r setup}
knitr::opts_chunk$set(cache=TRUE, echo=F, warning=F, error = F, message=F)
knitr::opts_knit$set(root.dir = "/users/scottsfarley/documents")
setwd("/users/scottsfarley/documents")
library(parallel)
library(doParallel)
library(akima)
library(ggplot2)
options(java.parameters = "-Xmx1500m")
library(bartMachine)
bartMachine::set_bart_machine_num_cores(3)
library(reshape2)
library(ggdendro)

threshold.time <- 20 ##seconds
threshold.cost <- Inf ##cents
threshold.numTex <- 45

numKeep = 100
dissimilarityCut = 0.5
```


First, get the training data and fit the model.  Perform some skill checks on it.
```{r modelFitting}
res <- read.csv("thesis-scripts/data/mars_full.csv")
predictors <- res[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
predictors <- data.frame(predictors)
response <- log(res[[c("totalTime")]]) ## take the log for prediction
MARS.rf <- bartMachine(predictors, response, serialize=T, verbose = T, run_in_sample = F)

predictors.acc <- res[c("numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
predictors.acc <- data.frame(predictors.acc)
response.acc <- res[[c("testingAUC")]] 

MARS.acc.rf <- bartMachine(predictors.acc, response.acc, serialize=T, verbose=T, run_in_sample = F)

```



Choose a finite number of possible solutions to the model.  Ideally, we would want every single combination of predictor variables [0, Inf].  This is obviously intractable.  Moreover, I only have data for a subset of that space anyways.  So randomly sample the subspace in which I have data to make the problem possible to solve.

```{r subset}
cores <- seq(1, 10)
GBMemory <- seq(1, 10)
trainingExamples <- seq(0, 10000, by=1000)
numPredictors <- seq(1, 5)
cells <- 100000

prices <- read.csv("/users/scottsfarley/documents/thesis-scripts/data/costs.csv")

## make the hypergrid
scenario <- expand.grid(numPredictors = numPredictors,
                        # cores = cores,
                        # GBMemory = GBMemory,
                        trainingExamples = trainingExamples,
                        cells = cells,
                        config = unique(prices$ConfigurationNumber))

for (value in unique(scenario$config)){
  rateRow = prices[which(prices$ConfigurationNumber == value), ]
  rate = rateRow$TotalRate
  cores = rateRow$CPUs
  mem = rateRow$GBsMem
  scenario$TotalRate[scenario$config == value] = rate
  scenario$cores[scenario$config == value] = cores
  scenario$GBMemory[scenario$config == value] = mem
}
```


Using that subset of data and the models we fit previously, predict each candidate configuration of algorithm inputs and hardware variables for execution time and SDM accuracy.
```{r}
scenario.acc <- scenario[c("numPredictors", "trainingExamples", "cells", 
                           "cores", "GBMemory")]
p.acc <- predict(MARS.acc.rf, scenario.acc)


scenario.time <- scenario[c("numPredictors", "trainingExamples", "cells", "cores", "GBMemory")]
## split here because my computer is stupid and runs out of heap space
p.time.1 <- predict(MARS.rf, scenario.time)
p.time <- c(p.time.1)
scenario$accuracy <- p.acc
scenario$seconds <- exp(p.time)
scenario$cost <- scenario$seconds * scenario$TotalRate
```

Plot the posterior means of the accuracy models against the algorithm inputs that should control accuracy. In this case, these are number of training examples and number of covariates.
```{r plotAccuracy}
i.acc <- interp(x = scenario$numPredictors, 
                y = scenario$trainingExamples, 
                z = scenario$accuracy, 
                xo = seq(1, 5),
                yo = seq(1, 10000),
                duplicate=T)

i.acc <- interp2xyz(i.acc, data.frame = T)


# ggplot(i.acc, aes(x, y, z = z)) +
#   geom_tile(aes(fill=z), height=1, width=1) +
#   stat_contour(binwidth=0.05, col='black', lwd=0.25) +
#   ggtitle("MARS Accuracy Surface") +
#   xlab("Number of Covariates") +
#   ylab("Number of Training Examples") +
#   scale_fill_continuous(low='pink', high='forestgreen')
```


The accuracy clearly varies from low (few training examples and few covariates) to very high (many covariates, many training examples).  Perhaps more data would be helpful here, but what are you going to do. Our task is to find the combinations of inputs that results in the highest accuracy model. If there's a tie, find the combination that needs the least data.    

Now, we know the combination of algorithm inputs that result in the highest accuracy.  The figure below shows the combination identified on the training examples and covariates axes.  This combination of training examples and number of covariates can be run on any combination of hardware.  Some might be suboptimal.  Thus, at this point, we've solved half of our challenge: algorithm inputs have been optimized, now it's time optimize hardware.


```{r checkHardwareEffect}
### select the combination that has the highest accuracy
## order by the things that control accuracy
## first by training examples, then by num predictors
## select the highest accuracy with the lowest valued predictors
sortedScenario <- i.acc[order(i.acc$y, i.acc$x),]
maxID <- which.max(sortedScenario$z)
theMax <- sortedScenario[maxID, ]
print(paste("Accuracy is maximized at", theMax$y, "training examples and", theMax$x, "predictors."))

theMax.trainingExamples <- theMax$y
theMax.numPredictors <- theMax$x
theMax.expectedAccuracy <- theMax$z
theMaxSet <- scenario[scenario$trainingExamples == theMax.trainingExamples &
                              scenario$numPredictors == theMax.numPredictors, ]


names(i.acc) <- c("numPredictors", "trainingExamples", "accuracy")
## plot the max onto the surface from before
ggplot(i.acc, aes(numPredictors, trainingExamples, z = accuracy)) +
  geom_tile(aes(fill=accuracy), height=1, width=0.5) +
  stat_contour(binwidth=0.01, col='black') +
  ggtitle("MARS Accuracy Surface") +
  xlab("Number of Covariates") +
  ylab("Number of Training Examples") +
  scale_fill_continuous(low='pink', high='forestgreen') +
  geom_point(data=theMaxSet, aes(x = numPredictors , y = trainingExamples), shape=25, fill='red', size=3)
```

In theory, the hardware parameters should not affect the SDM accuracy.  We can test this assumption here, by plotting the accuracies obtained for this combination of algorithm inputs against modeled accuracy on the number of CPUs and amount of memory.  If the assumption is valid, the plot should show no change in either the horizontal or vertical directions.  We see that there is, in fact, some change, though.  This is likely due to expeirmental design, and lack of a full factorial design setup.  The effect is realtively minor, and I choose to comment it and move along. 

```{r accAssumptionCheck}
# ggplot(theMaxSet) +
#   geom_tile(aes(x = cores, y=GBMemory, z=accuracy, fill=accuracy)) +
#   #geom_contour(aes(x = cores, y=GBMemory, z=accuracy, fill=accuracy)) +
#   ggtitle("MARS Hardware/Accuracy Surface") +
#   xlab("CPU Cores") +
#   ylab("RAM (GB)") +
#   scale_fill_continuous(low='blue', high='red')

acc.diff <- max(theMaxSet$accuracy) - min(theMaxSet$accuracy)
acc.diffFromExp <- mean(theMaxSet$accuracy) - theMax.expectedAccuracy
print(paste("Accuracy Range on Hardware: ", acc.diff))
print(paste("Accuracy Range from Expectation: ", acc.diffFromExp))

print("------")
print(paste("Fixing accuracy at: ", theMax.expectedAccuracy))
```

Now, fix the algorithm inputs at the accuracy-maximizing point-- effectively fixing expected model accuracy.  An algorithm with these inputs can be run on any combination of hardware. Project how long that specific model would take and how much it would cost on all computing types.  Plot those out on time vs. cost axes.

```{r timeCostProjection}
# ggplot(theMaxSet) +
#   geom_point(aes(x = cores, y = GBMemory))+
#   ggtitle("MARS Time/Cost Surface for Accuracy-Maximizing Point") +
#   xlab("Cost (cents)") +
#   ylab('Cost (seconds)') 
```

```{r }
# ggplot(theMaxSet) +
#   geom_point(aes(x = cost, y = seconds))+
#   ggtitle("MARS Time/Cost Surface for Accuracy-Maximizing Point") +
#   xlab("Cost (cents)") +
#   ylab('Cost (seconds)') 
```

The optimal solution is the one that balances time and cost equally during the minimization. We use euclidean distance here, which normalizes each dimension by its standard deviation, so they are weighted equally. For each candidate combiantion of hardware, we calculate the distance between it and the origin of these two axes.  We then find the minimum of that distance matrix and call that point the optimal.

```{r distCalc}
origin <- rep(0, length(theMaxSet))
pointSet <- theMaxSet[c("seconds", "cost")]
candidates <- rbind(pointSet, origin)
d <- as.matrix(dist(candidates))
fromOrigin <- d[,nrow(candidates)]
fromOrigin <- fromOrigin[fromOrigin > 0]
fromOrigin <- as.numeric(fromOrigin)
minDistIdx <- which.min(fromOrigin)
optimal <- theMaxSet[minDistIdx, ]

optimalDist <- min(fromOrigin)

xend = optimal$seconds
yend = optimal$cost
## Plot the same thing, but now with the optimal marked
# ggplot(theMaxSet) +
#   geom_point(aes(x = cost, y = seconds)) +
#   ggtitle("MARS Time/Cost Surface") +
#   xlab("Cost (cents)") +
#   ylab('Cost (seconds)')  +
#   geom_segment(data=theMaxSet, aes(x = 0, y=0, xend=cost, yend=seconds), alpha=0.075) +
#   geom_point(data=optimal, aes(x = cost, y = seconds), col='red', size=3, shape=25)

```

```{r}
fromOrigin <- data.frame(fromOrigin)

# 
# ggplot(fromOrigin) + geom_histogram(aes(x=fromOrigin), binwidth=1) +
#   ggtitle("Histogram of Distances from the Origin") +
#   xlab("Euclidean Distance from the Origin") +
#   ylab("Point-Count")
```

Our job is complete. We've now optimized both the harware and software dimensions of the problem. 

```{r printRes}
# print("------MARS OPTIMAL--------")
# print(paste("Predicted Optimal Accuracy", theMax.expectedAccuracy, "+/-", abs(acc.diffFromExp)))
# print(paste("Predicted Optimal Cost (seconds)", optimal$seconds))
# print(paste("Predicted Optimal Cost (cents)", optimal$cost))
# print(paste("Cores: ", optimal$cores))
# print(paste("Memory:", optimal$GBMemory))
# print(paste("Training Examples:", theMax.trainingExamples))
# print(paste("Covariates:", theMax.numPredictors))
```

Everything up to this point was done using the mean of the posterior distribution, a choice which simplifies the process but causes some information loss and may cause over-confidence in the predictions. We can modify our steps to include information from the entire posterior, which may solve this issue.  

```{r bayesSetup}
theMaxSet.time <- theMaxSet[c("numPredictors", "trainingExamples", "cells", "cores", "GBMemory")]
time.post <- bart_machine_get_posterior(MARS.rf, theMaxSet.time)$y_hat_posterior_samples
time.post <- data.frame(time.post)
time.post$cores <- theMaxSet$cores
time.post$GBMemory <- theMaxSet$GBMemory
time.post$config <- theMaxSet$config
time.post$TotalRate <- theMaxSet$TotalRate
```

Instead of projecting just the mean time and mean cost for use the the distance minimization, use the entire set of posterior samples.  Calculate the distance metric for each sample in the posterior independently.  You're then left with a density distribution of distances, from which we can infer the minimum value.

```{r}
library(plyr)
time.post$postMeanTime <- rowMeans(time.post[, 1:1000])
time.post$postMeanTime <- exp(time.post$postMeanTime)
time.post$postMeanCost <- time.post$postMeanTime * time.post$TotalRate

# ggplot(time.post) + 
#   geom_point(aes(x = postMeanCost, y = postMeanTime), shape=20, col='green', size=2) +
#   geom_point(data = theMaxSet, aes(x = cost, y = seconds), col='blue', size=2, shape=25)

```

```{r plot}
## get the data frame ready for plotting
c.d <- melt(time.post, id.vars = c("cores", "GBMemory", "config", "TotalRate", "postMeanTime", "postMeanCost"))
c.d <- na.omit(c.d)
c.d$value <- exp(c.d$value)
c.d$CostSample <- c.d$value * c.d$TotalRate
c.d$config <- as.factor(c.d$config)
ggplot(c.d) +
  geom_point(aes(x = CostSample, y = value, col=config), alpha=0.05) +
  scale_color_discrete(guide=F) +
  geom_point(data=time.post, aes(x = postMeanCost, y = postMeanTime), shape=20, size=2) +
  ggtitle("Time/Cost Posterior Estimates") +
  xlab("Cost") +
  ylab("Time")
```

The posteriors are in a line, since there's a fixed linear relationship between time and cost.

Now, find the distance metrics for all of those points.

```{r}
### okay, so we've got the posteriors for time and cost, now we can use them in calculating the optimal distnace

compDists <- list()
for (i in 1:nrow(time.post)){ ## all of the candidates in theMaxSet
  compDist <- vector()
  thisRate <- as.numeric(as.character(theMaxSet$TotalRate[i]))
  for (j in 1:1000){ ## 1000 posterior samples
    t.i <- exp(time.post[[j]][[i]]) ## psoterior of time for this config
    c.i <- t.i * thisRate ## make cost posterior from time samples times rate
    pt <- c(t.i, c.i)
    orig <- c(0, 0)
    m <- rbind(pt, orig)
    d <- as.matrix(dist(m))
    d <- d[d > 0]
    d <- as.numeric(d)
    compDist[j] <- d[[1]]
  }
  compDists[[i]] <- compDist
}

allDistances <- melt(compDists)

p <- na.omit(allDistances)
p$value <- as.numeric(p$value)
p$L1 <- as.factor(p$L1)
ggplot(p) + 
  geom_density(aes(x=value, group=L1, col=L1), alpha=0.025) +
  scale_fill_brewer('Dark2', guide=FALSE) + guides(colour=FALSE, L1=F) +
  xlab("Euclidiean Distance From Origin") +
  ggtitle("Euclidean Distance From Origin of Posterior") +
  geom_rug(aes(x= fromOrigin), data=fromOrigin, lwd=0.25, alpha=0.5) + ## this plots the mean
  xlim(0, max(fromOrigin$fromOrigin))
```

There's a lot of overlab in this figure, and many points are far away from the optimal.  We don't care about those.  Take the few closest to the minimum and look at their distributions. 

```{r minimalDistributions}
post.timeCost.sorted <- p[order(p$value),]
candidates <- unique(post.timeCost.sorted$L1)[1:numKeep ]

candidateMeans <- data.frame(fromOrigin$fromOrigin[candidates]) ## the means for each candidate distribution
names(candidateMeans) <- c("CandidateMeans")
candidateMeans$L1 <- candidates

## select their whole distributions
candidate.dist <- post.timeCost.sorted[post.timeCost.sorted$L1 %in% candidates, ]
# ggplot(candidate.dist) + 
#   geom_density(aes(x=value, group=L1, col=L1), alpha=0.2) +
#   scale_fill_brewer('Dark2', guide=FALSE) + guides(colour=FALSE, L1=F) +
#   xlab("Euclidiean Distance From Origin") +
#   ggtitle("Density of Euclidean Distance From Origin") +
#   # geom_rug(aes(x =value , group=L1, col=L1), alpha=0.025) +
#   geom_rug(data=candidateMeans, aes(x = CandidateMeans, col=L1)) +
#   xlim(0, 50) +
#   geom_vline(xintercept = optimalDist, col='black', lwd=0.5)

  

```

Now, the optimal configuration may be one of the following:

```{r minimalDistTable}
candidateConfigs <- theMaxSet[theMaxSet$config %in% candidates, ]
library(plyr)
distStats <- ddply(allDistances, .(L1), summarise, meanDist = mean(value), sdDist = sd(value))
candidateDists <- distStats[distStats$L1 %in% candidates, ]
candidateConfigs$distance.mean = candidateDists$meanDist
candidateConfigs$distance.sd = candidateDists$sdDist
candidateConfigs <- candidateConfigs[c("config", "cores", "GBMemory", "seconds", "cost", "distance.mean", "distance.sd")]
candidateConfigs <- candidateConfigs[order(candidateConfigs$distance.mean, candidateConfigs$distance.sd), ]
library(knitr)
#kable(candidateConfigs, row.names = F)
```

```{r}
clustPred <- candidateConfigs[c("seconds", "cost", "distance.mean", "distance.sd")]
d <- dist(scale(clustPred))
fit <- hclust(d, "ave")
fit$labels <-as.character(candidateConfigs$config)

dend <- as.dendrogram(fit)
dend_data <- dendro_data(dend, type = "rectangle")
groups <- cutree(fit, h=dissimilarityCut) # cut tree into 5 clusters

dend_data$labels$cluster <- vector('numeric', length=length(groups))

for (i in 1:nrow(dend_data$labels)){
  thisLabel <- as.character(dend_data$labels$label[i])
  thisCluster <- groups[thisLabel]
  dend_data$labels$cluster[i] <- thisCluster
}

ggplot(dend_data$segments) + 
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend))+
  geom_text(data = dend_data$labels, aes(x, y, label = label, col=as.factor(cluster)),
            hjust = 1, angle = 90, size = 3) +
  xlab("Hardware Configuration") +
  ylab("Dissimilarity") + theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank(),
  axis.ticks = element_blank()) +
  scale_color_discrete() +
  ggtitle("MARS Unconstrained Optimal Clustering")


candidateConfigs$cluster <- groups

# ggplot(candidateConfigs) + 
#   geom_point(aes(y = seconds, x = cost, col=as.factor(cluster), size=distance.sd), alpha=0.5) +
#   scale_color_discrete() +
#   ggtitle("MARS Data-Contrainted Optimal") +
#   ylab("Seconds") +
#   xlab("Cost")


clusterStats <- ddply(candidateConfigs, .(cluster), summarize, meanDistance = mean(distance.mean), sdDistance = mean(distance.sd))


## sort by mean distance and then by sd distance
clusterStats <- clusterStats[order(clusterStats$meanDistance, clusterStats$sdDistance ), ]

candidateConfigs <- join(candidateConfigs, clusterStats, by = "cluster")

bestCluster = clusterStats[1, ]$cluster

firstCluster <- candidateConfigs[candidateConfigs$cluster == bestCluster, ]
kable(firstCluster, row.names = F)
```

In the results above, you're accutally seeing the trade off between time and money play out quite nicely.  Adding cores costs money, but, in the case of MARSs, reduces time. Here, that tradeoff basically exactly evens out.

### Data Constraint

In this case, we've got a constraint on the amount of data available to us. 

```{r}

## interpolate accuracy, as before
i.acc <- interp(x = scenario$numPredictors, 
                y = scenario$trainingExamples, 
                z = scenario$accuracy, 
                xo = seq(1, 5),
                yo = seq(1, 10000),
                duplicate=T)

i.acc <- interp2xyz(i.acc, data.frame = T)


## subset accuracy interpolation to only include those with data satisfying the constraint
names(i.acc) <- c("numPredictors", "trainingExamples", "accuracy")
i.acc.subset <- i.acc[i.acc$trainingExamples < threshold.numTex, ]
# i.acc.subset.plot <- i.acc
# i.acc.subset.plot$accuracy[i.acc.subset.plot$trainingExamples > threshold.numTex] <- NA
#   
# ggplot(i.acc.subset.plot, aes(numPredictors, trainingExamples, z = accuracy)) +
#   geom_tile(aes(fill=accuracy), height=1, width=0.5) +
#   stat_contour(binwidth=0.05, col='black') +
#   ggtitle("MARS Accuracy Surface") +
#   xlab("Number of Covariates") +
#   ylab("Number of Training Examples") +
#   scale_fill_continuous(low='pink', high='forestgreen') 

print(paste("Current data threshold is ", threshold.numTex))
```

```{r}

## find, from the subsetted accuracies, the combinations of inputs that maximize accuracy
sortedScenario <- i.acc.subset[order(i.acc.subset$trainingExamples, i.acc.subset$numPredictors),]
maxID <- which.max(sortedScenario$accuracy)
theMax <- sortedScenario[maxID, ]
print(paste("Accuracy is maximized at", theMax$trainingExamples, "training examples and", theMax$numPredictors, "predictors."))
print(paste("Expected Max Accuracy is ", theMax$accuracy))
```

```{r}

subsetScenario <- prices[c("CPUs", "GBsMem")]
names(subsetScenario) <- c("cores", "GBMemory")
subsetScenario$trainingExamples <- theMax$trainingExamples
subsetScenario$numPredictors <- theMax$numPredictors
subsetScenario$cells <- cells

p <- predict(MARS.rf, subsetScenario)


scenarioOut <- subsetScenario
scenarioOut$seconds <- p
scenarioOut$TotalRate <- prices$TotalRate
scenarioOut$cost <- scenarioOut$seconds * scenarioOut$TotalRate
scenarioOut$config <- prices$ConfigurationNumber
```

```{r}
## this is different here, since we're working with interpolations
origin <- rep(0, length(scenarioOut))
pointSet <- scenarioOut[c("seconds", "cost")]
candidates <- rbind(pointSet, origin)
d <- as.matrix(dist(candidates))
fromOrigin <- d[,nrow(candidates)]
fromOrigin <- fromOrigin[fromOrigin > 0]
fromOrigin <- as.numeric(fromOrigin)
minDistIdx <- which.min(fromOrigin)
optimal <- scenarioOut[minDistIdx, ]

optimalDist <- min(fromOrigin)

xend = optimal$seconds
yend = optimal$cost

print(paste("Now there are only: ", nrow(scenarioOut), "candidates, instead of ", nrow(prices), "candidates that can complete this scenario under budget."))

# ## Plot the same thing, but now with the optimal marked
# ggplot(scenarioOut) +
#   geom_point(aes(x = cost, y = seconds)) +
#   ggtitle("MARS Time/Cost Surface") +
#   xlab("Cost (cents)") +
#   ylab('Cost (seconds)')  +
#   geom_segment(aes(x = 0, y=0, xend=cost, yend=seconds), alpha=0.075) +
#   geom_point(data=optimal, aes(x = cost, y = seconds), col='red', size=3, shape=25)

# print(paste("Recommended # cores: ", optimal$cores))
# print(paste("Recommended Memory: ", optimal$GBMemory))
# print(paste("Expected Cost: ", optimal$cost))
# print(paste("Expected Seconds: ", optimal$seconds))
```


```{r}
subsetScenario <- prices[c("CPUs", "GBsMem")]
names(subsetScenario) <- c("cores", "GBMemory")
subsetScenario$trainingExamples <- theMax$trainingExamples
subsetScenario$numPredictors <- theMax$numPredictors
subsetScenario$cells <- cells

p <- predict(MARS.rf, subsetScenario)


scenarioOut <- subsetScenario
scenarioOut$seconds <- exp(p)
scenarioOut$TotalRate <- prices$TotalRate
scenarioOut$cost <- scenarioOut$seconds * scenarioOut$TotalRate
scenarioOut$config <- prices$ConfigurationNumber

```


Finally, come back around, and find the computing hardware that's best for these inputs.
```{r}
## now, we've got a set of computing types that are capable of achieving the accuracy maximizing point under budget
## find the point that minimizes time and cost
## just like before
origin <- rep(0, length(scenarioOut))
pointSet <- scenarioOut[c("seconds", "cost")]
candidates <- rbind(pointSet, origin)
d <- as.matrix(dist(candidates))
fromOrigin <- d[,nrow(candidates)]
fromOrigin <- fromOrigin[fromOrigin > 0]
fromOrigin <- as.numeric(fromOrigin)
minDistIdx <- which.min(fromOrigin)
optimal <- scenarioOut[minDistIdx, ]

optimalDist <- min(fromOrigin)
fromOrigin <- data.frame(fromOrigin)

xend = optimal$seconds
yend = optimal$cost

print(paste("Now there are only: ", nrow(scenarioOut), "candidates, instead of ", nrow(prices), "candidates that can complete this scenario under budget."))

# ## Plot the same thing, but now with the optimal marked
# ggplot(scenarioOut) +
#   geom_point(aes(x = cost, y = seconds)) +
#   ggtitle("MARS Time/Cost Surface") +
#   xlab("Cost (cents)") +
#   ylab('Cost (seconds)')  +
#   geom_segment(aes(x = 0, y=0, xend=cost, yend=seconds), alpha=0.075) +
#   geom_point(data=optimal, aes(x = cost, y = seconds), col='red', size=3, shape=25)

print(paste("Recommended # cores: ", optimal$cores))
print(paste("Recommended Memory: ", optimal$GBMemory))
print(paste("Expected Cost: ", optimal$cost))
print(paste("Expected Seconds: ", optimal$seconds))
```

```{r}
time.post <- bart_machine_get_posterior(MARS.rf, subsetScenario)$y_hat_posterior_samples
time.post <- data.frame(time.post)
time.post$cores <- scenarioOut$cores
time.post$GBMemory <- scenarioOut$GBMemory
time.post$config <- scenarioOut$config
time.post$TotalRate <- scenarioOut$TotalRate

time.post$postMeanTime <- rowMeans(time.post[, 1:1000])
time.post$postMeanTime <- exp(time.post$postMeanTime)
time.post$postMeanCost <- time.post$postMeanTime * time.post$TotalRate

# ggplot(time.post) + 
#   geom_point(aes(x = postMeanCost, y = postMeanTime), shape=20, col='green', size=2) +
#   geom_point(data = scenarioOut, aes(x = cost, y = seconds), col='blue', size=2, shape=25)


compDists <- list()
for (i in 1:nrow(time.post)){ ## all of the candidates in theMaxSet
  compDist <- vector()
  thisRate <- as.numeric(as.character(scenarioOut$TotalRate[i]))
  for (j in 1:1000){ ## 1000 posterior samples
    t.i <- exp(time.post[[j]][[i]]) ## psoterior of time for this config
    c.i <- t.i * thisRate ## make cost posterior from time samples times rate
    pt <- c(t.i, c.i)
    orig <- c(0, 0)
    m <- rbind(pt, orig)
    d <- as.matrix(dist(m))
    d <- d[d > 0]
    d <- as.numeric(d)
    compDist[j] <- d[[1]]
  }
  compDists[[i]] <- compDist
}

allDistances <- melt(compDists)

p <- na.omit(allDistances)
p$value <- as.numeric(p$value)
p$L1 <- as.factor(p$L1)
# ggplot(p) + 
#   geom_density(aes(x=value, group=L1, col=L1), alpha=0.025) +
#   scale_fill_brewer('Dark2', guide=FALSE) + guides(colour=FALSE, L1=F) +
#   xlab("Euclidiean Distance From Origin") +
#   ggtitle("Euclidean Distance From Origin of Posterior") +
#   geom_rug(aes(x= fromOrigin), data=fromOrigin, lwd=0.25, alpha=0.5)  ## this plots the mean

post.timeCost.sorted <- p[order(p$value),]
candidates <- unique(post.timeCost.sorted$L1)[1:numKeep]

candidateMeans <- data.frame(fromOrigin$fromOrigin[candidates]) ## the means for each candidate distribution
names(candidateMeans) <- c("CandidateMeans")
candidateMeans$L1 <- candidates

## select their whole distributions
candidate.dist <- post.timeCost.sorted[post.timeCost.sorted$L1 %in% candidates, ]
ggplot(candidate.dist) + 
  geom_density(aes(x=value, group=L1, col=L1), alpha=0.2) +
  scale_fill_brewer('Dark2', guide=FALSE) + guides(colour=FALSE, L1=F) +
  xlab("Euclidiean Distance From Origin") +
  ggtitle("Density of Euclidean Distance From Origin") +
  # geom_rug(aes(x =value , group=L1, col=L1), alpha=0.025) +
  geom_rug(data=candidateMeans, aes(x = CandidateMeans, col=L1)) +
  xlim(0, 50) +
  geom_vline(xintercept = optimalDist, col='black', lwd=0.5)

```

```{r }
candidateConfigs <- scenarioOut[scenarioOut$config %in% candidates, ]
library(plyr)
distStats <- ddply(allDistances, .(L1), summarise, meanDist = mean(value), sdDist = sd(value))
candidateDists <- distStats[distStats$L1 %in% candidates, ]
candidateConfigs$distance.mean = candidateDists$meanDist
candidateConfigs$distance.sd = candidateDists$sdDist
candidateConfigs <- candidateConfigs[c("config", "cores", "GBMemory", "seconds", "cost", "distance.mean", "distance.sd")]
candidateConfigs <- candidateConfigs[order(candidateConfigs$distance.mean, candidateConfigs$distance.sd), ]
library(knitr)
#kable(candidateConfigs, row.names = F)
```

```{r}
clustPred <- candidateConfigs[c("seconds", "cost", "distance.mean", "distance.sd")]
d <- dist(scale(clustPred))
fit <- hclust(d, "ave")
fit$labels <-as.character(candidateConfigs$config)

dend <- as.dendrogram(fit)
dend_data <- dendro_data(dend, type = "rectangle")
groups <- cutree(fit, h=dissimilarityCut) # cut tree into 5 clusters

dend_data$labels$cluster <- vector('numeric', length=length(groups))

for (i in 1:nrow(dend_data$labels)){
  thisLabel <- as.character(dend_data$labels$label[i])
  thisCluster <- groups[thisLabel]
  dend_data$labels$cluster[i] <- thisCluster
}

ggplot(dend_data$segments) + 
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend))+
  geom_text(data = dend_data$labels, aes(x, y, label = label, col=as.factor(cluster)),
            hjust = 1, angle = 90, size = 3) +
  xlab("Hardware Configuration") +
  ylab("Dissimilarity") + theme(
  axis.text.x = element_blank(),
  axis.ticks = element_blank()) +
  scale_color_discrete() +   ggtitle("MARS Data-Constrained Optimal Clustering")


candidateConfigs$cluster <- groups

# ggplot(candidateConfigs) + 
#   geom_point(aes(y = seconds, x = cost, col=as.factor(cluster), size=distance.sd), alpha=0.5) +
#   scale_color_discrete() +
#   ggtitle("MARS Data-Contrainted Optimal") +
#   ylab("Seconds") +
#   xlab("Cost")

clusterStats <- ddply(candidateConfigs, .(cluster), summarize, meanDistance = mean(distance.mean), sdDistance = mean(distance.sd))


## sort by mean distance and then by sd distance
clusterStats <- clusterStats[order(clusterStats$meanDistance, clusterStats$sdDistance ), ]

candidateConfigs <- join(candidateConfigs, clusterStats, by = "cluster")

bestCluster = clusterStats[1, ]$cluster

firstCluster <- candidateConfigs[candidateConfigs$cluster == bestCluster, ]
kable(firstCluster, row.names = F)

```


### Cost Constraint II

```{r}

# accMaxResults <- data.frame(cost = vector(),
#                             seconds <- vector(),
#                             accuracy = vector(),
#                             cores = vector(),
#                             GBMemory = vector()
#                             )

accMaxResults <- list()
subsetScenario <- scenario
subsetScenario$grp <- interaction(subsetScenario$cores, subsetScenario$GBMemory)
idx <- 1
for (grpName in unique(subsetScenario$grp)){
  thisComp <- subsetScenario[subsetScenario$grp == grpName, ]
  thisCores <- mean(thisComp$cores)
  thisMem <- mean(thisComp$GBMemory)
  time.interp <- interp(x = thisComp$numPredictors, 
                        y = thisComp$trainingExamples,
                        xo = seq(1, 5),
                        yo = seq(1, 10000),
                        z = thisComp$seconds)
  time.interp <- interp2xyz(time.interp, data.frame=T)
  names(time.interp) <- c("numPredictors", "trainingExamples", "seconds")
  cost.interp <- interp(x = thisComp$numPredictors, 
                        y = thisComp$trainingExamples,
                        xo = seq(1, 5),
                        yo = seq(1, 10000),
                        z = thisComp$cost)
  cost.interp <- interp2xyz(cost.interp, data.frame=T)
  names(cost.interp) <- c("numPredictors", "trainingExamples", "cost")
    acc.interp <- interp(x = thisComp$numPredictors,
                       y = thisComp$trainingExamples,
                        xo = seq(1, 5),
                        yo = seq(1, 10000),
                       z = thisComp$accuracy)
  acc.interp <- interp2xyz(acc.interp, data.frame=T)
  names(acc.interp) <- c("numPredictors", "trainingExamples", "accuracy")
  time.interp$cost <- cost.interp$cost
  time.interp$accuracy <- acc.interp$accuracy
  
  ##subset on thresholds 
  time.interp <- time.interp[time.interp$cost < threshold.cost & time.interp$seconds < threshold.time, ]
  
  time.interp <- time.interp[order(time.interp$trainingExamples, time.interp$numPredictors), ]
  if(nrow(time.interp) == 0){
    warning(paste("No combiations can be computed on a configuration with ", thisCores, "cores and ", thisMem, "RAM"))
  }else{
      accMax <- time.interp[which.max(time.interp$accuracy), ] 
    accMax$cores <- thisCores
    accMax$GBMemory <- thisMem
    
    accMaxResults[[idx]] <- accMax
    idx <- idx + 1
  }
}

accMaxSet <- do.call(rbind.data.frame, accMaxResults)



write.csv(accMaxSet, "/users/scottsfarley/documents/thesis-scripts/data/mars_constrained_accMax.csv")
```

```{r}
accMaxSet <- read.csv("/users/scottsfarley/documents/thesis-scripts/data/gam_constrained_accMax.csv")
accMaxSet$seconds <- accMaxSet$seconds
accMaxSet$cells <- cells
time.post <- bart_machine_get_posterior(MARS.rf, accMaxSet[c("numPredictors", "trainingExamples", "cores", "GBMemory", "cells")])
```

```{r}

accMaxSet <- accMaxSet[order(accMaxSet$accuracy, accMaxSet$cost, accMaxSet$seconds), ]
print(paste("Best Configuration: ", accMaxSet[1, ]))
```

```{r}
time.post <- data.frame(time.post$y_hat_posterior_samples)
time.post$cores <- accMaxSet$cores
time.post$GBMemory <- accMaxSet$GBMemory
time.post$config <- accMaxSet$config

time.post$postMeanTime <- rowMeans(time.post[, 1:1000])
time.post$postMeanTime <- exp(time.post$postMeanTime)
time.post$postMeanCost <- time.post$postMeanTime * accMaxSet$TotalRate

# ggplot(time.post) +
#   geom_point(aes(x = postMeanCost, y = postMeanTime), shape=20, col='green', size=2) +
#   geom_point(data = accMaxSet, aes(x = cost, y = seconds), col='blue', size=2, shape=25)


compDists <- list()
for (i in 1:nrow(time.post)){ ## all of the candidates in theMaxSet
  compDist <- vector()
  thisRate <- as.numeric(as.character(accMaxSet$TotalRate[i]))
  for (j in 1:1000){ ## 1000 posterior samples
    t.i <- time.post[[j]][[i]] ## psoterior of time for this config
    c.i <- t.i * thisRate ## make cost posterior from time samples times rate
    pt <- c(t.i, c.i)
    orig <- c(0, 0)
    m <- rbind(pt, orig)
    d <- as.matrix(dist(m))
    d <- d[d > 0]
    d <- as.numeric(d)
    compDist[j] <- d[[1]]
  }
  compDists[[i]] <- compDist
}

allDistances <- melt(compDists)

fromOrigin = ddply(allDistances, .(L1), summarise, meanFromOrigin = mean(value))

p <- na.omit(allDistances)
p$value <- as.numeric(p$value)
p$L1 <- as.factor(p$L1)
ggplot(p) +
  geom_density(aes(x=value, group=L1, col=L1), alpha=0.025) +
  scale_fill_brewer('Dark2', guide=FALSE) + guides(colour=FALSE, L1=F) +
  xlab("Euclidiean Distance From Origin") +
  ggtitle("Euclidean Distance From Origin of Posterior") +
  geom_rug(aes(x= meanFromOrigin), data=fromOrigin, lwd=0.25, alpha=0.5)  ## this plots the mean

post.timeCost.sorted <- p[order(p$value),]
candidates <- unique(post.timeCost.sorted$L1)[1:numKeep]

candidateMeans <- data.frame(fromOrigin$meanFromOrigin[candidates]) ## the means for each candidate distribution
names(candidateMeans) <- c("CandidateMeans")
candidateMeans$L1 <- candidates

## select their whole distributions
candidate.dist <- post.timeCost.sorted[post.timeCost.sorted$L1 %in% candidates, ]
# ggplot(candidate.dist) +
#   geom_density(aes(x=value, group=L1, col=L1), alpha=0.2) +
#   scale_fill_brewer('Dark2', guide=FALSE) + guides(colour=FALSE, L1=F) +
#   xlab("Euclidiean Distance From Origin") +
#   ggtitle("Density of Euclidean Distance From Origin") +
#   # geom_rug(aes(x =value , group=L1, col=L1), alpha=0.025) +
#   geom_rug(data=candidateMeans, aes(x = CandidateMeans, col=L1)) 

```

```{r }
candidateConfigs <- accMaxSet[accMaxSet$config %in% candidates, ]
library(plyr)
distStats <- ddply(allDistances, .(L1), summarise, meanDist = mean(value), sdDist = sd(value))
candidateDists <- distStats[distStats$L1 %in% candidates, ]
candidateConfigs$distance.mean = candidateDists$meanDist
candidateConfigs$distance.sd = candidateDists$sdDist
candidateConfigs <- candidateConfigs[c("config", "cores", "GBMemory", "seconds", "cost", "distance.mean", "distance.sd")]
candidateConfigs <- candidateConfigs[order(candidateConfigs$distance.mean, candidateConfigs$distance.sd), ]
library(knitr)
#kable(candidateConfigs, row.names = F)
```

```{r}
clustPred <- candidateConfigs[c("seconds", "cost", "distance.mean", "distance.sd")]
d <- dist(scale(clustPred))
fit <- hclust(d, "ave")
fit$labels <-as.character(candidateConfigs$config)

dend <- as.dendrogram(fit)
dend_data <- dendro_data(dend, type = "rectangle")
groups <- cutree(fit, h=dissimilarityCut) # cut tree into 5 clusters

dend_data$labels$cluster <- vector('numeric', length=length(groups))

for (i in 1:nrow(dend_data$labels)){
  thisLabel <- as.character(dend_data$labels$label[i])
  thisCluster <- groups[thisLabel]
  dend_data$labels$cluster[i] <- thisCluster
}

ggplot(dend_data$segments) + 
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend))+
  geom_text(data = dend_data$labels, aes(x, y, label = label, col=as.factor(cluster)),
            hjust = 1, angle = 90, size = 3) +
  xlab("Hardware Configuration") +
  ylab("Dissimilarity") + theme(
  axis.text.x = element_blank(),
  axis.ticks = element_blank()) +
  scale_color_discrete() +   ggtitle("MARS Const-Constrained Optimal Clustering")


candidateConfigs$cluster <- groups

# ggplot(candidateConfigs) + 
#   geom_point(aes(y = seconds, x = cost, col=as.factor(cluster), size=distance.sd), alpha=0.5) +
#   scale_color_discrete() +
#   ggtitle("RF Data-Contrainted Optimal") +
#   ylab("Seconds") +
#   xlab("Cost")

clusterStats <- ddply(candidateConfigs, .(cluster), summarize, meanDistance = mean(distance.mean), sdDistance = mean(distance.sd))


## sort by mean distance and then by sd distance
clusterStats <- clusterStats[order(clusterStats$meanDistance, clusterStats$sdDistance ), ]

candidateConfigs <- join(candidateConfigs, clusterStats, by = "cluster")

bestCluster = clusterStats[1, ]$cluster

firstCluster <- candidateConfigs[candidateConfigs$cluster == bestCluster, ]
kable(firstCluster, row.names = F)
```



