Many contemporary Big Data applications, such as the popular microblogging service Twitter (http://twitter.com), require distributed, parallel, streaming methods to identify key analytical trends in real time (e.g., Bifet, Holmes, Pfahringer, & Gavalda, 2011).

With the recent growth in monitoring and occurrence data, some ecological analysts apply similar, data-driven machine-learning techniques to ecological forecasting problems and have seen significant increases in predictive skill (Elith et al., 2006).

Moreover, common statistical forecasting methods can be computationally demanding, particularly on larger datasets (Huang et al., 2013). Climate-driven ecological forecasting models, variously called ecological niche models (Peterson, 2003), predictive habitat distribution models (Guisan & Zimmerman, 2000), and species distribution models (SDMs)(Guisan & Thuiller, 2005b), have seen extensive application in ecology, global change biology, evolutionary biogeography (Araújo, Whittaker, Ladle, & Erhard, 2005; Wilfried Thuiller et al., 2008a), reserve selection (Guisan et al., 2013), and invasive species management (Ficetola, Thuiller, & Miaud, 2007). SDMs characterize a species’ response to biospatial environmental gradients (Franklin, 2010), and use these responses to forecast potential future distributions under climate change scenarios. Recently, studies with an large number of occurrence records, along with computationally intensive modeling approaches, including nonparametric data-driven techniques (Elith et al., 2006) and Bayesian methods that rely on repeated sampling of full joint probability distributions (Clark et al., 2014; Dawson et al., 2016), have become commonplace.

Paleoenvironmental proxy data, including fossil pollen, macrofossils, and freshwater and marine diatoms, can be an important supplement to modern biodiversity data and are an important part of these emerging efforts.

Cloud computing may offer a technological solution to some of these problems (Hampton et al., 2013; Michener & Jones, 2012). Cloud computing is a architectural design pattern that provides “ubiquitous, convenient, and on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort” (Hassan, 2011; Mell & Grance, 2012; Vaquero et al., 2008). With the rapid commercialization of cloud computing and the widespread availability of public cloud providers like Amazon Web Services (AWS) and the Google Cloud Compute Engine (GCE), scientists have a seemingly unlimited supply of computing resources at their disposal. The cloud has been touted by many of the largest players in Silicon Valley, and is credited with Obama’s 2012 presidential election win, Netflix’s ability to provide streaming entertainment to millions of consumers, and Amazon’s massive success in online retailing (Mosco, 2014). In 2010, the U.S. Federal Government embraced the efficiency, speed, and economy of the cloud, requiring all federal agencies to adopt a “Cloud-First” policy when considering new information technology (IT) developments (Kundra, 2010). Accordingly, the National Aeronautics and Space Administration (NASA) and the National Science Foundation (NSF) have both officially endorsed cloud technology (Mosco, 2014). Researchers in many disciplines have posited that the cloud as the key to solving future computing and modeling challenges (Hsu et al., 2013; Yang et al., 2011).

Research questions
My research questions aim to develop a theoretically sound and empirically grounded understanding of the drivers of SDM execution time, and put this understanding towards predicting the computing solution with the lowest cost to the researcher. In particular, my research questions are:
1.	With what skill can the runtime of climate-based SDMs be predicted?
2.	Can an optimal computing solution for a given modeling scenario be predicted with confidence that this prediction is better than a null model that suggests no change in execution time between SDM scenarios.
3.	What are the drivers of SDM execution time, and how sensitive is the runtime to changes in these drivers?


The second characteristic of Big Data in the four V’s framework is the Variety of the data, and its ‘various types with complicated relationships’ (Yang & Huang, 2013).


I set up a distributed computing system to complete my experiments, featuring one centralized database node and multiple distributed computing nodes. Fault tolerance was important, because I utilized Google’s less expensive ‘preemptible’ resources, which function like normal instances, but can be shutdown at any time if other customers require additional computational power. One Master Node hosts a MySQL database and a control script (written in python), and a pool of computing nodes that are fault tolerant and designed only for computing are provisioned and decommissioned as needed. The compute nodes, which run a Debian Linux operating system, are given only enough information to complete a given SDM, and the Master Node control script manages the progress of the project as a whole. A node.js script provides programmatic access to real-time database content and is used as the linkage between the master node and the worker nodes.
An outline of the system is described below and are illustrated in Figure [X].
	First, a pool of computing nodes is assembled. The Master Node control script queries the central database for experiments that have yet to be completed or threw an error the last time they were run. The database responds, via the node API, with a JSON object that contains the hardware parameters required by the next experiment. The python script parses the response and uses the ‘gcloud’ tools associated with the GCE to create a pool of virtual instances that have the necessary amount of memory and number of CPUs.
	Each node in the pool automatically begins running a startup script that begins the modeling process. First, a number of system-wide software packages, including R and Git are installed on the new instance. Git is used to clone the most recent version of the project repository (hosted as a private repository on GitHub) which contains all files necessary to compute an SDM. Once all packages have been installed, the timing script is initialized as a new R session. The R script queries the central database, identifying itself as a computing node with x cores and y memory. The database responds with the parameters needed to run a single experiment on the given infrastructure. The script then loads the necessary variables and runs the SDM. When finished, it reports its results to the database and marks the experiment as completed. Experiments are continued until there are no more experiments that can be computed on an instance of x cores and y memory. If an instance is preempted by the system or otherwise crashes, a shutdown script will be executed, marking the in-progress experiment as interrupted – signaling other worker nodes that it should be attempted again later.
	Because Google charges by the minute for the use of their virtual machines, instances must be torn down as soon as possible. As the computing nodes execute the experiments, the Master Node repeatedly polls the central database to determine the current position within the experiment table, determining the percentage completion of the current group of experiments. Once the group is complete, Master Node will use the gcloud tools to decomission the individual instances, and delete the instance pool and the instance template that was used to create each virtual worker node. After this, the Master Node is the only instance that remains online. At this point, Master Node returns to Step 1 to build a new pool of instances for a new hardware configuration.


Due to budgetary constraints, and thus inability to cover all possible parameterizations, experiments were divided into several categories in which different variables were systematically altered to determine sensitivity. This compromise aims to capture as much within-parameter variance as possible while simultaneously capturing the influence of interactions between variables. One basic series of experiments was run for all SDMs using a small subset of algorithm inputs (training examples and memory) on a wide variety of VM types (core/memory combinations). Five additional, separate analyses were completed to determine the sensitivity to specific parameterizations. Because execution time can vary non-linearly when the hardware parameters are changed, I tested as many combinations of memory and CPUs as possible. On each computer, a standard set of 160 experiments were run for each sequential SDM (MARS, GBM-BRT, GAM), including four spatial extents, four training example sets, and 10 replicates of each. All experiments were done on the Picea data set.
Individual, target experiment sets were done to assess the contribution and sensitivity of individual or sets of parameterizations. While no theoretical difference would suggest that execution times should vary between different taxa, a set of 191 model runs were done to evaluate whether differences exist in practice. The model uses aspatial input sets, which suggests that geographic range, abundance, or taxon-specific patterns should not bias the results of the experiments. Using all four taxa on six different VM instance types, inter-taxonomic sensitivity was recorded. The number of training examples and spatial resolutions was held constant for these runs.
Empirical Performance Models theory suggests that specific algorithm parameterizations will take longer to execute than others (Cannon & John, 2007). SDMs have a large number of potential parameters with which to alter, though many ecologists use the defaults, or packages that make it difficult to change the default parameter values (e.g., dismo (???)). To assess the magnitude of changes in execution time due to different parameterizations, the GBM-BRT was tested on a set of 70 different combinations of tree complexity and learning rate. The learning rate parameter of the GBM-BRT model is a shrinkage parameter that reduces the impact of each additional fitted tree, motivated by the boosting paradigm of fitting a model with many small models rather than fewer large trees. If one of the greedy iterations does not improve model fit, the contribution of that iteration can be easily reversed in the subsequent iterations (Natekin, 2013). The tree complexity parameter controls whether interactions between predictors are fitted. If tree complexity is 1, the tree will be an additive model with no interactions. A tree complexity of N will produce a model with N-way interactions between variables(Elith et al., 2008). These two variables together control the total number of trees needed to fit the model (Elith et al., 2008).
To assess the relative performance of parallel methods over sequential models, RF SDMs were fit both sequentially and in parallel on instances up to 24 CPU cores. Spatial resolution, memory, and taxon were held constant while number of ensemble members and number of training examples were systematically altered for each core. In total, 3576 random forests were fit using the randomForest function with the foreach package providing parallelization support. Sequential runs were fit using the same function but with number of cores set to only 1.
In addition to performance gains made by increasing the number of CPU cores and leveraging parallel methods, increasing instance memory should improve performance for very large datasets. Using simulated datasets, I attempted to assess the performance of the model when faced with more than 1 million input examples. R has little support for high memory tasks, and these tests routinely crashed the computer when trying to fit the SDM due to inability to allocate memory space.
Finally, I evaluated the effect of varying the number of predictors on the execution time of the algorithm. The literature on theoretical complexity of algorithms (e.g., (Hastie et al., 2009)) often characterize the complexity of machine learning algorithms in terms of both number of training examples and number of features in each example. I systematically modified the number of training examples between 1000 and 11000 and the number of predictors on all SDM classes, to get an understanding of how the two parameters interact. Because both of these algorithms run serially, they can safely be run on a single processor without the need for estimating the effects of additional cores.


As Kapelner & Bleich (2016) describe, the goal is to estimate f(X)+ϵ from a training set of data. Bayesian additive regression trees approximate the function as
f(X)≈T_1^M (X)+T_2^M (X)+...+T_m^M (X)+ϵ
ϵ∼N_m (0,σ^2 I_n)
In this framework,  is the training set of predictor features,  is the unexplained variance remaining. The model is composed as a sum of m trees, T^M, with parameters at each leaf node given by M, and the tree structure T. The T tree structure contains information about the splitting rules at each internal (non-leaf) node. M is the tree’s terminal node parameters, M_t=μ_t1,μ_t2,...,μ_(t_b ), where b is the total number of terminal nodes in the tree. Each observation’s value is found by navigating through each splitting rule at all internal nodes, summed over all M trees. As a Bayesian model, there is a prior distribution placed on the leaf parameters and tree structures and the noise term, which limits a single model term from dominating the model fit. The prior placed on tree structure enforces that trees must be shallow, with few internal nodes, which is ideal when building additive models, because it can reduce the risk of overfitting. The leaf node parameters have a normal prior on them which regularizes the predicted response by drawing the predictions towards the response mean. The noise variance prior is chosen so that 90% of it’s probability mass lies below the expected mean squared error from an ordinary least squared regression, which encourages the error term to be smaller than in an ordinary regression.


Data-Constrained Optimization
Constrained optimization routines proceeded very similarly to the unconstrained analyses above, but a prior constraint was placed on the space to subset it to only potential scenarios that would meet the constraint. A data-constrained optimization was performed by first slicing the accuracy space, as defined by algorithm variables, into reflect only those combinations of algorithm inputs with enough data to meet the constraint. For example, if the constraint called for a hard limit of 500 training examples, the accuracy maximization search would not search any values beyond 500o training examples. Similarly, the number of predictors could be limited if there were few covariates available for the researcher to utilize. Once the accuracy space had been subsetted, it was searched to find the accuracy-maximizing space within it. Once found, the analysis proceeded as in an unconstrained analysis, by fixing inputs to maximize accuracy and then minimizing time and cost.
Cost-Constrained Optimization
To calculate a cost- or time-constrained optimal configuration for a given SDM, first, the execution time model for that SDM was used to calculate a regular set of values for average execution time under various algorithm input combinations. Using bivariate interpolation, these values were interpolated across the entire space so each distinct combination of values had its own predicted time. Since the algorithm inputs, number of training examples and number of covariates are strong model drivers, these predictions give a time surface, which is subsequently truncated at the user’s threshold of time and/or cost. Once the time space has been subset, its boundaries are used to subset accuracy space. Next, the accuracy-maximizing point within the time-subset accuracy space is found. As in the data-constrained optimization procedure, the execution time model is used to find the posterior estimates for each hardware type and the time-and-cost minimizing cluster of points.


Each candidate configuration in the reduced space was assigned a dollar-per-hour rate using the Google Cloud Engine pricing scheme, based on its number of CPUs and amount of RAM. Using this scheme allows the prediction to be recomputed should a new pricing scheme become available. For every scenario in the candidate set, the execution time (in seconds) and accuracy (in AUC) was predicted. Finally, the execution time was multiplied by the dollar-per-hour rate to obtain a monetary cost for that experiment.
The candidate experiment generation was done using the expand.grid function in R and the model prediction was completed on a 16-core, 60 GB RAM instance on GCE. Predictions were spread across all 16 cores using the parallel package and the mclapply functionality provided in that package. Once each experiment in the hypercube had been predicted, it was possible to predict the optimal configuration under a variety of optimality criteria.
To calculate the unconstrained optimal configuration, the predicted values for execution time and accuracy were predicted for all values in the hypercube set. Using a bivariate interpolation method, the accuracy for every combiantion of algorithm inputs (training examples and number of predictors) was estimated. This entire space was searched for the minimal set of algorithm inputs that would result in the maximum accuracy the maximum accuracy. The minimal set of inputs was defined as the one with the least training examples and the least number of predictors.
This maximum-accuracy point, because it is only defined in terms of algorithm inputs, can be calculated on any combination of computer hardware. Holding the algorithm inputs, and thus, the accuracy, fixed at the maximum combination means that there are a number of different costs and times that will result when this algorithm is calculated, due to inherent differences in the hardware capabilities of each configuration. Using data on 287 computing configurations, the execution time model was used to estimate the cost, in seconds and dollars, for each. Under the Bayesian framework, the model was run 1000 iterations to get a posterior estimate of predicted time. Each time sample in the posterior was multiplied by the configuration’s rate per second to obtain a cost for obtaining the accuracy maximizing point.
The posterior distributions for time and cost were then plotted out on time and cost as orthogonal axes. Using the dist function in R, the euclidean distance between each sample in the posterior and the origin of the two axes was calculated to arrive at a density of potential distances for each hardware configuration. To assess the potential for similarly-distributed configurations that might compete for the lowest-cost and lowest-time, 25 distributions with the lowest means were selected as potential optimal candidates. Using hierarchical agglomerative cluster (hclust), the posterior distributions of distance were clustered according to their mean and standard deviation. The optimal point should both have a low mean, and a low standard deviation, and thus, low uncertainty. The top-25 potential candidate distributions were clustered hierarchically, and then split into distinct clusters at a dissimilarity of 1. Candidates with dissimilarity in their means and standard deviations larger than one were considered statistically different, and those within a dissimilarity of 1 were considered statistically the same. The cluster with the lowest mean distance to the time-cost origin was selected as the optimal cluster, and each member of that cluster was considered equally optimal.


While the results varied by SDM class, the predictive models for both time and accuracy showed considerable skill. For execution time, the most sucessful models were those for GBM-BRT and MARS, which both showed r^2 values of approximately 0.96. This is a surprising amount of variance able to be explained by a farily simple model with only five predictors. The MSE on both of these models was just over 1 second^2 (~ 0.06 ln(seconds)^2), indicating great ability to correctly predict to the observed value. The GAM and RF models showed slightly poorer performance. The variance explained by the GAM model was only 52.1%, while the RF model explained only 58%. While this is still a significant amount, there is still a large portion the the variance in these models that is not explained by the predictor set included here. Despite this, the GAM model still demonstrated a very low MSE (0.0121 ln(seconds)^2). The RF SDMs executed much more slowly, and thus, showed a larger MSE (~0.64 ln(seconds)^2), indeed, the worst MSE of all the SDMs.

Data-Constrained Optimization
To demonstrate the ability to predict the optimal configuration under a data-constraint, it is demonstrated here. This is clearly a very common occurrence in SDM applications, where there is simply no more data with which to fit the model. In this case, we’ll choose a relatively extreme example, and fit an optimal configuration with only 45 training examples. Covariates are still allowed to range between one and five.
Model	Fixed Accuracy	Training Examples	covariates
GAM	0.6776	45	5
GBM-BRT	XXX	XXX	XXX
MARS	0.69457	45	5
RF	0.7863	1	5
Table X Accuracy-Maximizing Points for each SDM under a strict data constraint of 45 training examples.
Once the accuracy is maximized at this low point, a significant decrease in the maximized accuracy is apparent, and expected. This decrease was about 0.05-0.1 points on the AUC, which corresponds with a good predictive model with a fair predictive model (FIND CITATION FOR THIS). As expected, most models require all 45 training points, and all require all five covariates to achieve maximum accuracy. The RF suggestion of accuracy maximization of 1 training example seems like a statistical artifact and should be treated with caution.
Configuration Number | CPUs | RAM | Seconds | Dollars | Mean Distance | Distance SD |
:————- | :————- | :————- | :————- | :————- | :————- | | :————- |
13| 2| 2| 5.224851| 0.3141180| 5.257580| 0.4889995|
25| 3| 2 |5.224851 |0.4711771| 5.269401| 0.4900990|
37| 4| 2| 5.224851| 0.6282361 |5.285905 |0.4916341|
Table X Optimal Hardware for GAM under a strict data constraint of 45 training examples.
XXXX NEED GBM-BRT TABLE HERE XXXX
| Configuration Number | CPUs | RAM | Seconds | Dollars | Mean Distance | Distance SD | | :————- | :————- | :————- | :————- | :————- | :————- | | :————- | |109 |10| 2 |4.738573| 1.4244151| 5.274748| 2.201935| |97 |9 | 2| 4.807257| 1.3005552| 5.298139| 2.156391| |133| 12| 2| 4.677079| 1.6871158| 5.298231| 2.211611 | |121| 11| 2| 4.732381| 1.5648092| 5.315057| 2.235309 | |145| 13| 2| 4.683376| 1.8301697| 5.353266| 2.217866 | |157| 14| 2| 4.679844| 1.9694656| 5.411921| 2.308578 | |169| 15| 2| 4.885003| 2.2026478 |5.706941| 2.324732 | |61 |6| 2| 5.679106| 1.0242836| 5.988099| 1.690359 | |49 |5| 2| 5.949659| 0.8942338| 6.257268| 1.810588 |
Table X Optimal Hardware for RF under a strict data constraint of 45 training examples.
| Configuration Number | CPUs | RAM | Seconds | Dollars | Mean Distance | Distance SD | | :————- | :————- | :————- | :————- | :————- | :————- | | :————- | |81| 7| 16| 1.796767| 2.142429| 3.580441| 2.695402| |93| 8| 16| 1.796767| 2.448490| 3.888889| 2.927606| |21 |2| 16| 3.274177| 1.115447| 4.152403| 2.109742| |105| 9 |16| 1.796767| 2.754551| 4.211241| 3.170276| |117| 10| 16| 1.796767| 3.060613| 4.544539| 3.421187| |33 |3 |16| 3.380755| 1.727634| 4.577421| 2.314046| |45| 4| 16| 3.380755| 2.303511| 4.932269| 2.493434| |57| 5| 16| 3.211710| 2.735413| 5.080262| 2.563458| |69| 6| 16| 3.211710| 3.282496| 5.530223| 2.790505|
Table X Optimal Hardware for MARS under a strict data constraint of 45 training examples.
The data-constrained optimals showed significant overlap with the unconstrained optimal predictions. One major difference between the two, however, is the decreased costs, in both time and money, of running the constrained experiment, since less data is used to fit the experiment.
In the case of MARS, we see that number of CPU cores is clearly not a big factor in the optimality of the solution, since everywhere between 2 and 10 CPU cores shows a statistically identical distribution. Again, we see that the memory requirement is large compared to the otehr SDM classes, however, this is likely still due to sampling design and unlikely to be a real recommendation. GAM suggested the same optimal set of 2, 3, or 4 cores with 2 GB of RAM. This makes sense that the GAM optimal is the same as from the unconstrained. There is very little hardware influence on the run time of the model, which predicts an unconstrained optimal will use the lowest amount of hardware. So, any experiment that requires less time than the unconstrained optimal would thus also need only the minimum amount of ahrdware. XXX about GBM-BRT. RF showed wide variability between the number of cores that would result in the optimal solution as well. It is likely to involve multiple cores, however, there is high uncertainty about the exact number required. Nevertheless, only two GB of RAM are suggested regardless of the number of CPU cores. This high degree of uncertainty is surprising, since the model shows high predictive skill of the test set, it would appear that the model itself should be able to differentiate between the different number of cores.
Cost-Constrained Optimization
A second type of constraint on the hardware configuration choice is a cost-limit or time-limit imposed by the user. In this case, the user has identified a limit that should not be surpassed, and a solution that falls within it should then be identified. In this particular example, the user has an unlimited supply of data with which to fit the model. However, the techniques from this section and the section above could be applied together to create a real-world situation in which both cost and and data are limited. In this example, the user has a limit of 22 seconds in which to run the model. This scenario could occur in a centralized web-based modeling application, in which the application requires latency to be below a certain time, so that user’s don’t get lost or distracted.
Model	Fixed Accuracy	Training Examples	covariates
GAM	0.71311	9000	5
GBM-BRT	0.7001	110	5
MARS	0.7016	130	5
RF	0.8136	1951	4


| Configuration Number | CPUs | RAM | Seconds | Dollars | Mean Distance | Distance SD | | :————- | :————- | :————- | :————- | :————- | :————- | | :————- | |13 | 2 | 2 | 5.302108 | 0.3187628| 5.330181| 0.4410313 | |25 | 3 | 2 | 5.302108 | 0.4781441| 5.342165| 0.4420229 | |37 |4 |2 |5.302108 |0.6375255 |5.358898 |0.4434074 |
Table X GAM unconstrained optimal predictions for computing hardware for achieving the accuracy-maximizing point.
| Configuration Number | CPUs | RAM | Seconds | Dollars | Mean Distance | Distance SD | | :————- | :————- | :————- | :————- | :————- | :————- | | :————- | |13| 2| 2 |1489.020| 89.51986| 1796.690 |1545.021|
Table X GBM-BRT unconstrained optimal predictions for computing hardware for achieving the accuracy-maximizing point.
| Configuration Number | CPUs | RAM | Seconds | Dollars | Mean Distance | Distance SD | | :————- | :————- | :————- | :————- | :————- | :————- | | :————- | |145| 13| 2| 21.86844| 8.545748| 24.84752| 8.630945| |157| 14| 2| 21.78000| 9.165896| 25.03071| 8.703743| |169| 15| 2| 21.71175| 9.789829| 25.19974| 8.699931| |133| 12| 2| 22.60012| 8.152314| 25.38249| 8.675544| |121| 11| 2| 22.74969| 7.522412| 25.38255| 9.059256| |181| 16| 2| 21.73022| 10.451368| 25.52088| 8.824121| |109| 10| 2| 23.31605| 7.008804| 25.83321| 9.269416| |193| 17| 2| 21.50065| 10.987265| 25.85966| 9.837260| |97 |9 |2| 24.22601| 6.554105| 26.64450| 9.645858|
Table X RF unconstrained optimal predictions for computing hardware for achieving the accuracy-maximizing point.
| Configuration Number | CPUs | RAM | Seconds | Dollars | Mean Distance | Distance SD | | :————- | :————- | :————- | :————- | :————- | :————- | | :————- | |81 |7| 16| 9.111372| 10.86422| 14.28928| 1.688315| |93 |8| 16| 9.111372| 12.41625| 15.52028| 1.833760| |105| 9 |16| 9.111372| 13.96828| 16.80676| 1.985762| |117| 10| 16| 9.111372| 15.52031| 18.13693| 2.142924| |129| 11| 16| 9.111372| 17.07234| 19.50185| 2.304192| |141| 12| 16| 9.111372| 18.62437| 20.89470| 2.468761| |153| 13| 16| 9.111372| 20.17640| 22.31026| 2.636013| |165| 14| 16| 9.111372| 21.72844| 23.74446| 2.805468| |177| 15| 16| 9.111372| 23.28047| 25.19413| 2.976750| |189| 16| 16| 9.111372| 24.83250| 26.65673| 3.149560|
Table X MARS unconstrained optimal predictions for computing hardware for achieving the accuracy-maximizing point.



Despite the challenges, several empirical runtime studies have attempted to include computer hardware’s effects on algorithm runtime and have achieved highly accurate results. Wu & Datla (2011) contend that the execution time of a program depends on the complexity of the algorithm and its input data, the static hardware configuration of the resource (e.g., amount and type of RAM, CPU clock rate), and the dynamic system state (e.g., number of processes competing for resources). Sadjadi et al. (2008) model execution time as a linear combination of contributions from elements of the computer’s hardware characteristics, though multivariate nonlinear systems may be more appropriate to capture some of the complex changes possible in the empirical data (Wu & Datla, 2011).
Previous attempts to model runtimes have relied on the subset of computing components thought to directly affect performance. Sadjadi et al. (2008) include CPU rate (GHz) and number of CPU cores, though other studies have also included memory amount and type, buffer size, and CPU cache size (Wu & Datla, 2011). While increasing the clock rate of a CPU is nearly guaranteed to improve execution time, the number of CPU cores can also affect execution time by allowing multiple programs to execute in parallel. To take advantage of multiple processors, algorithms must be specifically designed to partition their execution across multiple cores. Moreover, the addition of processor cores will only improve performance up to a point, after which all benefits of load sharing will be outweighed by cross-core communication (Gustafson, 1988). Computer memory can also affect a program’s runtime by reducing the number of times a computer must retrieve data from the physical storage device (e.g., hard disk) and can improve the the amount of concurrent work able to be done on a machine (Wu & Datla, 2011).


The MARS optimal configuration cluster calls for all configurations between 2 and 16 cores. Statistically, there is no quantitative difference between provisioning and running the model on a computer with two cores and one with 16 cores. THE GBM-BRT optimal cluster has a clear preference for a single core configuration. No optimal configurations are suggested with multiple cores. However, the optimal cluster suggests between 4 and 22 GB of RAM. Again, because all configurations fall within the same cluster, the same optimal result is expected on both the configuration with one core and 4 GB as the one with one core and 22 GB. Finally, the GAM SDM shows a small range of core types and a preference for 2 GB of RAM. In this cluster, all configuration with 2, 3, or 4 cores and 2GB of RAM are statistically similar.

Under a severely constrained dataset, with only 45 training examples, the models showed similar patterns as under the unconstrained optimal. GBM-BRT shows a wide spread of both cores and RAM requirements in the optimal cluster. Given the execution time, which is very small when compared to the unconstrained optimal, we are able to conclude that the runtime is just so low that any hardware configuration is just about the when computing it. Similarly, both GAM and MARS show the same optimal clusters for the data-constrained and unconstrained predictions. Because hardware has little bearing on the execution time of these models, it makes sense that if the optimal configuration to compute a hard model was on very weak hardware, the optimal to compute a small problem would be the same. The random forest model shows a requirement for slightly fewer cores (5) than under no constraints, which is consistent when fitting a model with less data.

If there is a finite amount of money that can be allotted to limit each model run to a fraction of the total available funds, allowing the researcher to complete all modeling experiments without going over budget. Here, however, I demonstrate the ability to place a constraint in the execution time on the model, and to come up with the optimal algorithm and hardware configuration under that limitation. This example is best thought of in terms of cloud computing and latency. While most SDMs are currently computed on desktop and laptop personal computers, it is likely that these models could be efficiently computed on a client-server environment. If the servers were distributed on a cloud platform, my framework could be used to automatically provision the optimal configuration that maximized the accuracy of the model results returned to the users, while minimizing the time and cost incurred by the server manager.

Once the models are fit, prediction, even for large datasets, is not a particularly large problem, rather, it is the model fitting process that must be differently designed. My results indicate that prediction takes only a fraction of the fitting time, even when a high spatial resolution is specified, and many thousands (or even millions) of points must be predicted. In the case of additive tree models, this is only a matter of evaluating the predictor set at each splitting point in the internal nodes of the tree, and averaging the predicted response of each tree in the ensemble. So even with very large datasets, this term is relatively small.  
