Many contemporary Big Data applications, such as the popular microblogging service Twitter (http://twitter.com), require distributed, parallel, streaming methods to identify key analytical trends in real time (e.g., Bifet, Holmes, Pfahringer, & Gavalda, 2011).

With the recent growth in monitoring and occurrence data, some ecological analysts apply similar, data-driven machine-learning techniques to ecological forecasting problems and have seen significant increases in predictive skill (Elith et al., 2006).

Moreover, common statistical forecasting methods can be computationally demanding, particularly on larger datasets (Huang et al., 2013). Climate-driven ecological forecasting models, variously called ecological niche models (Peterson, 2003), predictive habitat distribution models (Guisan & Zimmerman, 2000), and species distribution models (SDMs)(Guisan & Thuiller, 2005b), have seen extensive application in ecology, global change biology, evolutionary biogeography (Araújo, Whittaker, Ladle, & Erhard, 2005; Wilfried Thuiller et al., 2008a), reserve selection (Guisan et al., 2013), and invasive species management (Ficetola, Thuiller, & Miaud, 2007). SDMs characterize a species’ response to biospatial environmental gradients (Franklin, 2010), and use these responses to forecast potential future distributions under climate change scenarios. Recently, studies with an large number of occurrence records, along with computationally intensive modeling approaches, including nonparametric data-driven techniques (Elith et al., 2006) and Bayesian methods that rely on repeated sampling of full joint probability distributions (Clark et al., 2014; Dawson et al., 2016), have become commonplace.

Paleoenvironmental proxy data, including fossil pollen, macrofossils, and freshwater and marine diatoms, can be an important supplement to modern biodiversity data and are an important part of these emerging efforts.

Cloud computing may offer a technological solution to some of these problems (Hampton et al., 2013; Michener & Jones, 2012). Cloud computing is a architectural design pattern that provides “ubiquitous, convenient, and on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort” (Hassan, 2011; Mell & Grance, 2012; Vaquero et al., 2008). With the rapid commercialization of cloud computing and the widespread availability of public cloud providers like Amazon Web Services (AWS) and the Google Cloud Compute Engine (GCE), scientists have a seemingly unlimited supply of computing resources at their disposal. The cloud has been touted by many of the largest players in Silicon Valley, and is credited with Obama’s 2012 presidential election win, Netflix’s ability to provide streaming entertainment to millions of consumers, and Amazon’s massive success in online retailing (Mosco, 2014). In 2010, the U.S. Federal Government embraced the efficiency, speed, and economy of the cloud, requiring all federal agencies to adopt a “Cloud-First” policy when considering new information technology (IT) developments (Kundra, 2010). Accordingly, the National Aeronautics and Space Administration (NASA) and the National Science Foundation (NSF) have both officially endorsed cloud technology (Mosco, 2014). Researchers in many disciplines have posited that the cloud as the key to solving future computing and modeling challenges (Hsu et al., 2013; Yang et al., 2011).

Research questions
My research questions aim to develop a theoretically sound and empirically grounded understanding of the drivers of SDM execution time, and put this understanding towards predicting the computing solution with the lowest cost to the researcher. In particular, my research questions are:
1.	With what skill can the runtime of climate-based SDMs be predicted?
2.	Can an optimal computing solution for a given modeling scenario be predicted with confidence that this prediction is better than a null model that suggests no change in execution time between SDM scenarios.
3.	What are the drivers of SDM execution time, and how sensitive is the runtime to changes in these drivers?


The second characteristic of Big Data in the four V’s framework is the Variety of the data, and its ‘various types with complicated relationships’ (Yang & Huang, 2013).


I set up a distributed computing system to complete my experiments, featuring one centralized database node and multiple distributed computing nodes. Fault tolerance was important, because I utilized Google’s less expensive ‘preemptible’ resources, which function like normal instances, but can be shutdown at any time if other customers require additional computational power. One Master Node hosts a MySQL database and a control script (written in python), and a pool of computing nodes that are fault tolerant and designed only for computing are provisioned and decommissioned as needed. The compute nodes, which run a Debian Linux operating system, are given only enough information to complete a given SDM, and the Master Node control script manages the progress of the project as a whole. A node.js script provides programmatic access to real-time database content and is used as the linkage between the master node and the worker nodes.
An outline of the system is described below and are illustrated in Figure [X].
	First, a pool of computing nodes is assembled. The Master Node control script queries the central database for experiments that have yet to be completed or threw an error the last time they were run. The database responds, via the node API, with a JSON object that contains the hardware parameters required by the next experiment. The python script parses the response and uses the ‘gcloud’ tools associated with the GCE to create a pool of virtual instances that have the necessary amount of memory and number of CPUs.
	Each node in the pool automatically begins running a startup script that begins the modeling process. First, a number of system-wide software packages, including R and Git are installed on the new instance. Git is used to clone the most recent version of the project repository (hosted as a private repository on GitHub) which contains all files necessary to compute an SDM. Once all packages have been installed, the timing script is initialized as a new R session. The R script queries the central database, identifying itself as a computing node with x cores and y memory. The database responds with the parameters needed to run a single experiment on the given infrastructure. The script then loads the necessary variables and runs the SDM. When finished, it reports its results to the database and marks the experiment as completed. Experiments are continued until there are no more experiments that can be computed on an instance of x cores and y memory. If an instance is preempted by the system or otherwise crashes, a shutdown script will be executed, marking the in-progress experiment as interrupted – signaling other worker nodes that it should be attempted again later.
	Because Google charges by the minute for the use of their virtual machines, instances must be torn down as soon as possible. As the computing nodes execute the experiments, the Master Node repeatedly polls the central database to determine the current position within the experiment table, determining the percentage completion of the current group of experiments. Once the group is complete, Master Node will use the gcloud tools to decomission the individual instances, and delete the instance pool and the instance template that was used to create each virtual worker node. After this, the Master Node is the only instance that remains online. At this point, Master Node returns to Step 1 to build a new pool of instances for a new hardware configuration.


Due to budgetary constraints, and thus inability to cover all possible parameterizations, experiments were divided into several categories in which different variables were systematically altered to determine sensitivity. This compromise aims to capture as much within-parameter variance as possible while simultaneously capturing the influence of interactions between variables. One basic series of experiments was run for all SDMs using a small subset of algorithm inputs (training examples and memory) on a wide variety of VM types (core/memory combinations). Five additional, separate analyses were completed to determine the sensitivity to specific parameterizations. Because execution time can vary non-linearly when the hardware parameters are changed, I tested as many combinations of memory and CPUs as possible. On each computer, a standard set of 160 experiments were run for each sequential SDM (MARS, GBM-BRT, GAM), including four spatial extents, four training example sets, and 10 replicates of each. All experiments were done on the Picea data set.
Individual, target experiment sets were done to assess the contribution and sensitivity of individual or sets of parameterizations. While no theoretical difference would suggest that execution times should vary between different taxa, a set of 191 model runs were done to evaluate whether differences exist in practice. The model uses aspatial input sets, which suggests that geographic range, abundance, or taxon-specific patterns should not bias the results of the experiments. Using all four taxa on six different VM instance types, inter-taxonomic sensitivity was recorded. The number of training examples and spatial resolutions was held constant for these runs.
Empirical Performance Models theory suggests that specific algorithm parameterizations will take longer to execute than others (Cannon & John, 2007). SDMs have a large number of potential parameters with which to alter, though many ecologists use the defaults, or packages that make it difficult to change the default parameter values (e.g., dismo (???)). To assess the magnitude of changes in execution time due to different parameterizations, the GBM-BRT was tested on a set of 70 different combinations of tree complexity and learning rate. The learning rate parameter of the GBM-BRT model is a shrinkage parameter that reduces the impact of each additional fitted tree, motivated by the boosting paradigm of fitting a model with many small models rather than fewer large trees. If one of the greedy iterations does not improve model fit, the contribution of that iteration can be easily reversed in the subsequent iterations (Natekin, 2013). The tree complexity parameter controls whether interactions between predictors are fitted. If tree complexity is 1, the tree will be an additive model with no interactions. A tree complexity of N will produce a model with N-way interactions between variables(Elith et al., 2008). These two variables together control the total number of trees needed to fit the model (Elith et al., 2008).
To assess the relative performance of parallel methods over sequential models, RF SDMs were fit both sequentially and in parallel on instances up to 24 CPU cores. Spatial resolution, memory, and taxon were held constant while number of ensemble members and number of training examples were systematically altered for each core. In total, 3576 random forests were fit using the randomForest function with the foreach package providing parallelization support. Sequential runs were fit using the same function but with number of cores set to only 1.
In addition to performance gains made by increasing the number of CPU cores and leveraging parallel methods, increasing instance memory should improve performance for very large datasets. Using simulated datasets, I attempted to assess the performance of the model when faced with more than 1 million input examples. R has little support for high memory tasks, and these tests routinely crashed the computer when trying to fit the SDM due to inability to allocate memory space.
Finally, I evaluated the effect of varying the number of predictors on the execution time of the algorithm. The literature on theoretical complexity of algorithms (e.g., (Hastie et al., 2009)) often characterize the complexity of machine learning algorithms in terms of both number of training examples and number of features in each example. I systematically modified the number of training examples between 1000 and 11000 and the number of predictors on all SDM classes, to get an understanding of how the two parameters interact. Because both of these algorithms run serially, they can safely be run on a single processor without the need for estimating the effects of additional cores.


As Kapelner & Bleich (2016) describe, the goal is to estimate f(X)+ϵ from a training set of data. Bayesian additive regression trees approximate the function as
f(X)≈T_1^M (X)+T_2^M (X)+...+T_m^M (X)+ϵ
ϵ∼N_m (0,σ^2 I_n)
In this framework,  is the training set of predictor features,  is the unexplained variance remaining. The model is composed as a sum of m trees, T^M, with parameters at each leaf node given by M, and the tree structure T. The T tree structure contains information about the splitting rules at each internal (non-leaf) node. M is the tree’s terminal node parameters, M_t=μ_t1,μ_t2,...,μ_(t_b ), where b is the total number of terminal nodes in the tree. Each observation’s value is found by navigating through each splitting rule at all internal nodes, summed over all M trees. As a Bayesian model, there is a prior distribution placed on the leaf parameters and tree structures and the noise term, which limits a single model term from dominating the model fit. The prior placed on tree structure enforces that trees must be shallow, with few internal nodes, which is ideal when building additive models, because it can reduce the risk of overfitting. The leaf node parameters have a normal prior on them which regularizes the predicted response by drawing the predictions towards the response mean. The noise variance prior is chosen so that 90% of it’s probability mass lies below the expected mean squared error from an ordinary least squared regression, which encourages the error term to be smaller than in an ordinary regression.


Data-Constrained Optimization
Constrained optimization routines proceeded very similarly to the unconstrained analyses above, but a prior constraint was placed on the space to subset it to only potential scenarios that would meet the constraint. A data-constrained optimization was performed by first slicing the accuracy space, as defined by algorithm variables, into reflect only those combinations of algorithm inputs with enough data to meet the constraint. For example, if the constraint called for a hard limit of 500 training examples, the accuracy maximization search would not search any values beyond 500o training examples. Similarly, the number of predictors could be limited if there were few covariates available for the researcher to utilize. Once the accuracy space had been subsetted, it was searched to find the accuracy-maximizing space within it. Once found, the analysis proceeded as in an unconstrained analysis, by fixing inputs to maximize accuracy and then minimizing time and cost.
Cost-Constrained Optimization
To calculate a cost- or time-constrained optimal configuration for a given SDM, first, the execution time model for that SDM was used to calculate a regular set of values for average execution time under various algorithm input combinations. Using bivariate interpolation, these values were interpolated across the entire space so each distinct combination of values had its own predicted time. Since the algorithm inputs, number of training examples and number of covariates are strong model drivers, these predictions give a time surface, which is subsequently truncated at the user’s threshold of time and/or cost. Once the time space has been subset, its boundaries are used to subset accuracy space. Next, the accuracy-maximizing point within the time-subset accuracy space is found. As in the data-constrained optimization procedure, the execution time model is used to find the posterior estimates for each hardware type and the time-and-cost minimizing cluster of points.


Each candidate configuration in the reduced space was assigned a dollar-per-hour rate using the Google Cloud Engine pricing scheme, based on its number of CPUs and amount of RAM. Using this scheme allows the prediction to be recomputed should a new pricing scheme become available. For every scenario in the candidate set, the execution time (in seconds) and accuracy (in AUC) was predicted. Finally, the execution time was multiplied by the dollar-per-hour rate to obtain a monetary cost for that experiment.
The candidate experiment generation was done using the expand.grid function in R and the model prediction was completed on a 16-core, 60 GB RAM instance on GCE. Predictions were spread across all 16 cores using the parallel package and the mclapply functionality provided in that package. Once each experiment in the hypercube had been predicted, it was possible to predict the optimal configuration under a variety of optimality criteria.
To calculate the unconstrained optimal configuration, the predicted values for execution time and accuracy were predicted for all values in the hypercube set. Using a bivariate interpolation method, the accuracy for every combiantion of algorithm inputs (training examples and number of predictors) was estimated. This entire space was searched for the minimal set of algorithm inputs that would result in the maximum accuracy the maximum accuracy. The minimal set of inputs was defined as the one with the least training examples and the least number of predictors.
This maximum-accuracy point, because it is only defined in terms of algorithm inputs, can be calculated on any combination of computer hardware. Holding the algorithm inputs, and thus, the accuracy, fixed at the maximum combination means that there are a number of different costs and times that will result when this algorithm is calculated, due to inherent differences in the hardware capabilities of each configuration. Using data on 287 computing configurations, the execution time model was used to estimate the cost, in seconds and dollars, for each. Under the Bayesian framework, the model was run 1000 iterations to get a posterior estimate of predicted time. Each time sample in the posterior was multiplied by the configuration’s rate per second to obtain a cost for obtaining the accuracy maximizing point.
The posterior distributions for time and cost were then plotted out on time and cost as orthogonal axes. Using the dist function in R, the euclidean distance between each sample in the posterior and the origin of the two axes was calculated to arrive at a density of potential distances for each hardware configuration. To assess the potential for similarly-distributed configurations that might compete for the lowest-cost and lowest-time, 25 distributions with the lowest means were selected as potential optimal candidates. Using hierarchical agglomerative cluster (hclust), the posterior distributions of distance were clustered according to their mean and standard deviation. The optimal point should both have a low mean, and a low standard deviation, and thus, low uncertainty. The top-25 potential candidate distributions were clustered hierarchically, and then split into distinct clusters at a dissimilarity of 1. Candidates with dissimilarity in their means and standard deviations larger than one were considered statistically different, and those within a dissimilarity of 1 were considered statistically the same. The cluster with the lowest mean distance to the time-cost origin was selected as the optimal cluster, and each member of that cluster was considered equally optimal.
 
