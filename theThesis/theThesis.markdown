## Predicting the Optimal Computing Platform for Climate-Driven Ecological Forecasting Models
#### Scott Sherwin Farley
#### Master's Thesis
#### Advisor: John W. Williams

### Introduction
Human-induced global environmental change, including climate warming, anthropogenic land use, and the spread of invasive species, threatens to severely alter biodiversity patterns worldwide in the coming century [@Thuiller:2007cn; @Thuiller:2008enb; @Root:2005ha; @Lowe:2011fl]. Habitat degradation and fragmentation, expansion of invasive species, and a loss of climatically suitable areas are expected to result in large-scale biotic reorganizations, including the extinction of over one-quarter of all species [@Thomas:2010ht]. Climate exerts significant control over species distributions, particularly over those of vascular plants [@woodward1987climate; @Salisbury:1926fn], implying that anthropogenic perturbations to atmospheric CO<sub>2</sub> concentrations can have significant impact on species ranges [@Lowe:2011fl]. Assuming uniformitarianism [@Williams:2007iwa], statistical methods that quantify species responses to climatic gradients can then be used to forecast future biotic assemblages under different warming scenarios [@Thuiller:2008enb; @Guisan:2013hqa; @Maguire:2015eva; @Clark:2014gp; @Guisan:2005gz; @Guisan:2000tc].
Despite the potential for widespread, irreversible changes to the Earth's biosphere [@Barnosky:2012js], a growing volume of ecological data puts contemporary global change researchers in a position to forecast and mitigate impending catastrophic changes.  Environmental monitoring efforts, such as the Long Term Ecological Research program (LTER, [@Hobbie:2003ej]), National Ecological Observatory Network (NEON, [@Schimel:2009tx]), community curated databases, like the Neotoma Paleoecological Database (http://neotomadb.org) and the Paleobiology Database (PBDB, http://paleobiodb.org), and modern biodiversity occurrence databases, such as the Global Biodiversity Information Facility (GBIF, http://www.gbif.org), form a network of informatics infrastructure for supporting global environmental change research. New cyberinfrastructures for ecoinformatics organize, store, and serve a tremendous amount of information to researchers attempting understand and forecast responses to perturbations in the earth system [@Brewer:2012bk; @Michener:2012ho]. Paleoenvironmental proxy data, including fossil pollen, macrofossils, and freshwater and marine diatoms, can be an important supplement to modern biodiversity data and are an important part of these informatics efforts.  When forecasting to future climate spaces, paleoecolgical proxies can enhance understanding climatic changes by providing information about bioclimatic gradients not found on Earth today [@Veloz:2012jw]. Understanding responses to past climates can improve prediction of distributions under climates with no modern analog that are likely to emerge in the near future[@Williams:2007iwa]. However, though collections of modern and historical biodiversity data have the potential for forecasting studies, their volume, uncertainty, and heterogeneity can make their uptake challenging for ecologists [@Hampton:2013ko].
Once a dataset has been assembled, statistical methods can be used to mine it for new insight, extract parameters, and simulate future scenarios, though these techniques can be computationally demanding, particularly on larger datasets [@Huang:2013iy]. Many contemporary Big Data applications, such as the popular microblogging service Twitter (http://twitter.com), require distributed, parallel, streaming methods to identify key analytical trends in real time [e.g., @Bifet:2011wa]. With the recent growth in monitoring and occurrence data, some ecological analysts apply similar, data-driven machine-learning techniques to ecological forecasting problems and have seen significant increases in predictive skill [@Elith:2006vt]. Climate-driven ecological forecasting models, variously called ecological niche models [@Peterson:2003gl], predictive habitat distribution models [@Guisan:2000tc], and species distribution models (SDMs)[@Guisan:2005gza], have seen extensive application in ecology, global change biology, evolutionary biogeography [@Thuiller:2008enb; @Araujo:2005jy], reserve selection [@Guisan:2013hqa], and invasive species management [@Ficetola:2007bn].  SDMs characterize a species' response to biospatial environmental gradients [@Franklin:2010tn], and use these responses to forecast potential future distributions under climate change scenarios. Recently, studies with an large number of occurrence records, along with computationally intensive modeling approaches, including nonparametric data-driven techniques [@Elith:2006vt] and Bayesian methods that rely on repeated sampling of full joint probability distributions [@Dawson:2016wa; @Clark:2014gp], have become commonplace.

With over 1 billion occurrence records in Neotoma and GBIF, traditional statistical methods for analyzing and forecasting ecological processes often cannot be applied without compromising analysis scope. Most leading SDM methods, though popular in the literature and highly skillful, are not scalable to very large datasets because they are not designed to take advantage of parallel processing or distributed computing. With so much data, ecological modelers are likely to need to adopt Big Data techniques, including distributed computing and ensemble parallelism, common in fields like bioinformatics [@Schatz:2010js], geonomics [@Anonymous:aiu_1fsc], climate analytics [@Schnase:2014dn], and private industry [@Mosco:2014cu]. As the volume of ecological data increases and the need for high resolution, accurate projections of biotic distributions becomes more pressing, reducing project scope [e.g., @Bolker:2009csa] can no longer be considered a valid option.

Cloud computing may offer a technological solution to some of the problems posed by the increasing Bigness of ecological data [@Hampton:2013ko; @Michener:2012ho]. Cloud computing is a architectural design pattern that provides "ubiquitous, convenient, and on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort" [@Mell:2012jj; @Hassan:2011uh; @Anonymous:mc8EfgMa]. With the rapid commercialization of cloud computing and the widespread availability of public cloud providers like Amazon Web Services (AWS) and the Google Cloud Compute Engine (GCE), scientists have a seemingly unlimited supply of computing resources at their disposal. The cloud has been touted by many of the largest players in Silicon Valley, and is credited with Obama's 2012 presidential election win, Netflix's ability to provide streaming entertainment to millions of consumers, and Amazon's massive success in online retailing ([@Mosco:2014cu]). In 2010, the U.S. Federal Government embraced the efficiency, speed, and economy of the cloud, requiring all federal agencies to adopt a "Cloud-First" policy when considering new information technology (IT) developments [@Kundra:2010vb]. Accordingly, the National Aeronautics and Space Administration (NASA) and the National Science Foundation (NSF) have both officially endorsed cloud technology [@Mosco:2014cu]. Moreover, researchers in many disciplines have posited that the cloud as the key to solving future computing and modeling challenges [e.g., @Yang:2011bd; @Hsu:2013jz].  

Although the cloud offers a promising technological solution to the computational demands of ecological forecasting, there are few guiding principles on when the benefits, in reduced computing time, outweigh the financial costs of a cloud-based solution.
The cloud introduces a novel expense model for computing: by partitioning large physical resources into smaller virtual machines (VMs), cloud providers can charge consumers for the use of computing resources, rather than their purchase [@Hassan:2011uh]. Under this new operational expense model, consumers are no longer tied to a single hardware configuration, rather they can scale their virtual instances up or down depending on computational demand [@Armbrust:2009wla]. Despite being more complex to set up and maintain, Cloud-based solutions can offer significant advantages over traditional desktop computing.  While the exact costs of migrating to a cloud environment are difficult to estimate [but see @KeweiSun:2013dp], the computational time gains achieved by running models on high performance virtual instances can be measured empirically and combined with cost estimates to provide guidance on when a cloud based solution would be economically rational.

In this thesis, I develop a framework for predicting the optimal computing solution for a set of SDM activities in order to better understand limits to the application of climate-driven forecasting models and improve large scale modeling of biodiversity shifts in response to climate change. Treating workflow characteristics as model parameters, I posit a theoretical model for a scenario-specific optimal computing solution that minimizes computational time and financial cost. I gather data on the runtimes of four different classes of species distribution models under different parameterizations and on different computing hardware. Using this empirical dataset, I fit a nonparameteric learning model to this data to assess the drivers of SDM runtime and predict the execution time of future modeling scenarios. My findings suggest that if SDMs, and biodiversity more generally, are to benefit from cloud computing, future effort should be directed towards developing models that more explicitly take advantage of parallelism and distributed computing frameworks.

## Research questions
My research questions aim to develop a theoretically sound and empirically grounded understanding of the drivers of SDM execution time, and put this understanding towards predicting the computing solution with the lowest cost to the researcher. In particular, my research questions are:

1.  With what skill can the runtime of climate-based SDMs be predicted?
2.  Can an optimal computing solution for a given modeling scenario be predicted with confidence that this prediction is better than a null model that suggests no change in execution time between SDM scenarios.
3.  What are the drivers of SDM execution time, and how sensitive is the runtime to changes in these drivers?

## Background and Previous Work
#### Big Data in Ecology
The vast expansion of data the sciences has necessitated the development of revolutionary measures for data management, analysis, and accessibility [@Schaeffer:2008kl]. Worldwide data volume doubled nine times between 2006 and 2011, and successive doubling has continued into this decade [@Chen:2014fc]. With the large influx of massive geonomic sequences, long term environmental monitoring projects, phylogenetic histories, and biodiversity occurrence data, robust, expressive and quantitative methods are essential to the future of the biological sciences [@Schaeffer:2008kl]. As datasets become larger, significant challenges are encountered, including inability to move datasets across networks, necessity of high performance and high throughput computing techniques, increased metadata requirements for storage and data discovery, and the need to respond to new uses for the data [@Schnase:2014dn].

Ecological occurrence data are records of presence, absence, or abundance or individuals of a species, clade or higher taxonomic grouping that are fundamental to biodiversity analyses, ecological hypothesis testing, and global change research. These data are increasingly being stored in large, dedicated, community-curated databases like Neotoma, GBIF, PBDB. Since the early 1990s, the internet and associated IT and an increased willingness to share primary data between scientists precipitated rapid influxes of  digital occurrence records. While there are known problems with the quality and consistency of data records in large occurrence databases [@soberon2002issues; ], they provide a low-friction way to consume large amounts of data that would otherwise be prohibitively time consuming to derive from the literature or in the field [@Beck:2014ky; @Grimm:2013uu]. Entire new fields, namely 'Biodiversity Informatics' [@Soberon:2004jh], 'Ecoinformatics' [@Michener:2012ho], and 'Paleoecoinformatics' [@Brewer:2012bk] have been developed and delineated to address the growing challenges and opportunities presented by the management, exploration, analysis and interpretation of primary data regarding life, particularly at the species level, now centralized in biodiversity databases [@Soberon:2004jh].

The term Big Data is typically used to describe very large datasets, whose volume is often accompanied by lack of structure and a need for real-time analysis. Big Data, while posing significant management and analysis challenges, can provide new insights into difficult problems [@Chen:2014fc]. Though the precise definition of Big Data is loose, there are two prominent frameworks for discriminating Big Data from traditional data. One characterizes Big Data as "term used to describe data sets so large and complex that they become awkward to work with using standard statistical software" [@Snijders:2012ww].  This ambiguous delineation is echoed in the advertising and marketing literature that accompanies products, like cloud computing, that facilitate Big Data analysis. For example, Apache Hadoop, a popular distributed computing framework, has described Big Data as “datasets which could not be captured, managed, and processed by general computers within an acceptable scope” [@Chen:2014fc].

Under this framework, the Bigness of the data is specific to both the time of analysis and the entity attempting to analyze it. @Manyika:2015vk suggest that the volume of data required to be Big can change over time, and may grow with time or as technology advances. Furthermore, the criteria for what constitutes Big Data can vary between problem domains [@Chen:2014fc], the size of datasets common in a particularly industry, and the kinds of software tools that are commonly used in that industry [@Manyika:2015vk]. The Big Data label is most often applied to datasets between several terabytes and several petabytes (2^40 to 2^50 bytes). However, because of ecology's lack of experience with massive datasets and limited analysis software in the discipline, ecological occurrence data clearly falls under the banner of Big Data.

The recent development of complex relational databases that store spatiotemporal occurrence records and their metadata suggests that traditional methods of data handling were not sufficient for modern ecological analyses. While the datasets are not particularly large in storage volume, they are composed of millions of heterogenous records with complex linkages. Consider the complexity of the relationships between different data records, for example. Databases like Neotoma have an exceptionally complicated relational table structure.  Together these tables describe the complicated web of relationships between each entity.  Further developments, like application programming interfaces and language specific bindings, supplement the tasks of accessing, filtering and working with the large occurrence datasets [@Goring:2015cr; @Anonymous:sFxqyl5u; @Anonymous:GSjiJRTH]. While occurrence data does not require the disk space of popular commercial applications like Twitter and Youtube, it has recently demonstrated a need for new, custom built tools to store, analyze, and use large numbers of records.

A second important framework by which to assess Big Data is the 'Four V's Framework'. First introduced by IBM and used by large technological companies in the early 2000's to characterize their data, it is now a popular and flexible framework under which to describe Big Data. Under this framework, a dataset's Bigness is described by its Volume, Variety, Veracity, and Velocity. @Yang:2013gm describe this framework, suggesting that “volume refers to the size of the data; velocity indicates that big data are sensitive to time, variety means big data comprise various types of data with complicated relationships, and veracity indicates the trustworthiness of the data” [@Yang:2013gm p 276.].

Since the late 1990s, the scale of biodiversity information alone has become challenging to manage. Figures [X]a and [X]b track the growth in collections of Neotoma and GBIF through time. In 1990, only 2 of the records now stored in Neotoma were in digitized collections. Today, there are over 14,000 datasets containing [XXX] individual occurrence records, and associated spatial, temporal, and taxonomic metadata, corresponding to an average growth rate of 1.4 records [[[XXX occurrences]]] per day. Nearly all records in Neotoma are derived from sedimentary coring or macrofossil extraction efforts, data gathering techniques that require large expenditures of time and effort [@Davis:1963hk; @Glew:2002fv]. GBIF houses digital records of well over 600 million observations, recorded specimens (both fossil and living), and occurrences described in the scientific literature. Since its conception at the turn of the century, the facility’s holdings have grown nearly 300%, from about 180 million records in 2001 to over 614 million records today. GBIF's reliance on literature and museum specimens allow its holdings to precede its origin in 2001, however, it is within the last 15 years that the data's volume has become clearly apparent.

The second characteristic of Big Data in the four V’s framework is the Variety of the data, and its ‘various types with complicated relationships’ [@Yang:2013gm]. Biodiversity data is highly diverse with many very complicated relationships and interrelationships.  As shown in Figure [X]a, Neotoma’s holdings feature 23 dataset categories, including X-ray fluorescence (XRF) and isotopic measurements, macro fossils of both vertebrates and plants, modern and fossil pollen records, and freshwater diatom and water chemistry series. Similarly, in GBIF, there are 9 distinct record types, including human observations, living and fossil specimens, literature review, and machine measurements (Figure [X]b).  Though the records coexist in large biodiversity database, they are distinctly different, derived using different protocols by different communities of researchers.

The data's spatial and temporal nature causes complex interrelationships between data entities. All of Neotoma's records and 87.6% of GBIF's records are georeferenced to specific places on the earth's surface. The spatial information in these databases is supplemented by other fields that describe the location of the observation, including depositional setting, lake area, and site altitude, to improve contextual interpretation of occurrence data. Managing data with a spatial component is nearly always more challenging than managing data without it.  Digital representations of spatial phenomena must grapple with data that is a discrete representation of a continuous physical feature, correlations between parameters, space and time, and processes differences at heterogenous spatiotemporal scales [@Yang:2011iy].  Furthermore, occurrence data represents the work of many dispersed individual researchers and research teams.  The controlled vocabularies and organization of aggregating databases helps to efficiently assimilate large numbers of records, however, nearly every record was collected, analyzed, and published by a different scientist. While some scientists have contributed many datasets to occurrence databases, most have only contributed one or two. The median number of datasets contributed to Neotoma is only 2 and the third quantile value is just 7 datasets.  Each researcher is apt to use different equipment, employ different lab procedures, and utilize different documentation practices, contributing to a highly variable dataset.

Biodiversity data also has high levels of uncertainty associated with it, comprising the third of the Four V's. Some of the sources of uncertainty in the data, like spatial or temporal positional uncertainty can be estimated [@Wing:2005wl] or modeled [@Blaauw:2010kg]. Other sources of uncertainty have yet to be quantified, for example inter-researcher identification differences, measurement errors, and data lost in the transition from field to lab to database. A recent paper by the Paleon working group used expert elicitation to quantify the differences between the dates assigned to European settlement horizon, a process they argue varies between sites, and depends on the “temporal density of pollen samples, time-averaging of sediments, the rapidity of forest clearance and landscape transformation, the pollen representation of dominant trees, which can dampen or amplify the ragweed signal, and expert knowledge of the region and the late-Holocene history of the site”[@Dawson:2016wa]. The findings of this exercise suggest that paleoenvironmental inference from proxy data is highly variable between researchers.  Moreover, some information is undoubtedly lost in the process of going from a field site through a lab workflow to being aggregated in the dataset. Though some procedural information accompanies the data records in the database, not all process details can be incorporated into database metadata fields, and probably more importantly, contextual details essential to proper interpretation of the data often gets lost on aggregation.

Both Neotoma and GBIF show high levels of quantifiable uncertainty, and are likely to show high levels of unquantifiable uncertainty as well. Of a random sample of 10,000 records of the genus *Picea* from GBIF, over half did not report spatial coordinate uncertainty. Of the 4,519 records that did, the average uncertainty was 305 meters, and the maximum was 1,970 meters. Clearly, such high levels of uncertainty might be problematic for modeling efforts [@Beck:2014ky]. Neotoma records show a similar uncertainty in their temporal information. Neotoma records each have a minimum, maximum, and most likely age for each age control point (e.g., radiocarbon date). Out of a sample of 32,341 age controls in the database, only 5,722 reported any age uncertainty at all. The summary statistics for these age controls suggest that the median age control has a temporal uncertainty of 260.0 years. The 25% percentile is an uncertainty of 137.5 years and the 75% 751.2 years, suggesting that dates are only identifiable down to ± 130 years of the actual date, on average. [NEOTOMA UNCERTAINTY THROUGH TIME]. Considering sediment mixing, laboratory precision, and other processes at work this is a relatively minor uncertainty, but it certainly contributes to occurrence data's lack of veracity.

The final piece of the Big Data framework is the dataset's velocity, which characterizes the dataset's sensitivity to time. High velocity data must be analyzed in real time as a stream to produce meaningful insights. Tweets, for example, are analyzed for trends as they are posted. User's are drawn to participation in up-to-the minute discussion, and significant effort has been put towards the development of sophisticated algorithms that can detect clusters and trends in real time [@Kogan:2014hh; @Bifet:2011wa]. The rate of increase in data volume in both Neotoma and GBIF is not fast enough to invalidate the results from previous analyses, suggesting that it's velocity is not high enough to warrant streaming techniques. Neotoma's growth rate of approximately 1.4 new datasets each day (1990-2016 average) and GBIF's daily growth rate of about 59,000 records (2000-2015 average) are small compared to the total number records in the database. Unlike in many private sector applications, there is little incentive to researchers to immediately analyze new biodiversity records, since all new findings will be reported in the academic paper cycle, typically several months to years. Moreover, automated analyses of distributional data have been warned against, due to the overall poor data quality [@soberon2002issues] and high levels of uncertainty.

While not time sensitive, ecological occurrence data requires advanced, sophisticated techniques to store and analyze, and demonstrates high volume, low veracity, and significant variety, and should therefore fall under the auspices of Big Data. Traditional statistical analysis techniques and storage methods for occurrence data may begin to suffer because they were not designed to handle Big Data. Both GBIF and Neotoma are experiencing sustained and increasing growth that has not diminished the early 1990s. To fully and accurately derive value from new data being added to distributional databases, novel and advanced techniques for modeling and analyzing this data are required.

## Cloud Computing in the Sciences
In recent years, large technology companies have promoted cloud computing as a way of overcoming the computational challenges associated with Big Data. Like grid computing, the cloud model standardized requests for computer resource to leverage distributed networks of physical machines to create a computing utility. As @Foster:2008cl suggest,

>"Cloud Computing not only overlaps with Grid Computing, it is indeed evolved out of Grid Computing and relies on Grid Computing as its backbone and infrastructure support. The evolution has been a result of a shift in focus from an infrastructure that delivers storage and compute resources (such is the case in Grids) to one that is economy based aiming to deliver more abstract resources and services (such is the case in Clouds)."

Grid computing never achieved large-scale success in industry, though it did succeed in forming massive federated systems providing computing power and data to scientists that still exist today, such as the Earth System Grid [@Foster:2008cl]. Unlike the grid's collective and project based utility-style, the cloud provides a pay-as-you-go business model and large economies of scale [@Armbrust:2009wl; @Hassan:2011uh]. While some organizations and universities have developed 'private clouds', large collections of virtualized servers not made available to the general public, many researchers have recognized the potential for incorporating public clouds, utility computing provided as a service by a cloud provider, into their workflows.  With this technology, scientists with little or no computational infrastructure can have access to scalable and cost-effective computational resources[@Hsu:2013jz].

Major scientific organizations in the United States, including the NSF and NASA, have made major pushes to promote cloud computing in their own operation. Spurred by the U.S. Office of Management and Budget's 2010 "25 Point Plan to Reform Federal Information Technology Management" [@Kundra:2010vb], federal agencies are now required to adopt a "Cloud First" policy when "contemplating IT purchases and evaluate secure, reliable, and cost-effective cloud computing alternatives when making new IT investments" [@Anonymous:sigZdwPI]. The federal plan also provisioned programs to help agencies adopt cloud technologies, reducing the effort needed to screen cloud providers for data security policies and enable rapid procurement of cloud services [@Kundra:2010vb].  NASA developed its own high performance open-source cloud stack, Nebula, before concluding that its own needs were better served by public cloud providers [@Anonymous:sigZdwPI], though private clouds are still a popular method for large academic organizations [@Huang:2013iy]. In 2013, the NSF announced a $20 million dollar solicitation for supporting "research infrastructure that enables the academic research community to develop and experiment with novel cloud architectures addressing emerging challenges, including real-time and high-confidence systems"[^1]. The public cloud now provides several large open-access datasets for public consumption, including Landsat images, real-time NEXRAD radar, and the 1000 Geonomes project, and claims that many research institutions, including the NASA Jet Propulsion Laboratory, among others, use their products and services[^2].

[^1]: https://aws.amazon.com/government-education/research-and-technical-computing/. Accessed 18 September, 2016.
[^2]: http://www.nsf.gov/pubs/2013/nsf13602/nsf13602.htm

Cloud technology, both public and private, has been extensively lauded for its application to the traditional Big Data fields of bioinformatics [@Stein:2007jo; @Hsu:2013jz, @Issa:2013jp] and climate analytics[@Schnase:2015wp; @Schnase:2014dn; @Lu:2011ii].  Cloud based solutions for bioinformatics research relieve the large memory requirements often present in geonomics and drug-design data [@Hsu:2013jz]. Biomedical scientists have developed a variety of public-cloud based applications including low latency, streaming methods for data analysis [@Issa:2013jp] and biology-specific operating systems that support protein analytics out of the box [@Kajan:2013cb]. Climate analytics and reanalysis are also demonstrative of fields adopting cloud computing technology early in their history. @Schnase:2014dn describes the development of Climate Analytics as a Service, an effort to integrate data storage and high performance computing to perform data-proximal analytics[@Schnase:2014dn; @Schnase:2015wp].

Cloud services have also been used in the geosciences, and in ecological modeling problems specifically. @Yang:2011bd note that despite recent advances in computing, geoscientific problems are still limited by computational ability, including data volume bottlenecks, processing limitations, multi-user concurrency, and spatiotemporal velocity of data [@Yang:2011bd]. Many scholars suggest that the cloud provides a means of overcoming these challenges by leveraging distributed computational resources without increasing the carbon footprint or financial budget [@Yang:2011bd]. Several large spatial data infrastructure (SDI) projects are currently hosted on public clouds, though geospatial algorithms are more difficult to implement on public cloud infrastructure than on more traditional computing environments like grid computing [@Huang:2010us]. Numerical models, such as real-time dust storm forecasting, have seen significant performance increases when run on high performance cloud VMs [@Yang:2011iy]. Environmental models, when implemented in a consumption-oriented way, can also be run in the cloud [@Granell:2013ix]. @Candela:2013bl describe a novel 'hybrid' platform that supports SDM specifically, suggesting that a cloud-based approach can aid in data discovery and increase processing capabilities. OpenModeller [@deSouzaMunoz:2009bn], while not cloud-specific, offers a generic interface for running multiple SDM algorithms on a remote server.

#### Species Distribution Models

Species Distribution Models (SDMs) are a class of statistical models that quantify the relationships between a species and its environmental range determinants [@Svenning:2011jq].  While these models sometimes include mechanistic or process components, they most often refer to correlative models [@Elith:2009gj], using supervised statistical learning algorithms to approximate the functional relationship between species occurrence and its environmental or climatic covariates. Used in a variety of global change-related fields, the models have been shown to provide reliable estimates of climate-driven range shifts when compared to independent datasets [@Guisan:2006bz; @Guisan:2000tc]. With the widespread availability of statistical software and machine learning code libraries and lower barriers to access to environmental and occurrence data, the utilization of these techniques has grown substantially in recent years [@Franklin:2010tn; @Svenning:2011jq], shown in Figure [X]. Science and engineering as a whole had a 7% growth rate between 20013 and 2013 [@NationalScienceBoardUS:2016uv], while SDM literature far outpaced this with a 10.8% average growth rate over the same period [Don't know how or if to cite this].

SDMs work by approximating the species niche and then applying it to future scenarios. @Hutchinson:2016tg characterized a species' fundamental niche as an n-dimensional hypervolume that defines the environmental spaces where the intrinsic population growth rate of the species is positive [@Williams:2007iwa]. The realized niche describes the subset of environmental space that the species actually occupies at some point in time, and is smaller than the fundamental niche due to competing biotic interactions with other species. Most scholars argue that SDMs come close to approximating the species' realized niche [@Guisan:2000tc; @Soberon:2005vt; @Miller:2007br], though the inclusion of fossil data in the model fitting process can increase the likelihood that calibration captures the fundamental niche by exposing the model to states of the climate system not present on Earth today [@Veloz:2012jw].

SDMs rely on three important assumptions. As a fundamental justification for applying predictions across space and time, all SDMs assume niche conservatism, i.e., that the niche of species remains constant across all spaces and times [@Pearman:2008it]. @Thuiller:2008ena argues that SDM fitting with fossil data also lessens effect of assuming niche conservatism through time.  Though speciation and evolution are not accounted for in the SDM paradigm, @Peterson:1999ff suggests that species typically demonstrate niche conservatism on multi-million year time scales.  Second, SDMs rely on the assumption that species are at equilibrium with their environment, being present in all environmentally suitable areas while being absent from all unsuitable ones [@NoguesBravo:2009iva].  Given dispersal limitations and biotic interactions between species, this is rarely the case. @Svenning:2008gs showed that many European tree species are still limited by postglacial migrational lag.  Finally, SDMs must account for extrapolation to novel and no-analog climates for which there is no modern or fossil data.  Inductive learning is severely impacted when it is used to predict onto future cases that were not within the range of values provided in the training set. @Williams:2007iwa note the high likelihood of encountering novel and no-analog climates in the near future.  Fitting the models with fossil data increases the likelihood that climatic assemblages will be included in the training data, however, given rapid and highly uncertain climate change, the problem of projecting models onto unseen climates is a major limit to their application.

Despite their strong assumptions, SDMs have been used for a wide variety of paleo and contemporary studies of geographic and environmental distribution. SDMs are very often used to confirm ecological hypotheses, comparing hindcast projections with the fossil record. In this context, SDMs have been used to support hypotheses on the extinction of Eurasian megafauna [@Anonymous:2008jc], identifying late-Pleistocene glacial refugia [@Waltari:2007gc; @Keppel:2011ft; @Flojgaard:2009ha], and to assess the effect of post-glacial distributional limitations and biodiversity changes [@Svenning:2008gs]. SDMs are often combined with genetic, phylogeographic, and other methods to develop a complete assessment of a species biogeographical history [@Fritz:2013er]. In an anthropogenic climate change context, SDMs have been used to assess the effectiveness of modern reserve planning [@Araujo:2004fd], predict the distribution of both endangered [@Thuiller:2005fm] and invasive species[@Ficetola:2007bn; @Vaclavik:2009gr], and ecosystems [@Hamann:2006jf], and evaluate the effectiveness of conservation planning for the future [@LOISELLE:2003gh].

#### A Taxonomy of Species Distribution Models

SDMs range from simple algorithms that characterize a 'climate envelope' for a species [@Guisan:2000tc] to multivariate bayesian techniques that use Markov Chain Monte Carlo simulations (MCMC) to develop probability distributions around projections and parameters. While all have the same fundamental goal of characterizing responses to climatic gradients, [@Franklin:2010tn] notes a conceptually meaningful way of grouping modeling algorithms into data-driven and model-driven algorithms.  The data-driven/model-driven dichotomy is introduced in @Hastie:2009up and often employed when differentiating between 'statistical' and 'machine learning' algorithms. I add the burgeoning set of methods that employ stochastic, probability-based Bayesian methods to this taxonomy due to their recent uptake and high accuracy. No individual method or class of methods has consistently outperformed any other [@Veloz:2012jw; @Elith:2006vt; @ARAUJO:2007ep], though many scholars have attempted to assess interclass variation [@Araujo:2006bi; @Elith:2006vt], and variation between different parameterizations of the same model class [@Thuiller:2008enb; @Veloz:2012jw; @ARAUJO:2007ep].

Supervised learning techniques map inputs to outputs by using a set of training examples, $T = (x_i, y_i), i= 1, 2,..., n$, where both values are known, then approximating the the real relationship between the two $f$, with a function ,$\widehat{f}$, that minimizes a loss function based on the difference between the real and predicted value, $y_i - \widehat{f}(i)$. Each training example may be composed of a $p$-dimensional vector of predictors, $\pmb{x_i} = x_{i1}, x_{i2}, ..., x_{ip}, p=1, 2, ..., p$. Models can either make *a priori* assumptions about the form of the input-output relationship or adapt to fit any given matrix of training examples.  The former class of models, model-driven models, exhibit low bias but high variance and can make poor predictions if the assumptions are not upheld.  The latter, data-driven models, do not rely on any stringent assumptions about the form of the relationship, but any particular subregion depends on only a handful on input points, making the models highly sensitive to small changes in the input data [@Hastie:2009up].

Model-driven learners fit parametric statistical models to a dataset, making assumptions about how inputs and outputs are related, including linearity and error distribution. These models were the first to see substantial use in SDM applications and continue to be widely used because of their strong statistical foundations and ability to realistically model ecological relationships [@Austin:2002vy]. These models include simple boxcar algorithms, which build multidimensional bounding boxes around species presence in environmental space [@Guisan:2000tc], as well as more complex methods such as generalized linear models [@Guisan:2002dc; @Vincent:1983uw]. Other model-driven techniques include variants of linear and logistic regression for abundance and presence-absence outputs [@Franklin:2010tn].

The increase in available computing power has spurred the development and application of non-parametric, data-driven learning algorithms.  These models, have, in some cases, been shown to significantly out perform their model-driven counterparts [@Elith:2006vt].  These models include genetic algorithms [@Elith:2006vt], classification and regression trees [@Elith:2008el], artificial neural networks [@Hastie:2009up], support vector machines [@DRAKE:2006cp], and maximum entropy techniques [@Elith:2010cea; @Anonymous:2008kla].  Since its introduction in 2006, MaxEnt, a maximum entropy algorithm and Java-based runtime environment has seen widespread use in SDM and has demonstrated its ability to perform consistently even on small sample sizes [@Phillips:2006ffa; @Elith:2010cea; @Anonymous:2008kla].  Trends in the SDM literature suggest that MaxEnt is the most popular SDM method in use today, and appearing in over 20% of all SDM studies published after 2008.  Recent evaluations of MaxEnt, however, claim that its performance may be questionable when compared with other SDM algorithms [@Fitzpatrick:2013cb]. Data-driven models can be more computationally intensive than their model-driven counterparts because they usually take at least two passes over the input dataset to process the data and build the model. Furthermore, data-driven learners are often combined with techniques like bagging, which builds a collection of models based on subsets of the input data, and boosting, which combines many weakly predictive models into a single, highly predictive ensemble. A common application of boosting in SDM is in boosted regression trees [@Elith:2008el; @Elith:2006vt], where stochastic gradient descent is used to sequentially combine many small regression trees together into an ensemble that minimizes model deviance.

Models in the third category utilize Bayesian methods to develop the relationship between environmental predictors and species presence.  Advantages of the Bayesian approach include the ability to include prior ecological knowledge in model formulation [@Ellison:2004fj] and the ability to estimate model uncertainty without the need for bootstrapping procedures [@Dormann:2012cj; @Elith:2009gja]. With improved computational infrastructure and better MCMC sampling algorithms, Bayesian methods have become increasingly popular in recent years, and have been used in several recent SDM studies [@Hegel:2010gu]. @Golding:2016bt introduce SDMs that incorporate Gaussian processes, a flexible method that demonstrates both high predictive accuracy and ecologically sound predictions. While most model- and data-driven SDMs use only abiotic environmental factors as their predictors, using a joint probability distribution of all entities in an ecosystem, joint SDMs can model both the climatic range limitations of a species and its biotic interactions with other species [@Clark:2014gp]. Though it can be challenging for ecologists trained in the frequentist perspective to transition to a Bayesian approach [@Ellison:2004fj; @Hegel:2010gu], some software packages are in development for implementing Bayesian models out-of-the-box in languages like R [e.g., @Vieilledent:2012ty]. Though MCMC methods are computationally very expensive, numerical approximations and analytical solutions can, when available, significantly reduce computational burden [@Golding:2016bt].

A review of recent literature suggests that the majority of ecologists are using nonparametric models for SDM. Of 100 randomly sampled papers from the most recent 4,000 citations in Web of Science that met the query "(Species Distribution Model*) OR (Ecological Niche Model*) OR (Habitat Suitability Model*)", the overwhelming majority utilized techniques that were data-driven. Out of 203 modeling experiments described, 38 were model-driven, 131 were data-drive, and 1 was Bayesian. 33 additional experiments used unsupervised clustering analyses that are not capable of predicting future presence.  Figure [X] shows the distribution of the 47 different models, and their aggregation into the categories described above. Of all algorithms, MaxEnt was the most popular (64 instances). Models in the model-driven category included generalized linear models (15), logistic regression (5) and multiple linear regression (2). Data-driven modeling applications included boosted regression trees (16), generalized additive models (11), genetic algorithms (11), random forests (8), artificial neural nets (6), and multivariate adaptive regression splines (4).

Because of the overwhelming propensity of scholars to employ methods in the second category, I focus my analyses on this class of algorithms. Many authors have alluded to the limitations imposed by computational complexity, though few have estimated those limits precisely. @Elith:2006vt recorded the execution time of the runs they used in their often-cited review of novel SDM techniques, noting execution times of up to several weeks for some modeling algorithms [@Elith:2006vt]. Their longest running model is the genetic algorithms for ruleset production, which they claim took six weeks to converge. Other popular learning models, including boosted regression trees (80 h), generalized additive models (17h), generalized linear models (17h), and MaxEnt (2.75 h), were all shown to be extremely computationally intensive. The authors suggest that performance could be improved if model building was split over multiple processing cores.  While processor speeds have increased since the 2006 analysis, models are still often built sequentially, unable to leverage multiple processors.

In some cases, methodological papers advise against large modeling efforts due to computational limitations. A 2009 review paper in *Trends in Ecology and Evolution* suggests that, when fitting a generalized linear mixed model (GLMM), if a user encounters insufficient computer memory or time limitations, the user should reduce model complexity, perhaps using a subset of the original dataset [@Bolker:2009cs].  Many authors warn of the computational expense of running SDMs, noting that "considerable computational capacity is necessary for the development of models even for a single species" [@Peterson:2003gl]. @Thuiller:2008enb cautions that "limits to the broad application of this approach may be posed ... by the computational challenges encountered in the statistical fitting of complex models."  While better computing infrastructure may alleviate some of this problem, the the computation intensity of SDMs can cause challenges that are difficult to resolve without reducing model complexity.

#### Algorithm Execution Time: Drivers and Measurement
Analyzing the constantly evolving and multifaceted dimensions of computer performance has posed problems for analysts and scholars since the advent of modern computing [@Nordhaus:2001th]. While some figures, such as millions/billions of floating point operations per seconds (MFLOPS/GFLOPS), are used in the supercomputing literature, a computer's performance depends on the way in which it is used [@Lilja:TcjNvdug], rendering such universal metrics incomparable. While it is difficult to effectively and meaningfully quantify a computer's performance, both empirical and theoretical methods of estimating algorithm execution time exist.

Theoretically, it is possible to determine the upper, lower, and average run times using asymptotic complexity analysis. In this exercise, the order of growth of an algorithm's runtime is determined as its input is increased to infinity, so that only first order terms are relevant [@Knuth:1976if]. Asymptotic analysis is useful, because the algorithm that is more efficient asymptotically will typically be the best choice for all but very small inputs [@Cormen:2009uw]. The time complexity of an algorithm is most often applied when considering an algorithm's scalability [@Goldsmith:2007jd]. An estimate of worst case run time (Big-O), can usually be obtained by inspecting the structure of the algorithm and counting how many operations are required when the inputs is sufficiently large [@Cormen:2009uw]. For any given input, however, particularly on real-world programs, the actual runtime will vary [@Cormen:2009uw; @Goldsmith:2007jd].

Empirical complexity studies have attempted to bridge the gap between asymptotic characterization and real programs. These studies use observations of algorithm runtime under different sized parameters and inputs to build models that predict the run time of future model applications and parameterizations.  These techniques seek a method "with the generality of a big-O bound by measuring and statistically modelling [sic] the performance ... across many workloads" [@Goldsmith:2007jd].  @Brewer:1995fh describes an initial attempt to develop a statistical model for the run and compile time of algorithms in a C library.  While most contemporary empirical runtime models use data-driven pattern recognition, linear regression between input size and execution time has been shown to perform well in some cases [@Fink:1998vg]. Empirical complexity models have recently become an important subfield of artificial intelligence and have important applications to algorithm selection [@Hutter:2014cia]. Algorithms for solving very difficult ($NP$-Hard) combinatorial problems, can exhibit high variance between different problem instances.  When a collection of different algorithms are available, empirical runtime modeling can be used to select the model that will most efficiently reach a solution [@LeytonBrown:2003tn; @Hutter:2014du]. @Hutter:2014cia outlines a comprehensive analysis of strategies and methods uses for empirical runtime models in the context of algorithm portfolio optimization. Parameterized algorithms can be treated the same way as nonparametric algorithms, by including model parameters as input features in the execution time model, "notwithstanding the fact that they describe the algorithm rather than the problem instance, and hence are directly controllable by the experimenter" [@Hutter:2014cia]. Nonlinear, tree based methods for empirical performance modeling, including random forests, were shown to be superior to other methods because of their ability to group similar inputs together and fit local responses, so that some large outliers do not interfere with the predictions of other groups [@Hutter:2014cia; @Hutter:2014du].

Concurrently running programs, operating system tasks, and other processes may affect the execution time of a real computer program at any point in time.  Changes in dynamic system state are stochastic and can cause unpredictable, non-linear and non-additive changes in program runtime [@Kalibera:2013kh; @Lilja:TcjNvdug].  Random variation in system state makes deterministic statistical modeling of hardware's influence on execution time difficult.  These variations are a result of the way in which memory access patterns differ in space and time when small changes are made to the operating system state, timing device, or algorithm and its inputs [@Lilja:TcjNvdug], and few attempts have been made to model them explicitly.  @Kalibera:2013kh suggest that models based on benchmarked runtime may provide an accurate estimate of an upper bound of execution time, though due to potentially large, nondeterminstic, system-induced variance in empirical results, it is important to perform the benchmarking experiment many times.  @Dongarra:1987br suggest that a failure to properly characterize the workload, running benchmarks that are too simplistic, or running benchmarks in inconsistent environments can lead to meaningless results.

Despite the challenges, several empirical runtime studies have attempted to include computer hardware's effects on algorithm runtime and have achieved highly accurate results. @Wu:2011es contend that the execution time of a program depends on the complexity of the algorithm and its input data, the static hardware configuration of the resource (e.g., amount and type of RAM, CPU clock rate), and the dynamic system state (e.g., number of processes competing for resources). @Sadjadi:2008fe model execution time as a linear combination of contributions from elements of the computer's hardware characteristics, though multivariate nonlinear systems may be more appropriate to capture some of the complex changes possible in the empirical data [@Wu:2011es].

Previous attempts to model runtimes have relied on the subset of computing components thought to directly affect performance. @Sadjadi:2008fe include CPU rate (GHz) and number of CPU cores, though other studies have also included memory amount and type, buffer size, and CPU cache size [@Wu:2011es]. While increasing the clock rate of a CPU is nearly guaranteed to improve execution time, since it can increase the number of operations able to be processed per unit time, the number of CPU cores can also affect execution time by allowing multiple programs to execute in parallel. To take advantage of multiple processors, algorithms must be specifically designed to partition their execution across multiple cores.  Moreover, the addition of processor cores will only improve performance up to a point, after which all benefits of load sharing will be outweighed by cross-core communication [@Gustafson:1988dh]. Computer memory can also affect a program's runtime by reducing the number of times a computer must retrieve data from the physical storage device (e.g., hard disk) and can improve the the amount of concurrent work able to be done on a machine [@Wu:2011es].

#### Theoretical Problem Formulation

To conceptualize the optimal configuration for an SDM modeling workflow, it is useful to conceptualize the modeling process using the following framework. Here, the workflow is presented as a series of steps that advance a user towards her goal of obtaining scientific insight from an SDM model.  The use of computing resources is essential towards this end, and the cost of these resources is proportional to their power. Computing prices are set by an external computing provider.  As a rational consumer, the modeler will wish to minimize her costs, in both time and money, while maintaining the maximum accuracy under given budgetary constraints.  

1. Consider a pool of computing resources, $H$.  As posited by [@Wu:2011es], at any time $t$, the effective processing power of $H$ is related to both the static and dynamic configuration of its hardware and software.  Thus,
$$H(t) = H_static + H_dynamic + \zeta$$
in which $H(t)$ is the effective processing power, $H_satic$ represents the static, hardware capabilities of the machine that do not change with time and $H_dynamic$ represent the portions of the system that do vary with time. Execution times can vary non-deterministically with hardware due to stochastic changes in system state, so $\zeta$ represents process uncertainty and natural system variability that cannot be included in the model.

2. Consumers of computing services are part of a market driven by supply and demand, and face a costs set by computing providers dictated by the effective computing power provided. Particularly when operating in the cloud computing paradigm, computing providers can be seen as a utility provider [@Foster:2008cl], while in a traditional desktop computing model, the provider of the computers can be seen as a typical rational producer. Figure [X] demonstrates an instance of a Google Cloud Computing Engine's cost surface as a function of memory and CPUs. Notice that as the hardware capabilities of the virtual machines increase (i.e., more CPUs, more memory), the cost to a user of provisioning these resources is increased.

3.  Every user of a modeling application has a particular set of goals for using it in the first place [@Norman:1984fg].  We can conceptualize, for a given model, a finite set of use cases for that model that fall within the bounds of existing or expected use [@Carroll:1999hh; @Rosson:2002vj].  For example, consider a hypothetical scenario that could apply to a typical species distribution model user (use case adapted from [@Smith:2013cs]):

  *Jessica Smith is a land manager at Yellowstone National Park, interested in understanding how Mountain Pine Beetle infestations may change under different anthropogenic climate change scenarios.  Dr. Smith primarily wishes to characterize how the beetle range might change under the three different IPCC emissions pathways [@Moss:2010en], rather than differences between algorithms or characterization of modeling uncertainty.*

From this brief scenario, we are able to infer that the model goal is to model a single taxonomic group (*Dendroctonus ponderosae*), in a single area of known size (Yellowstone National Park, ~3,500 mi^2), under three climate scenarios.  She requires only a single modeling algorithm, as she is not interested in assessing the differences in the outputs from multiple SDM models.

To generalize this example, I introduce $U$ -- a vector of characteristics that fully describe the user's goals in the scenario. The components of $U$ include a number of experiments to undertake, as well as user traits, such as experience with the model and familiarity with the interface employed, motivation, skill, and accuracy required. $U$ also contains a list of experiments desired by the user, which specifies the number and character of modeling runs the users wants to do. Each element in the experiments list contains enough information to specify the inputs for a single model run, such as modeling algorithm, spatial resolution, amount of data to be used as training data, availability of environmental covariates, and number of past or future time periods to project onto.

4. The time to compute a given algorithm with a given set of inputs is proportional to $H(t)$, the effective computing power. However, in addition to computing the model, the user must also undertake a number of other pre- and post-processing steps.  The total time elapsed during a modeling experiment can be expressed as
$$ T_{model} = T_{Input} + T_{Prep} + T_{Compute} + T_{Output} + T_{Interp}$$
In this formulation, $T_{Input}$ represents the portion of time that is spent by user gathering the resources needed to model. In a species distribution modeling context, this term represents the time needed to find and download occurrence points and find and download predictor variables. $T_{Input}$ can be thought of as a function of computing resources available to the user (how fast can data be downloaded?) and the experiment (what is the data?). $T_{Prep}$ is the time required by the modeler to prepare the data for entry into an algorithm.  In this case, $T_{Prep}$ time might include data cleaning, projection, and conversion, as well as setting up and configuring computing platforms and environments, like R.  This component can vary widely between modelers and between model applications, based on data source and quality, user skill and motivation, and the interface and equipment user has on hand.  [@Elith:2006vt] notes the potential impact of how experienced a user is with a model on the modeling time and results. $T_{Output}$ includes the time it takes to return the output from the computation to the user, which may be non-trivial if the model is run on a set of remote resources and the output must be downloaded over a network to reach the client's machine. Finally, $T_{Interp}$ represents the amount of time spent by the user evaluating model output and determining whether her goals were met during the modeling process.

5.  Single experiments can be combined together to form workflows, so that a user's time-to-goal for a workflow of $N$ modeling experiments can be expressed as
$$T_g = \sum\limits_{i=1}^NT_{Model}(Experiment_i, H(t))$$

6.  Combining equations from (2) and (5), we find the total time cost of a modeling experiment is the sum of total time of spent modeling and the total monetary cost is the cost of provisioning computing resources for this time. Thus, we derive a multivariate cost function for a modeling scenario that takes into account both time spent modeling and the cost of provisioning resources:
$$C(U, H(t))= f(T_g(U, H(t)), C_{Compute}(H(t)))$$

7. Each user-based scenario, $U$, will have its own cost curve that's subject to both the particular characteristics of the workflow and the specific cost surface imposed by the computing provider. If we select a single element from the finite set of all possible $U$'s, and call it $U*$, we obtain a unique cost function for this set of activities that depends only on the computing resources used to fit the model. The cost surface, $C$, is defined for the set of all real computing solutions, however, some may be suboptimal for that particular experiment. Indeed, the optimal solution for $U*$ is multidimensional minimum of $C$ that maximizes model accuracy subject to any constraints imposed by the user. Such constraints may include limited budget, minimum required accuracy, or maximum number of data points.

Without any user constraints, we can determine the unconstrained optimal configuration using a three step process. First, we conceptualize all inputs into the cost equation as orthogonal axes in $p$-dimensional space, where $p$ is the number of characteristics in $U$ plus the number of hardware variables in $H$. Using the model for computing $t_g$ and a regular sampling method, we calculate the time to compute each experiment within a finite bounded hypercube. Because the computing resources, $H(t)$, should have no impact on the accuracy obtained from the SDM model, we can estimate the accuracy of the model $U*$, the algorithm inputs, for every experiment contained within the hypercube. We then search those results for the combination of $U$ elements that will result in the highest accuracy.

Second, we slice the hypercube on this set of $U$ values that will result in the highest accuracy model. We are left with a hypercube of reduced dimensionality, who axes are the orthogonal components of $H$.  For each of potential combinations of $H$, which may be limited by the computing provider and physical reality, we calculate the cost of that experiment in both time and money, using $T_g$.  

Finally, we put each of the points calculated in the second step onto two new orthogonal axes, time and money.  The ideal model experiment would lie at the origin of these two axes: no time, no money.  However, it is economically and physically unrealistic to think that any model will actually lie at this ideal origin: no computing time is free and every modeling experiment can be broken down into simple instruction sets that must be executed by the computer.  Therefore, no matter how fast the computer is, some time will still be expended while executing these commands.  Thus, the optimal experiment configuration is the one that lies closest to this unobtainable optimal point.  We find this point by calculating the distance between the origin and all candidate resource sets, and select the candidate with the smallest distance to the origin.

If a user constraint is placed on the optimization *a priori*, the same technique can be applied to subset the hypercube to contain only candidate solutions that meet the user's criteria.  For example, if the user has a minimum accuracy under which the results of the model are not acceptable, the first step proceeds as described above.  The hypercube is then searched for all potential configurations that might result in an accuracy greater than or equal to the user defined threshold. If no such experiment exists, then the user must relax her constraints or adjust her modeling protocol.  If one or more solutions exist, the hypercube is subset to include only these candidates.  Then, the second and third steps of optimizing to maximize accuracy while minimizing cost proceeds.  

Similarly, a user may place a constraint on the total amount of money or time may be spent on a modeling workflow. In this case, the user would then wish to identify the configuration that maximizes the accuracy while not exceeding the monetary to time threshold.  First, the cost of all experiments in the hypercube is estimated using $T_g$. The set of candidate solutions is truncated to include only those that fall within the user's predefined limits, if any exist.  Finally, the reduced-space hypercube is searched for the configuration that yields the highest accuracy.  

### Methods
#### Approach
My method (1) develops an empirical dataset of SDM runtime and accuracy under systematically varied algorithm inputs and hardware capabilities, (2) builds predictive models of runtime and accuracy using a Bayesian regression tree framework, and (3) identifies the optimal configuration of algorithm inputs and hardware capabilities that maximizes accuracy while minimizing time and cost.

#### Limitations and Assumptions
This approach has several important limitations. While a real SDM workflow contains may contain important tasks other than the modeling itself ($T_{compute}$  above), I focus in this thesis on only computing time in the perscription on the optimal modeling configuration.  The additional resources used to gather and prepare the data and interpret the results are excluded in this study because they are highly variable, based on qualitative differences between users, including their individual training, skill, and motivation, and can depend on factors that are difficult model.  Furthermore, I exclude the time to manipulate the data into a modeling-ready form, and the resources needed to output the model results to the user's workspace in order to keep this project to a manageable scope.  While excluding these terms improve the analytical tractability of this problem, it prevents the prediction of the true optimal value.  Future work could be pointed towards efficient modeling these additional workflow stages to allow their incorporation into the optimal prediction model.

A second limitation is that my analysis is limited to virtual computing instances hosted on Google Cloud Computing Engine (GCE), rather than real-world, physical machines. This design was chosen to add validity to the measurements of computing time, by providing them with a consistent environment unaffected by other tasks or concurrent programs [@Dongarra:1987br].  However, because real-world machines do have many concurrent and interacting processes, this choice may bias the results towards faster predictions than real-world workflows would be capable of.  It is difficult to characterize the particular use-cases that would cause a real-world workflow to execute more slowly than the virtual instances. While the use of dedicated virtual instances may affect the results, it simplifies the problem into one that can be modeled without an assumption of user behavior.  An additional consequence of using the GCE platform is the inability to systematically vary CPU clock rate as a hardware parameter. Algorithmic runtime is directly related to clock rate, as it limits the number of instructions that can be completed per unit time. By using GCE instances, I am limited to only the CPUs provided by Google -- currently, a state-of-the-art 2.6 GHz Intel Xeon E5 processor.  While I am not able to systematically change the rate experimentally, my results will not be affected by using machines with different CPU rates, which is a possibility if physical machines are used in place of virtual instances.

This works is limited to the analysis of SDMs in the data-driven category, and therefore does not attempt to characterize the computational time of either parametric or Bayesian approaches. Systematic literature review suggests that a majority of SDM users use these or similar machine learning methods.  By the same logic, I limit my analyses to popular ```R``` implementations of these SDMs, which increases the probability that the code libraries are well documented and open source.  While there are known limitations to the language design and speed of ```R```[e.g., @Anonymous:W1-t_FiC], the platform is the most widely used for SDM analyses. Maxent, the most popular modeling algorithm in recent years, is excluded because (1) it is written primarily in Java, with bindings linking it to the R platform and (2) it is not open source, it is distributed as a black-box code library.

Finally, this study was strongly limited by computational cost, both in financial and time contexts. Each SDM configuration was tested between five and ten times to ensure robust results.  Experimental runtime results ranged from less than five seconds to greater than five hours. In order to gather enough data to develop a robust predictive model, I curtailed the number of very long running models.  Similarly, I limited my experimentation on virtual servers with very high hardware capabilities (many CPUs and/or high memory allocations). The cost of these instances was more than an order of magnitude of larger than smaller instances (> $1/hr), so experimentation was shifted to less costly servers with less powerful hardware. The effects of this data limitation include low probability of prediction for high-powered machines (because there are relative few data points in the experimental set) and low confidence in model results with very high data requirements.  Additional data collection, particularly on high powered machines and on models with large data requirements, may improve the robustness of my results.

#### Data Collection

#### SDM Data Preparation
I collected data on the execution time and accuracy of four SDM algorithms that have shown competitive accuracy results in the literature: multivariate adaptive regression splines (MARS) [@Leathwick:2006bd], gradient boosted regression trees (GBM-BRT)[@Elith:2008el; @Friedman:2001db; @Natekin:2013ji], generalized additive models (GAM) [@Yee:1991jb; @Guisan:2002dc], and Random Forests [@Breiman:X1hY-WAY; @Elith:2009dl]. All of the SDMs were run in the R statistical environment [@Rcore] with standard packages for fitting these models.  GBM-BRT tree models were fit using the ```dismo``` package version 1.1-1 [@Hijmans:2012ej], GAMs  using the ```gam``` package, version 1.12 [@gam], MARS using the ```earth``` package version 4.4.4 [@earth], and random forests with the package ```randomForest``` [@rf].  These standard packages were chosen for their popularity in the field, and are designed to represent normal use cases in species distribution modeling workflows. Due to complexities inherent in specific implementations, it is likely that these results are highly sensitive to ```R```-package implementation details.

Each SDM simulation was fit using fossil pollen data obtained from the Neotoma Paleoecological Database in April 2016. All records for the genera *Picea* (spruce), *Quercus* (oak), *Tsuga* (hemlock), and *Betula* (birch) were downloaded using the ```neotoma``` ```R``` package [@Goring:2015cr].  Global Quaternary occurrence records were filtered to only include those in the most recent 22,000 years and located in North America. For each record, the latitude, longitude, age, and relative abundance of the taxon were retained and stored in a comma separated value format file.

The climatic covariates for fitting the SDMs variables from downscaled and debiased Community Climate System Version 3 (CCSM3) model simulations for North America [@Lorenz:2016hu].  The post-processed model output was obtained in NetCDF format with a 0.5 degree spatial resolution and decadal temporal resolution for the last 22,000 years [@dryad_1597g_2].  Bioclimatic variables (BV) [@ODonnell:2012ww] were calculated at each timestep using the biovars function in the ```dismo``` R package [@Hijmans:2012ej]. The BV values were extracted at each space-time location in the fossil occurrences dataset.

The occurrence-BV dataset was filtered to include only the six least correlated BV predictors, a common practice when using learning algorithms. Collinearity among predictors can decrease model performance, can cause highly volatile model results, and "in truly extreme cases, prevent the numerical solution of a model" [@Obrien:2007ir]. The Variance Inflation Factor (VIF) was calculated and used to determine variable collinearity using the ```usdm``` package.  Based on the result of this analysis, I retained the five least intercorrelated variables, leaving a maximum pairwise correlation of 0.51.  The final dataset included BV2 (mean diurnal temperature range), BV7 (annual temperature range), BV8 (mean temperature of wettest quarter), BV15 (precipitation of warmest quarter) and BV18 (precipitation of driest quarter).

Future climate layers for 2100 CE were obtained from the CMIP project, HadCM3 climate model.  These layers represent modeled climate variables under the UN IPCC RCP 8.5, a scenario that assumes high population, moderate economic growth, and a sustained dependence on fossil fuels [@Riahi:2011dk]. These layers were used to compute bioclimatic variables and resampled to various resolutions.

#### Computing Infrastructure
Google Cloud Compute Enginge (GCE) was used to compute all SDM simulations. A popular Infrastructure-as-a-Service (IaaS) provider [@Hassan:2011uh], the platform is capable of provisioning a wide array of virtual server instances, from basic (1 CPU, 0.6 GB RAM) to exceptionally powerful (32 CPU, 208 GB RAM).  The platform also provides a set of application programming interfaces (APIs) that enable workflow automation, as well as a graphical user interface (GUI) for interactive resource management. Google's platform was chosen over other public cloud vendors because of its ability to create 'custom' instance types that conform to user defined specifications.  Other vendors (e.g., Amazon Web Services) provide a larger number of predefined instance types, but do not allow you to create an instance with arbitrary hardware capabilities. By providing the ability to create custom types, Google's service best fits my experimental design and lets me avoid using software to artificially alter hardware parameters.

I set up a distributed computing system to run all simulations that featured a centralized database node and multiple distributed computing nodes. One Master Node hosted a MySQL database and a python control script that ran full time, while a pool of computing nodes designed only for computing were provisioned and decommissioned as needed.  The compute nodes, which ran a Debian Linux operating system, were supplied enough information to simulate an SDM with the required algorithm inputs, while the Master Node control script managed the progress of the project as a whole. A node.js script provided programmatic access to real-time database content and was used to link the master node and the worker nodes.

An outline of the system is described below and is illustrated in Figure [X].

For each experimental simulation:

1. First, a pool of computing nodes is assembled.  The Master Node control script queries the central database for experiments that have yet to be completed or threw an error the last time they were run. The database responds, via the node API, with a JSON object that contains the hardware parameters required by the next experiment.  The python script parses the response and uses the 'gcloud' tools associated with the GCE to create a pool of virtual instances that have the necessary amount of memory and number of CPUs.

2. Each node in the pool automatically begins running a startup script that specifies model setup and execution.  First, a number of system-wide software packages, including R and Git are installed on the new instance.  Git is subsequently used to clone the most recent version of the project repository (hosted as a private repository on GitHub) which supplies all data and code necessary to compute an SDM.  Once all required R packages have been installed, the model script is initialized as a new R session via the command line tool ```Rscript```. The R program queries the central database, identifying itself as a computing node with $x$ cores and $y$ memory. The database responds with the parameters needed to run a single experiment on the given infrastructure.  The script then loads the necessary variables and executes the SDM simulation.  When finished, it reports the results to the database and marks the experiment as completed.  Experiments are continued until there are no more experiments that can be computed on an instance of the given capabilities. If an instance is preempted by the system or otherwise crashes, a shutdown script will be executed, marking the in-progress experiment as interrupted -- signaling other worker nodes that it should be attempted again at a later time.

3.  Because Google charges by the minute for the use of their virtual machines, instances must be torn down as soon as possible. As the computing nodes execute the experiments, the Master Node repeatedly polls the central database to determine the current position within the experiment table, determining the percentage completion of the current group of experiments.  Once the group is complete, Master Node will use the ```gcloud``` tools to decommission the individual instances, and delete the instance pool and the instance template that was used to create each virtual worker node.  After this, the Master Node is the only instance that remains online. At this point, Master Node returns to Step 1 to build a new pool of instances for a new hardware configuration.

#### SDM Model Protocol
The experimental parameters were communicated to the worker node by the central database.  Each computing node parsed the database's response and a new session was initialized.  First, the set of occurrences corresponding with the species to be modeled were loaded from the disk, then randomly partitioned two disjoint subsets, a training set of $N$ occurrences, and a testing set of 20% of the total number (*Picea* = 9935, *Quercus* = 8953, *Betula* = 10226, *Tsuga* = 7140). All occurrences were converted from relative pollen abundance to binary presence-absence values using the Nieto-Lugilde [@NietoLugilde:2015bza] method for determining local presence from fossil pollen records.  The training set was then sent to the specified SDM model where a learner of that type was fit using $p$ predictors. The fit SDM was used to predict that taxon's range in 2100 AD under the RCP 8.5 scenario. The testing set was used to assess the model's skill in discriminating presence and absence. During the execution, separate times are recorded for model fitting, prediction, and accuracy calculation, as well as the total time -- the sum of the three.  Furthermore, the Area Under the Receiver Operator Curve (AUC) accuracy metric, a popular method of evaluating logistic output (but see [@Lobo:2008du]), is computed and stored with the timing results.  There is no database I/O inside of the timing script, so results should not be biased by network connection or context switching. Learning parameters (learning rate, number of trees, tree complexity, etc) were held constant for all runs. Timing was done within R using the ```proc.time``` function.

Due to budgetary constraints, and thus inability to cover all possible parameterizations, experiments were divided into several categories in which different variables were systematically altered to determine sensitivity. This approach aimed to capture as much within-parameter variance as possible while simultaneously catpuring the influence of interactions between variables.  One series of simulations was run for all SDMs using a small subset of algorithm inputs (training examples and memory) on a wide variety of VM types (core-memory combinations). Five additional, separate analyses were completed to determine the sensitivity to specific parameterizations.  Because execution time can vary non-linearly when the hardware parameters are changed, I tested as many combinations of memory and CPUs as possible.  On each computer, a standard set of 160 experiments were run for each sequential SDM (MARS, GBM-BRT, GAM), including four spatial extents, four training example sets, and 10 replicates of each. All experiments were done on the *Picea* data set.

Individual, target experiment sets were done to assess the contribution and sensitivity of individual or sets of parameterizations.  While no theoretical difference would suggest that execution times should vary between different taxa, a set of 191 model runs were done to evaluate whether differences exist in practice. The model uses aspatial input sets, which suggests that geographic range, abundance, or taxon-specific patterns should not bias the results of the experiments.  Using all four taxa on six different VM instance types, inter-taxonomic sensitivity was recorded.  The number of training examples and spatial resolutions was held constant for these runs.

Empirical Performance Models theory suggests that specific algorithm parameterizations will take longer to execute than others [@Cannon:2007ge]. SDMs have a large number of potential parameters with which to alter, though many ecologists use the defaults, or packages that make it difficult to change the default parameter values (e.g., ```dismo``` [@Hijmans:2012ej]). To assess the magnitude of changes in execution time due to different parameterizations, the GBM-BRT was tested on a set of 70 different combinations of tree complexity and learning rate. The learning rate parameter of the GBM-BRT model is a shrinkage parameter that reduces the impact of each additional fitted tree, motivated by the boosting paradigm of fitting a model with many small models rather than fewer large trees. If one of the greedy iterations does not improve model fit, the contribution of that iteration can be easily reversed in the subsequent iterations [@Natekin:2013ji]. The tree complexity parameter controls whether interactions between predictors are fitted.  If tree complexity is 1, the tree will be an additive model with no interactions.  A tree complexity of $N$ will produce a model with $N$-way interactions between variables[@Elith:2008el]. These two variables together control the total number of trees needed to fit the model [@Elith:2008el].  

To assess the relative performance of parallel methods over sequential models, RF SDMs were fit both sequentially and in parallel on instances up to 24 CPU cores. Spatial resolution, memory, and taxon were held constant while number of ensemble members and number of training examples were systematically altered for each core. In total, 3576 random forests were fit using the randomForest function with the foreach package providing parallelization support.  Sequential runs were fit using the same function but with number of cores set to only 1.

In addition to performance gains made by increasing the number of CPU cores and leveraging parallel methods, increasing instance memory should improve performance for very large datasets.  Using simulated datasets, I attempted to assess the performance of the model when faced with more than 1 million input examples.  R has little support for high memory tasks, and these tests routinely crashed the computer when trying to fit the SDM due to inability to allocate memory space.

Finally, I evaluated the effect of varying the number of predictors on the execution time of the algorithm. The literature on theoretical complexity of algorithms (e.g., [@Hastie:2009up]) often characterize the complexity of machine learning algorithms in terms of both number of training examples and number of features in each example. I systematically modified the number of training examples between 1000 and 11000 and the number of predictors on all SDM classes, to get an understanding of how the two parameters interact.  Because both of these algorithms run serially, they can safely be run on a single processor without the need for estimating the effects of additional cores.

#### Modeling Execution Time and Accuracy

To model algorithm execution time, I utilized a Bayesian additive regression tree framework, which provides the ability to investigate the mean prediction for each node, as well as gain an entire probability distribution for each prediction point.  Tree based models, including additive regression trees and random forests, have seen successful use in the past for empirical performance models, as they are able to effectively model local responses to specific combinations of variables [@Hutter:2014cia]. Additive regression trees have even greater than single trees to capture interactions between variables and both linear and non-linear effects in the estimated function[@Kapelner:2016tf]. By placing the model-building in a Bayesian context, the response at each leaf node will be a probability distribution, rather than the single maximum likelihood estimate.  As @Kapelner:2016tf describe, the goal is to estimate $f( \boldsymbol{X}) + \boldsymbol{\epsilon}$ from a training set of data $ \boldsymbol{Y}$.  To do this, using Bayesian additive regression trees, approximate the function as

$$ f(\boldsymbol{X})	\approx \mathcal{T}_1^\mathcal{M}(\boldsymbol{X}) + \mathcal{T}_2^\mathcal{M}(\boldsymbol{X}) + ... + \mathcal{T}_m^\mathcal{M}(\boldsymbol{X}) + \boldsymbol{\epsilon}$$

$$\boldsymbol{\epsilon} \sim \mathcal{N}_m(0, \sigma^2\boldsymbol{I}_n)$$

In this framework, \boldsymbol{X} is the training set of predictor features, \boldsymbol{\epsilon} is the unexplained variance remaining.  The model is composed as a sum  of $m$ trees, $\mathcal{T}^\mathcal{M}$, with parameters at each leaf node given by $\mathcal{M}$, and the tree structure $\mathcal{T}$.  The $\mathcal{T}$ tree structure contains information about the splitting rules at each internal (non-leaf) node.  $\mathcal{M}$ is the tree's terminal node parameters, $\mathcal{M}_t = {\mu_{t1}, \mu_{t2}, ..., \mu_{t_b}}$, where $b$ is the total number of terminal nodes in the tree.  Each observation's value is found by navigating through each splitting rule at all internal nodes, summed over all $M$ trees.  As a Bayesian model, there is a prior distribution placed on the leaf parameters and tree structures and the noise term, which limits a single model term from dominating the model fit.  The prior placed on tree structure enforces that trees must be shallow, with few internal nodes, which is ideal when building additive models, because it can reduce the risk of overfitting. The leaf node parameters have a normal prior on them which regularizes the predicted response by drawing the predictions towards the response mean.  The noise variance prior is chosen so that 90% of it's probability mass lies below the expected mean squared error from an ordinary least squared regression, which encourages the error term to be smaller than in an ordinary regression.

The regression trees were fit with the ```bartMachine``` R package, version 1.2.3 [@Bleich:2016ub].  For each SDM class, the collected data was randomly split in a training set (80%) and testing set (20%).  The training set was used to fit the Bayesian regression tree model, using the default priors suggested by the authors [@Kapelner:2016tf]. The analysis performed 1250 Monte Carlo simulations, of which the first 250 were discarded as burn-in, leaving 1000 posterior samples.  Each Markov Chain Monte Carlo iteration built an entire additive model of 50 trees.

The log transform of SDM execution time was used a response variable, used because the transform does not allow for negative predictions, which are possible under non-transformed inputs but physically impossible for runtime, and have been shown to be more accurate when observed responses span large ranges [@Hutter:2014cia]. In the MARS, GAM, and RF SDM classes, the predictor set contained five features: number of training examples, number of cells in the raster grid used for prediction, number of CPU cores, gigabytes of RAM, and number of environmental covariates. In the GBM-BRT SDM case, two additional features were used: tree complexity and learning rate of the algorithm. The accuracy models were fit using the same predictor features set, and the response was the area under the receiver operator curve (AUC).  While other accuracy metrics are encountered in the literature, the AUC is a particularly common method of evaluating presence-absence classification accuracy.  

#### Model Evaluation
Models were evaluated using the 20% holdout set that was not included in model fitting.  Using this testing set, the predictive skill of the execution time and accuracy models were evaluated using mean squared error (MSE) and the model's $r^2$ value between observed and predicted values.  The $r^2$ value can be interpreted as the percentage of variance that is explained using the model, and the MSE is a metric of the average deviation of the predicted values from the observed, real values.  The model's fit was evaluated primarily using the posterior mean returned from the bartMachine predict function.  The expected variance in the predictions was also considered, by assessing the standard deviation of the posterior.  This can be interpreted as the confidence in the position of the mean used in the previous two metrics.  Finally, visual assessment of model results was done by plotting the predicted values against the observed data, which, if perfectly predicted, should fall along the y=x line.

Model drivers were evaluated by cross validating the performance of a separately built model using only a subset of the predictors.  Each predictor was left out of a model fit, and the $r^2$ of the subset model was evaluated in its absence. The reduction in model skill by removing the additional term is interpreted as the importance of that variable to the model.  Variables with a large reduction in model skill are very important, while variables with small reductions are only slightly important to the overall model fitting. A reduction in $r^2$ is then simply interpreted as the reduction in data variance explained by the model when that predictor is removed.

#### Optimal Prediction

##### Unconstrained Optimization
To predict the optimal configuration, the runtime and accuracy prediction models for each SDM class were used to generate predictions of execution time and accuracy for a large set of potential experiments.  Ideally, the model would predict the value at every real combination of computing hardware and algorithm inputs, essentially describing every possible experiment.  Although there are a finite number of hardware configurations on the market today, there are an infinite number of ways to combine these with sets of algorithm inputs, which makes the problem intractable. Therefore, it is essential to reduce the scope of the prediction into a large, but finite, selection of possible experimental configurations. To accomplish this, I regularly sampled values along each orthogonal axis of the experimental components, from very small (near zero) values, to very large, but still reasonable, values.  This allowed to to partition the infinite space into a real number of combinations to work with.  Each candidate configuration in the reduced space was assigned a dollar-per-hour rate using the Google Cloud Engine pricing scheme, based on its number of CPUs and amount of RAM.  Using this scheme allows the prediction to be recomputed should a new pricing scheme become available. For every scenario in the candidate set, the execution time (in seconds) and accuracy (in AUC) was predicted.  Finally, the execution time was multiplied by the dollar-per-hour rate to obtain a monetary cost for that experiment.

The candidate experiment generation was done using the expand.grid function in R and the model prediction was completed on a 16-core, 60 GB RAM instance on GCE.  Predictions were spread across all 16 cores using the parallel package and the mclapply functionality provided in that package.  Once each experiment in the hypercube had been predicted, it was possible to predict the optimal configuration under a variety of optimality criteria.


| Orthogonal Axis    | Minimum Value     | Maximum Value     | Step Size     | Number of Steps     |
| :------------- | :------------- | :------------- | :------------- |:------------- |
| Training Examples      | 0       | 500000      | 10000      | 51      |
| Cells      | 10000      | 1000000       | 100000       | 10       |
| Number of Covariates      | 1       | 5       | 1      | 5      |
| CPU Cores     | 1      | 24       | 1     | 24      |
| Memory (GB)     | 2      | 22      | 2       | 12     |
| Learning Rate *(GBM-BRT Only)*     | 0.001       | 0.11  |  0.005 | 22      |
| Tree Complexity *(GBM-BRT Only)*     | 1       | 5       | 1      | 5      |

To calculate the unconstrained optimal configuration, the predicted values for execution time and accuracy were predicted for all values in the hypercube set.  Using a bivariate interpolation method, the accuracy for every combination of algorithm inputs (training examples and number of predictors) was estimated. This entire space was searched for the minimal set of algorithm inputs that would result in the maximum accuracy the maximum accuracy.  The minimal set of inputs was defined as the one with the least training examples and the least number of predictors.

This maximum-accuracy point, because it is only defined in terms of algorithm inputs, can be calculated on any combination of computer hardware. Holding the algorithm inputs, and thus, the accuracy, fixed at the maximum combination means that there are a number of different costs and times that will result when this algorithm is calculated, due to inherent differences in the hardware capabilities of each configuration. Using data on 287 computing configurations, the execution time model was used to estimate the cost, in seconds and dollars, for each.  Under the Bayesian framework, the model was run 1000 iterations to get a posterior estimate of predicted time.  Each time sample in the posterior was multiplied by the configuration's rate per second to obtain a cost for obtaining the accuracy maximizing point.

The posterior distributions for time and cost were then plotted out on time and cost as orthogonal axes.  Using the ```dist``` function in R, the euclidean distance between each sample in the posterior and the origin of the two axes was calculated to arrive at a density of potential distances for each hardware configuration. To assess the potential for similarly-distributed configurations that might compete for the lowest-cost and lowest-time, 25 distributions with the lowest means were selected as potential optimal candidates. Using hierarchical agglomerative cluster (```hclust```), the posterior distributions of distance were clustered according to their mean and standard deviation.  The optimal point should both have a low mean, and a low standard deviation, and thus, low uncertainty.  The top-25 potential candidate distributions were clustered hierarchically, and then split into distinct clusters at a dissimilarity of 1.  Candidates with dissimilarity in their means and standard deviations larger than one were considered statistically different, and those within a dissimilarity of 1 were considered statistically the same.   The cluster with the joint lowest mean distance to the time-cost origin and lowest mean posterior standard deviation was selected as the optimal cluster, and each member of that cluster was considered equally optimal.

##### Data-Constrained Optimization
Constrained optimization routines proceeded very similarly to the unconstrained analyses above, but a prior constraint was placed on the space to subset it to only potential scenarios that would meet the constraint. A data-constrained optimization was performed by first slicing the accuracy space, as defined by algorithm variables, into reflect only those combinations of algorithm inputs with enough data to meet the constraint.  For example, if the constraint called for a hard limit of 500 training examples, the accuracy maximization search would not search any values beyond 500o training examples.  Similarly, the number of predictors could be limited if there were few covariates available for the researcher to utilize. Once the accuracy space had been subsetted, it was searched to find the accuracy-maximizing space within it. Once found, the analysis proceeded as in an unconstrained analysis, by fixing inputs to maximize accuracy and then minimizing time and cost.

##### Cost-Constrained Optimization
To calculate a cost- or time-constrained optimal configuration for a given SDM, first, the execution time model for that SDM was used to calculate a regular set of values for average execution time under various algorithm input combinations.  Using bivariate interpolation, these values were interpolated across the entire space so each distinct combination of values had its own predicted time.  Since the algorithm inputs, number of training examples and number of covariates are strong model drivers, these predictions give a time surface, which is subsequently truncated at the user's threshold of time and/or cost.  Once the time space has been subset, its boundaries are used to subset accuracy space.  Next, the accuracy-maximizing point within the time-subset accuracy space is found. As in the data-constrained optimization procedure, the execution time model is used to find the posterior estimates for each hardware type and the time-and-cost minimizing cluster of points.


### Results
#### Model Performance

While the results varied by SDM class, the predictive models for both time and accuracy showed considerable skill.  For execution time, the most sucessful models were those for GBM-BRT and MARS, which both showed $r^2$ values of approximately 0.96.  This is a surprising amount of variance able to be explained by a farily simple model with only five predictors. The MSE on both of these models was just over 1 second^2 (~ 0.06 ln(seconds)^2), indicating great ability to correctly predict to the observed value.  The GAM and RF models showed slightly poorer performance.  The variance explained by the GAM model was only 52.1%, while the RF model explained only 58%.  While this is still a significant amount, there is still a large portion the the variance in these models that is not explained by the predictor set included here.  Despite this, the GAM model still demonstrated a very low MSE (0.0121 ln(seconds)^2).  The RF SDMs executed much more slowly, and thus, showed a larger MSE (~0.64 ln(seconds)^2), indeed, the worst MSE of all the SDMs.  The variance of the posterior samples shows that most of these estimates are tightly constrained.  The posterior standard deviation for GAM, GBM-BRT, and MARS were between 0.01 and 0.035 log-seconds, which is very small when compared to the mean error on the prediction. In addition to poor predictive skill, the posterior standard deviation of the RF model is less well constrained, showing higher uncertainty on the predictions from the model.  This model is closer to 0.1 log-seconds in prediction variance. Figure [X] shows the model skill by relating predicted to observed values on the holdout testing set for the execution time model

One possible reason for the lower predictive skill for the GAM model is that it is relatively fast to converge. Most GAM models converged within 10 seconds.  The higher MSE may be due to low-level system processes that cannot be incorporated in the model that increase the variance of execution time at these time scales.  At larger time scales, these small-scale processes may begin to have less of an effect (minutes to hours). Both the GAM and RF models are fit with smaller datasets than the the GBM-BRT and MARS models, which may partially explain their loss of performance and the RF model's high posterior variance. With more data, perhaps these models would be able to better model particular variable combinations, and more precisely predict future values.  

Nonetheless, these models explained a majority of the variance in the SDM runtime.  The stochastic variance in benchmarks that is often suggested to cause problems when modeling algorithm execution time is not a significant barrier to modeling SDM runtime.  The combination of algorithm input and hardware capabilities together seem sufficient to yield effective predictions for runtime.

| Model    | nTrain | nTest | MSE    | $r^2$    | Posterior SD |
| :------------- | :------| :------| :------------- | :------------- | :------------- |
| GBM-BRT       | 9256| 2314| 0.0646       | 0.9615       | 0.0257 |
| GAM      |  2636| 659| 0.0121     |  0.5213      | 0.01069 |
| MARS       |  6632| 1657| 0.0561        | 0.9648       | 0.03397|
| RF       |  2861| 715| 0.64824      | 0.5851     | 0.10174 |
*Table X* shows the results of the evaluation of runtime model skill.

Given the large number of parameters that can be tinkered with in most implementations of learning algorithms in R, it is surprising how well SDM accuracy can be modeled using this framework.  All models explained more than 87% of the data's variance.  The best performing models was the RF model, with an $r^2$ of of 0.98 and a MSE of less than 3.5x10^-5 AUC.  Accuracy for other models performed high skillfully as well. The second best of the four models was the MARS model, which explained 94% of the data's variance and had an mse of 0.00016.  GAM and GBM-BRT models performed similarly, each with an $r^2$ just under 0.9.  All of the model show fairly constrained posterior estimates on their predictions.  Not only are these models accurate, there is very little uncertainty at each leaf node about the value of the response variable, further supporting that this set of predictors are sufficient to fully model accuracy.  Figure [X] shows the predictive skill of the accuracy model.

| Model    | nTrain | nTest | MSE    | $r^2$    | Posterior SD |
| :------------- | :------| :------| :------------- | :------------- | :------------- |
| GBM-BRT       | 9256| 2314| 0.000245       | 0.8748       | 0.0012 |
| GAM      |  2636| 659| 0.000718    | 0.8993     | 0.004998|
| MARS       |  6632| 1657| 0.000168      | 0.94175       |0.001421 |
| RF       |  2861| 715| 0.000034       | 0.9818     |  0.00062|
*Table X* shows the results of the evaluation of accuracy model skill.

#### Model Drivers
Upon investigation of the drivers of SDM runtime and accuracy, it is clear that data volume can often be significant contributor to both accuracy and long run times.  

| Predictor    | GBM-BRT    | GAM     | MARS    | RF     |
| :------------- | :------------- | :------------- | :------------- | :------------- |
| Number of Predictors       | 0.0138269164      | 0.00002921527     | 0 | 0 |
| Number of CPUs |0.0007196373| 0.00357912281| 0.00360559534 | 0.1100450350 |
| Memory (GB) | 0.0014692250 | 0.00077365222 | 0.02060023091 | 0.0104282285 |
| Number of Training Examples| 0.7459094228 | 0.00056633610 | 0.33670908117 | 0.4200279389|
| Number of Cells |  0.0592688780 | 0.37449462087 | 0.04663870144 | 0.0004306765|
| Learning Rate | 0.0003260443 | NA | NA | NA |
| Tree Complexity | 0.0281510594 | NA | NA | NA |
| Parallel | NA | NA | NA | 0.0006440635 |
| Sequential | NA | NA | NA | 0.0021486491|

*Table X* shows the reduction in explained variance ($r^2$) when a model fit will all terms except one is compared to the full model, for an execution time model.

| Predictor    | GBM-BRT    | GAM     | MARS    | RF     |
| :------------- | :------------- | :------------- | :------------- | :------------- |
| Number of Predictors       | 0.0973334060    | 0.00002921527     | 0.2875883935 | 0.38990130804|
| Number of CPUs |0.0006968174| 0 | 0.0005195602  | 0|
| Memory (GB) | 0.0001511123| 0 | 0.0028887021 | 0 |
| Number of Training Examples| 0.6949723552| 0 | 0.2311663197  | 0.44445147018  |
| Number of Cells |  0.0002975617 | 0|  0.0001914743 | 0.00003363167 |
| Learning Rate | 0.0038748993 | NA | NA | NA |
| Tree Complexity | 0.0047865705  | NA | NA | NA |

*Table X* shows the reduction in explained variance ($r^2$) when a model fit will all terms except one is compared to the full model, for an accuracy model.

The drivers of model runtime are similar for GBM-BRT, GAM, and RF, and show at least a small relationship with the theoretical underpinnings of how these models are computed. These models rely in a very large part on the number of training examples used to fit the data. This is very logical, as the more data is used to fit the model, the more computation must be done during training. Indeed, in the GBM-BRT model, the removal of the number of training examples reduces model $r^2$ by nearly 0.75. MARS and RF also show large reductions in skill if this term is removed. Surprisingly, the number of environmental covariates does not appear to be a very important predictor of runtime.  Theoretical complexity analyses suggests that learning algorithms are often asymptotically proportional in time to a combination of both training examples and number of features in each training example. However, the leave-one-out cross validation performed here does not suggest that the number of covariates used to fit the SDM has any strong impact on the time to fit it. It is also surprising that GBM-BRT do not show more dependence on the learning rate or tree complexity parameters.  These parameters together are thought to control the number of trees that are built during model fittings, so, theory, should have at least a moderate impact on the execution time.  However, experimental results indicate that less and 1% and about 3% of data variance is all that is explained by these two parameters, respectively. Figure [X] represents the drivers of execution time.

GAM runtime appears to be controlled solely by the number of cells used for prediction. These models converge quite quickly so their fitting time is negligible.  Instead, nearly all of the total time during each experiment was spent on predicting the fit model onto the novel climate scenario.  Without knowing the number of cells to predict onto, the GAM model would have nearly no skill at all.

Importantly, with the exception of RF, the computing variables, RAM and cores, have nearly no influence at all on the execution time of the SDMs. In the first three models, the number of cores accounted for less than 0.5% of the variance.  For all models, memory accounts for less than 3% of all variance.  This lack of dependence on hardware capabilities is surprising.  Without any of the variance in the data being explained in the data by the hardware, the models should run at the same speed on my laptop as on the NCAR yellowstone supercomputer. Random Forests do show a dependence on the number of cores on the computer used to run the model, because their computations are split across multiple cores. In this case, the number of cores accounts for 11% of the RF data's variance.  

The accuracy of SDM models can be most effectively predicted by including the number of environmental covariates and the number of training examples.  The number of environmental covariates included in model fitting is also very important.  In the cases of RF and GBM-BRT, these two together can account for nearly 80% of the model's total predictive skill.  In the GAM model, this is the only significant term towards predicting accuracy.  In all models, no other terms consistently increase model skill. Learning rate and tree complexity, designed to regularize the model to prevent overfitting, do not enhance the skill of the GBM-BRT accuracy model. Figure [X] shows the drivers of model accuracy.

#### Optimization
It is instructive to discuss the prediction of the unconstrained optimal point for each SDM type. In addition, a data-constrain optimization will be demonstrated to show its utility in situation were little data is available to model users.  Finally, a time- constrained optimization was performed and is discussed.  

##### Unconstrained Optimal
In the unconstrained optimization procedure, the algorithm inputs, and therefore the accuracy, were fixed before optimizing the hardware configuration.  Each model presented a different accuracy surface.  In all the models, the accuracy-maximizing point lies at the right-hand size of the training example-covariate space.  However, the individual surface determines the accuracy and the point's exact position. Most points lie close to the very top right of the space, indicating that many training examples and many covariates lead to the highest accuracy, which we would expect from the investigation of the accuracy model drivers as well.  Of course, this space was limited due to experimental design constraints, so if the accuracy-maximizing point lies at the point with the highest data requirements, it may not truly be optimal, but instead require additional data.  More data will nearly always produce a better model.  However, given my review of the SDM literature, in practice the limits I've artificially imposed seem like reasonable limits beyond the data inputs of most SDM studies.  

| Model   | Fixed Accuracy   | Training Examples   | covariates   |
| :------------- | :------------- | :------------- |  :------------- |
| GAM     | 0.7131     | 9000      | 5       |
| GBM-BRT     | 0.8087      | 10000      | 5       |
| MARS     | 0.7722      | 1000      | 5       |
| RF     | 0.8523       | 10000      | 5       |

***Table X*** Accuracy-Maximizing Points for each SDM

The GAM model fixed inputs at the lowest predicted accuracy, with a maximum predicted accuracy of only 0.71.  MARS required the least data, with only 1000 training examples needed to reach the maximum accuracy.  The MARS accuracy surface is interesting in that after ~1000 training examples, in which accuracy increases quite quickly, only the addition of more covariates can increase accuracy.  This trend is also seen, to a lesser degree, in the GAM model. However, in the RF and GBM-BRT models, additional training examples continue to increase model accuracy throughout the surface. In all cases, additional covariates continued to increase accuracy significantly up to the five covariates included in this analysis.

Once an accuracy-maximizing point was developed for each SDM class, we can proceed to identify the optimal hardware configuration for running this set of algorithm inputs. Because of the different levels of influence of hardware components on each model type, the optimal for each class different slightly. Using the Bayesian posterior distribution of the predictions, rather than the posterior means, means that there are multiple configurations that were statistically indistinguishable from each other. These clusters were taken together as the optimal if their dissimilarity of mean and standard deviation was less than 1.

| Configuration Number |	CPUs |	RAM |	Seconds |	Dollars | Mean Distance	| Distance SD |
| :------------- | :------------- | :------------- | :------------- | :------------- | :------------- | | :------------- |
|13	| 2	| 2	| 5.302108 |	0.3187628|	5.330181|	0.4410313	|
|25 |	3	| 2	| 5.302108	| 0.4781441|	5.342165|	0.4420229	|
|37	|4	|2	|5.302108	|0.6375255	|5.358898	|0.4434074	|

***Table X*** GAM unconstrained optimal predictions for computing hardware for achieving the accuracy-maximizing point.


| Configuration Number |	CPUs |	RAM |	Seconds |	Dollars | Mean Distance	| Distance SD |
| :------------- | :------------- | :------------- | :------------- | :------------- | :------------- | | :------------- |
|13|	2|	2	|1489.020|	89.51986|	1796.690	|1545.021|

***Table X*** GBM-BRT unconstrained optimal predictions for computing hardware for achieving the accuracy-maximizing point.


| Configuration Number |	CPUs |	RAM |	Seconds |	Dollars | Mean Distance	| Distance SD |
| :------------- | :------------- | :------------- | :------------- | :------------- | :------------- | | :------------- |
|145|	13|	2|	21.86844|	8.545748|	24.84752|	8.630945|
|157|	14|	2| 21.78000|	9.165896|	25.03071|	8.703743|
|169|	15|	2|	21.71175|	9.789829|	25.19974|	8.699931|
|133|	12|	2|	22.60012|	8.152314|	25.38249|	8.675544|
|121|	11|	2|	22.74969|	7.522412|	25.38255|	9.059256|
|181|	16|	2|	21.73022|	10.451368|	25.52088|	8.824121|
|109|	10|	2|	23.31605|	7.008804|	25.83321|	9.269416|
|193|	17|	2|	21.50065|	10.987265|	25.85966|	9.837260|
|97 |9	|2|	24.22601|	6.554105|	26.64450|	9.645858|

***Table X*** RF unconstrained optimal predictions for computing hardware for achieving the accuracy-maximizing point.


| Configuration Number |	CPUs |	RAM |	Seconds |	Dollars | Mean Distance	| Distance SD |
| :------------- | :------------- | :------------- | :------------- | :------------- | :------------- | | :------------- |
|81	|7|	16|	9.111372|	10.86422|	14.28928|	1.688315|
|93	|8|	16|	9.111372|	12.41625|	15.52028|	1.833760|
|105|	9	|16|	9.111372|	13.96828|	16.80676|	1.985762|
|117|	10|	16|	9.111372|	15.52031|	18.13693|	2.142924|
|129|	11|	16|	9.111372|	17.07234|	19.50185|	2.304192|
|141|	12|	16|	9.111372|	18.62437|	20.89470|	2.468761|
|153|	13|	16|	9.111372|	20.17640|	22.31026|	2.636013|
|165|	14|	16|	9.111372|	21.72844|	23.74446|	2.805468|
|177|	15|	16|	9.111372|	23.28047|	25.19413|	2.976750|
|189|	16|	16|	9.111372|	24.83250|	26.65673|	3.149560|

***Table X*** MARS unconstrained optimal predictions for computing hardware for achieving the accuracy-maximizing point.

Only the GBM-BRT model had a clearly defined optimal solution for this problem, with only one distribution significantly different than the rest.  The GAM class also called for a cluster of only three similar solutions. RF and MARS, however, each had larger clusters, 9 and 10, respectively. The optimal solution for GAM and GBM-BRT each called for very low hardware requirements, with only two cores and two GB memory.  The RF model, as expected because of its ability to run on parallel cores, suggests an optimal with between 7 and 16 cores, though with little memory required.  The MARS model, oddly, requires a high number of cores, even though it runs sequentially, and a high amount of memory.  The other models all suggest that 2GB of RAM is sufficient to run the model optimally. Figure 12 shows the different optimal configurations for the different SDM classes.

#### Data-Constrained Optimization
To demonstrate the ability to predict the optimal configuration under a data-constraint, it is demonstrated here.  This is clearly a very common occurrence in SDM applications, where there is simply no more data with which to fit the model.  In this case, we'll choose a relatively extreme example, and fit an optimal configuration with only 45 training examples.  Covariates are still allowed to range between one and five.  


| Model   | Fixed Accuracy   | Training Examples   | covariates   |
| :------------- | :------------- | :------------- |  :------------- |
| GAM     | 0.6776    | 45      | 5       |
| GBM-BRT     | XXX     | XXX     | XXX       |
| MARS     | 0.69457     | 45      | 5       |
| RF     | 0.7863      | 1      | 5       |

***Table X*** Accuracy-Maximizing Points for each SDM under a strict data constraint of 45 training examples.

Once the accuracy is maximized at this low point, a significant decrease in the maximized accuracy is apparent, and expected.  This decrease was about 0.05-0.1 points on the AUC, which corresponds with a good predictive model with a fair predictive model (FIND CITATION FOR THIS).  As expected, most models require all 45 training points, and all require all five covariates to achieve maximum accuracy. The RF suggestion of accuracy maximization of 1 training example seems like a statistical artifact and should be treated with caution.


| Configuration Number |	CPUs |	RAM |	Seconds |	Dollars | Mean Distance	| Distance SD |
| :------------- | :------------- | :------------- | :------------- | :------------- | :------------- | | :------------- |
| 13|	2|	2|	5.224851|	0.3141180|	5.257580|	0.4889995|
| 25| 3|	2	|5.224851	|0.4711771|	5.269401|	0.4900990|
| 37|	4|	2|	5.224851|	0.6282361	|5.285905	|0.4916341|

***Table X*** Optimal Hardware for GAM under a strict data constraint of 45 training examples.

XXXX NEED GBM-BRT TABLE HERE XXXX

| Configuration Number |	CPUs |	RAM |	Seconds |	Dollars | Mean Distance	| Distance SD |
| :------------- | :------------- | :------------- | :------------- | :------------- | :------------- | | :------------- |
|109	|10|	2	|4.738573|	1.4244151|	5.274748|	2.201935|
|97	|9 |	2|	4.807257|	1.3005552|	5.298139|	2.156391|
|133|	12|	2|	4.677079|	1.6871158|	5.298231|	2.211611	|
|121|	11|	2|	4.732381|	1.5648092|	5.315057|	2.235309	|
|145|	13|	2|	4.683376|	1.8301697|	5.353266|	2.217866	|
|157|	14|	2|	4.679844|	1.9694656|	5.411921|	2.308578	|
|169|	15|	2|	4.885003|	2.2026478	|5.706941|	2.324732	|
|61	|6|	2|	5.679106|	1.0242836|	5.988099|	1.690359	|
|49	|5|	2|	5.949659|	0.8942338|	6.257268|	1.810588	|

***Table X*** Optimal Hardware for RF under a strict data constraint of 45 training examples.


| Configuration Number |	CPUs |	RAM |	Seconds |	Dollars | Mean Distance	| Distance SD |
| :------------- | :------------- | :------------- | :------------- | :------------- | :------------- | | :------------- |
|81|	7|	16|	1.796767|	2.142429|	3.580441|	2.695402|
|93|	8|	16|	1.796767|	2.448490|	3.888889|	2.927606|
|21	|2|	16|	3.274177|	1.115447|	4.152403|	2.109742|
|105|	9	|16|	1.796767|	2.754551|	4.211241|	3.170276|
|117|	10|	16|	1.796767|	3.060613|	4.544539|	3.421187|
|33	|3	|16|	3.380755|	1.727634|	4.577421|	2.314046|
|45|	4|	16|	3.380755|	2.303511|	4.932269|	2.493434|
|57|	5|	16|	3.211710|	2.735413|	5.080262|	2.563458|
|69|	6|	16|	3.211710|	3.282496|	5.530223|	2.790505|

***Table X*** Optimal Hardware for MARS under a strict data constraint of 45 training examples.

The data-constrained optimals showed significant overlap with the unconstrained optimal predictions.  One major difference between the two, however, is the decreased costs, in both time and money, of running the constrained experiment, since less data is used to fit the experiment.

In the case of MARS, we see that number of CPU cores is clearly not a big factor in the optimality of the solution, since everywhere between 2 and 10 CPU cores shows a statistically identical distribution.  Again, we see that the memory requirement is large compared to the otehr SDM classes, however, this is likely still due to sampling design and unlikely to be a real recommendation. GAM suggested the same optimal set of 2, 3, or 4 cores with 2 GB of RAM. This makes sense that the GAM optimal is the same as from the unconstrained.  There is very little hardware influence on the run time of the model, which predicts an unconstrained optimal will use the lowest amount of hardware.  So, any experiment that requires less time than the unconstrained optimal would thus also need only the minimum amount of ahrdware. XXX about GBM-BRT.  RF showed wide variability between the number of cores that would result in the optimal solution as well. It is likely to involve multiple cores, however, there is high uncertainty about the exact number required.  Nevertheless, only two GB of RAM are suggested regardless of the number of CPU cores. This high degree of uncertainty is surprising, since the model shows high predictive skill of the test set, it would appear that the model itself should be able to differentiate between the different number of cores.  

##### Cost-Constrained Optimization
A second type of constraint on the hardware configuration choice is a cost-limit or time-limit imposed by the user. In this case, the user has identified a limit that should not be surpassed, and a solution that falls within it should then be identified.  In this particular example, the user has an unlimited supply of data with which to fit the model. However, the techniques from this section and the section above could be applied together to create a real-world situation in which both cost and and data are limited. In this example, the user has a limit of 22 seconds in which to run the model.  This scenario could occur in a centralized web-based modeling application, in which the application requires latency to be below a certain time, so that user's don't get lost or distracted.  


| Model   | Fixed Accuracy   | Training Examples   | covariates   |
| :------------- | :------------- | :------------- |  :------------- |
| GAM     | 0.71311   | 9000     | 5       |
| GBM-BRT     | 0.7001     | 110     | 5       |
| MARS     | 0.7016    | 130      | 5       |
| RF     | 0.8136     | 1951     | 4       |


####  Discussion
##### Establishing the Accuracy Maximizing Point
Establishing the accuracy-maximizing point under unconstrained optimization yields an interesting perspective on the factors that contribute to SDM accuracy.  Curiously, there are significant differences between models. Without an empirical dataset, it would be logical to assume that all models would become more accuracy with more data.  Given more predictors and more training examples, the model would be able to better characterize the system which they are describing, thus being better able to predict future situations. However, the dataset does not suggest that this is the case for all models.  GBM-BRT and RF require all 10,000 training examples and five covariates to maximize accuracy.  By using all data to maximize accuracy, it appears that these two models require additional training examples and covariates, and that accuracy could be further increased by adding more data.  However, given the experimental limitations of data collection and the realities of SDM modeling, five covariates and 10,000 training examples is likely close to the maximum amount of data that would be used to fit a typical model.  GAM and MARS both require less than the full dataset to maximize accuracy.  Both suggest that all five covariates are important to the maximization, but need less training examples.  When testing against an independent holdout testing set, overfitting is a problem, because the data may be too closely fit to the training dataset.  However, overfitting, while possible, is often associated with complex models, where the number of parameters or terms is high relative to the number of training examples.  It is unusual to overfit a model due to adding additional training examples.  

Figure [X] demonstrates the accuracy surfaces faced by each optimization as a function of the algorithm inputs covariates and training examples.  This figure provides an interesting complementary view to Figure [x] that describes the drivers of model accuracy.  This figure establishes the accuracy substitution rate (ASR) between algorithm inputs that characterizes the amount of one input that can be substituted to get to the accuracy obtained by an increase in the other input.  The rate clearly changes between models and at different points in the training examples-covariates space.  The models all show a rapid increase in accuracy that corresponds to the first ~1000 training examples, after which the number of covariates is the more important term in increase accuracy.  GAM, GBM-BRT, and MARS all show nearly vertical contours of accuracy after approximately 2500 training examples, meaning that additional observations are unlikely to significantly affect model accuracy.  Instead, to get a higher accuracy, more covariates are needed.  GBM-BRT indicates a shift back to a more horizontal contour in the high training examples-high covariates portion of the space, though many thousands of training examples are required to increase accuracy in that way.  RF shows an oblique pattern of accuracy contours throughout, indicating that training examples can be easily substituted for covariates, and visa versa, if sufficient data exists. Like GBM-BRT, a flattening of the accuracy contours indicates that the ASR decreases, so that many thousands of training examples or needed to increase accuracy, though adding more covariates is unlikely to significantly affect accuracy either.   

The ASR is an important quantity for three reasons. First, it provides the model user a guide on how to achieve the maximum accuracy with the given data.  With very little data, there is no reason to expect very high accuracy, but knowing how the accuracy surface lies lets the model user estimate the accuracy before they begin the modeling process.  Second, it is important in helping the model user identify when additional data is no longer necessary.  Take the MARS model, for example. There is no reason to find more than 1000 training examples if sufficient covariates are on hand. The time and effort going to collecting these training examples would essentially be wasted, since they will not go towards increase accuracy.  The researcher could better spend her time finding relevant covariates.  Third, the drivers of accuracy are not the same as the drivers of model execution time. The execution time models respond differently to increased numbers of training examples than to numbers of covariates.  Therefore, knowing the ASR helps optimize the execution time model.  If the same accuracy can be be achieved at many covariates and few training examples, or few covariates and many training examples, if the data is available, it makes more sense in the optimization framework to complete the experiment in the way that minimizes execution time. All of the models respond much more strongly to increase training examples than to increased numbers of covariates. Thus, it make sense to choose the method of obtaining maximum accuracy with the minimum number of training examples. The ASR formalizes the tradeoffs between the two parameters so that this choice can be modeled.

##### Sequential Model Hardware Responses
GAM, GBM-BRT, and MARS are all fit sequentially in the chosen R implementations. The empirical models support the theoretical claim that hardware should minimally affect the execution time of these models.  The optimal cluster of posterior distances either contains a wide range of optimal machine types, or a clear preference for few cores and low memory.  In either case, there is no demonstrated advantage to a more powerful hardware configuration.  The MARS optimal configuration cluster calls for all configurations between 2 and 16 cores.  Statistically, there is no quantitative difference between provisioning and running the model on a computer with two cores and one with 16 cores.  THE GBM-BRT optimal cluster has a clear preference for a single core configuration.  No optimal configurations are suggested with multiple cores.  However, the optimal cluster suggests between 4 and 22 GB of RAM.  Again, because all configurations fall within the same cluster, the same optimal result is expected on both the configuration with one core and 4 GB as the one with one core and 22 GB.  Finally, the GAM SDM shows a small range of core types and a preference for 2 GB of RAM.  In this cluster, all configuration with 2, 3, or 4 cores and 2GB of RAM are statistically similar.  

It is important to recognize the different scale of response times predicted by the optimal cluster in each SDM class.  The GAM algorithms converges within about 5 seconds, incurring only fractions of a cent each time the model is run.  The GAM model is not strongly influenced by either algorithm input or hardware, making in only sensitive to the regional extent of the model run. MARS also terminates quite quickly, typically within 16-18 seconds under the accuracy-maximizing algorithm inputs, which only results in 15-20 cent model runs.  GBM-BRT SDMs however, are much slower to converge, taking over 2500 seconds to fit the accuracy-maximizing inputs. Of course, these models are significantly more expensive due to their increased time, with several configurations in the optimal cluster costing over $5.  While the time and money expended on these runs are significantly more than the GAM and MARS runs, the expected accuracy is significantly higher.  GAMS and MARS both show maximum expected accuracy in the 0.7 range, while GBM-BRT maximum accuracy is closer to 0.85.  This accuracy increase takes the model results from qualitatively 'fair' to 'good'  [@Swets:1988gl; @Araujo:2005ft].  The specific application may dictate whether or not the tradeoff between an increase in accuracy is worth the 10+ fold increase in execution time and cost.

The models also have varying degrees of uncertainty associated with their prediction. The GAM SDM predictions have a mean posterior standard deviation of distance of about 0.5, which is approximately 10% of their total runtime.  Therefore, we are relatively certain in these predictions.  Our certainty diminishes, however, with both MARS and GBM-BRT.  These model predictions have larger uncertainties as a percentage of their posterior mean, both approaching 50%.  The difference between the three model classes in posterior variance is likely due to the fact that GAMs converge so quickly, so all models are relatively homogeneous in run time.  MARS and GBM-BRTs have much larger variance overall in the experimental dataset, so the corresponding models place less confidence in each prediction.  However, the clustering algorithm takes both posterior spread and posterior mean into account when determining the optimal cluster, so these predictions are the ones that have both the lowest mean and lowest posterior standard deviation.

The models do not appear to respond to changes in memory.  The GAM and GBM-BRT optimal clusters require very small amounts of memory.  This is consistent with the relative size of the input datasets to the instances' main memory. MARS, however, suggests an optimal of 16GB of RAM.  This appears to be statistically significant, because there is no intra-cluster variation of memory prediction.  There is no clear reason why the MARS models should require greater memory than the other sequential algorithms.  In addition, these models fail under very large dataset sizes.  When the datasets exceeded several hundred thousand rows (100MB), the algorithms would fail to properly manage in-memory storage and crash. R is known for poor memory management and for making many copies of data objects within built-in and external packages [@Anonymous:W1-t_FiC].  When datasets become large, making in-memory copies is not possible, resulting in program termination.  Some programmers have addressed these problems with packages that can handle datasets too large to fit into an instance's main memory (https://cran.r-project.org/web/views/HighPerformanceComputing.html accessed October 10, 2016). However, popular SDM packages require modification before they can take advantages of many of these implementations.

##### Random Forest Model Hardware Responses
Random Forest SDMs are embarrassingly parallel, and so can easily leverage additional cores in powerful hardware configurations.  The accuracy maximum is on-par with the GBM-BRT SDM runs, but in a fraction of the time.  The RF accuracy-maximizing point is the same as that for GBM-BRT, with 10000 training examples and 5 covariates. However, random forests can be fit in between 10% and 30% of the time to fit a GBM-BRT model.  While the GBM-BRT took between 150 and 550 seconds to converge, the member of the RF optimal cluster all converge between 46 and 49 seconds.  This results in a cost between 13 and 19 cents, closer to the cost of MARS and GAM models. Because the accuracy is so high, the research seeking the truly optimal configuration would rationally be better off choosing the RF model, run in parallel across many cores, rather than waiting for the GBM-BRT to converge sequentially.

The Random Forest optimal cluster shows an interesting trade off between monetary cost and time.  Statistically, there is no euclidean distance difference between the member of the cluster, with a mean distance of 53 and standard deviation of 15.6.  However, because the algorithm can make effective use of the additional hardware, more powerful hardware is accompanied by an decreased execution time but an increased rate.  Conversely, the cheaper machines with less cores take additional time to fit, but have a cheaper rate.  In total, the two really balance each other out, clearly illustrating the tension between hardware cost and time when considering algorithms that can effectively leverage multiple cores. In the unconstrained optimal accuracy-maximizing configuration, everywhere between 9 and 17 cores can be considered optimal.  However, because the floor is so high -- 9 cores -- it is statistically significant that the algorithm requires additional cores to be optimal.  

The random forest, like all algorithms able to be run in parallel, is subject to diminishing speed returns as it is spread across additional cores[@Gustafson:1988dh]. Because the algorithm must run, at least in part, sequentially, when setting up and combining the trees in the ensemble, it is theoretically impossible to obtain an infinite speedup.  Because of the challenge of managing inter-core communication, as more cores are added, it becomes more and more difficult to outweigh the benefits of parallelization with the increased overhead.  Figure [X] shows the diminishing marginal returns that are demonstrated by the RF algorithm when run on between 1 and 25 cores.  The difference between a model run sequentially and one run in parallel but with just two cores is great.  However, the marginal benefit of adding the 16th core is small, in comparison.  This is often measured as parallel efficiency, which is given by the difference between parallel and serial runtimes of the algorithm, divided by the number of cores the parallel implementation was run on. Efficiency gives a measure of diminishing returns of adding additional cores by penalizing the addition of cores, where speedup simply indicates that the decrease in execution time.  The ideal algorithm would show an efficiency of 1, where there are no diminishing marginal returns over all cores.  My dataset shows that efficiencies vary on workload size, but range between 0.05 and 0.4 at 25 cores.  Far from ideal, but it is important to note that the bigger workloads come closer to the ideal efficiency, at least on this range of cores.

The amount of marginal return when adding additional cores is dependent on workload size (Figure [X]).  Small workloads (1000 training examples) experience a great gain in execution time when the first execution time when the first parallel core is added, but then experience a steep linear decrease in marginal decreases in runtime.  Large workloads, over 10,000 training examples, experience a much slower decline in marginal returns.  Because there is more work to do in fitting the, now more complex, model surface, more work can be done spread across cores, before it is combined and returned to the user.  Therefore, these models show a less steep decrease in marginal benefit.  The addition of the 14th, 15th, and 16th cores is still meaningful when fitting a large dataset, when these additions might not be worth in when fitting a smaller dataset.  As more data becomes available with which to fit SDMs, these results suggest that efficiency will increase when fitting parallel algorithms, making it more worth it to invest in fitting models on powerful configurations.

##### Utility of Constrained Optimization
The unconstrained maximization routine to fix algorithm inputs at the accuracy-maximizing point then sequentially optimize hardware to balance time and cost is useful when neither data nor runtime or cost is limited.  However, in many real-world situations, scenarios where neither time, cost, or data is limited are unlikely.  The methods described here can be used alone or in conjunction to meet the realities of modeling with real data. The two presented here demonstrate two likely cases when determining the best configuration given the circumstances.  

A constraint on the number of training examples or covariates available to the research seems probable. My analysis of the SDM literature shows that a significant portion of recent studies fit models with less than 100 training examples. Given the low-friction availability of biodiversity records, both past and contemporary, it is logical to conclude that these studies were fit with such small datasets due to lack of digital data.  A similar analysis shows that many studies also use fewer than five covariates.  Therefore, it is clearly important to be able to determine the maximum accuracy theoretically possible given a constraint on the available data, either training examples or covariates, and optimize the hardware when facing a reduced dataset.

Under a severely constrained dataset, with only 45 training examples, the models showed similar patterns as under the unconstrained optimal.  GBM-BRT shows a wide spread of both cores and RAM requirements in the optimal cluster.  Given the execution time, which is very small when compared to the unconstrained optimal, we are able to conclude that the runtime is just so low that any hardware configuration is just about the when computing it.  Similarly, both GAM and MARS show the same optimal clusters for the data-constrained and unconstrained predictions.  Because hardware has little bearing on the execution time of these models, it makes sense that if the optimal configuration to compute a hard model was on very weak hardware, the optimal to compute a small problem would be the same.  The random forest model shows a requirement for slightly fewer cores (5) than under no constraints, which is consistent when fitting a model with less data.

It can also be useful to place a hard maximum bound on the execution time or cost of the SDM.  If there is a finite amount of money that can be allotted to limit each model run to a fraction of the total available funds, allowing the researcher to complete all modeling experiments without going over budget.  Here, however, I demonstrate the ability to place a constraint in the execution time on the model, and to come up with the optimal algorithm and hardware configuration under that limitation.  This example is best thought of in terms of cloud computing and latency.  While most SDMs are currently computed on desktop and laptop personal computers, it is likely that these models could be efficiently computed on a client-server environment.  If the servers were distributed on a cloud platform, my framework could be used to automatically provision the optimal configuration that maximized the accuracy of the model results returned to the users, while minimizing the time and cost incurred by the server manager.  However, users of interactive web applications are apt to loose interest and turn their attention to other tasks if the application takes too long to respond [@Roth:2013fv].  If the user requests the unconstrained optimal for, say, the GBM-BRT model, they may be waiting over 1 hour for a response.  It would benefit the server manager, then, to limit the time it takes to complete the model so that (a) their costs are minimized and (b) they retain the user's interest and business.  By constraining the optimal with a hard maximum bound on the time it takes to compute the model, the server manager can provision the cheapest resources, return results of high accuracy, and continue to keep their audience engaged.

Another way to optimize configurations that was not explored in this work was 3-way joint optimization between accuracy, cost, and time.  In my method, accuracy is first maximized to yield the accuracy-maximizing combination of algorithm inputs.  Then, hardware is optimized to minimize time and costs.  Some limited use cases may wish to explicitly model the multi-directional trade off between these three parameters.  This optimization problem could easily be accomplished using euclidean distance minimization, as I've discussed, but with the inclusion of a 1-accuracy term in the distance calculation.  Then, the three-way joint minimum would be the one that best balanced the complement of accuracy, time and cost.  I saw few use cases that would find such a technique useful, since scientists are unlikely to want to explicitly compromise accuracy for a decrease in cost.  Instead, it seemed more liekly one would want to set a limit on the maximum allowable time or cost, then find the best accuracy could be obtained out of that. Therefore, the three-way joint optimization was not presented more rigorously, but could easily be implemented using the same dataset.

##### Cloud Computing's Role
Given the relatively minor contributions from hardware to model execution time, the sequential models are not, in general, good candidates to obtain benefits from transitioning to the pay-as-you-go, utility model of cloud computing.  Because GAM, GBM-BRT, and MARS all have an unconstrained optimal cluster of very weak hardware configurations, it does not benefit the user to put these models into a cloud enabled system that can be adapted to very powerful instances.  Rather, the user is best off using the cheapest available instances. If they choose to provision additional cores or RAM, they are likely wasting money that is not returned by decreased execution time.  If the user designs the cloud-based ecosystem correctly, she may be able to run multiple model instances, effectively in parallel, by simultaneously provisioning multiple isolated computing nodes.  Each node could then run independently, fitting the model for a single SDM run.  To make this work automatically, efficiently, and without error would take a significant amount of technical skill. Particularly the centralization of results into a cohesive database would be challenging.  However, it would allow users of sequential models to benefit from working in the cloud.

Random Forests, however, are a good candidate for transitioning into a full cloud platform, running on a single node with multiple CPU cores.  Random forests benefit immensely from parallelization, and therefore require hardware configurations that are more powerful than the most basic requirements of the sequential models.  Furthermore, the requirements of the exact number of cores is dictated by the workload, as demonstrated by the data-constrained optimal configuration, which uses a smaller workload, and results in a less-powerful optimal configuration.  Thus, the ability to easily scale hardware based on problem specific features would be helpful to RF model users.  This ability is provided by cloud computing IaaS providers, where users could put all necessary modeling code and programs (e.g., R) onto a virtual hard disk, and then automatically scale the hardware configuration up or down depending on the difficulty of the problem.

Of course, the difficulty of the transition to a cloud-based solution may outweigh its benefits, particularly for small collections of models.  It is not a trivial task to provision and configure virtual hardware and install and prepare modeling software effectively on a cloud instance.  When only a few models are being run, it is foreseeable that this step would far outweigh any benefit obtained by proper scaling of hardware capabilities.  However, in a large modeling experiment, for researchers experienced with virtual machines, or a server-based SDM enterprise, this benefits of running the models on the optimal balance of cost and execution time may be significantly larger than the configuration time.

Moreover, cloud computing's role becomes more important as workloads and dataset sizes scale up.  As figure [x] shows, RF models with very large input datasets are more efficient to run on many parallel cores than smaller workloads.  Problems with large numbers of training examples are therefore better suited to running on the cloud-based environment, because they can be easily scaled up to many cores, rather than being limited to the number of cores on a purchased physical machine. Biodiversity database size trends suggests that much more data will be available to research to fit SDMs over the next decade.  Therefore, SDMs, when fit with parallel methods, are likely to see increased benefits of running in a cloud-based environment.


##### Future Model Development
Because the training time of most SDMs, excluding random forests, is not driven by hardware capabilities, they are unlikely to be able to cope with the massive influx of data. [@Austin:2007jwa] has posited that a solid foundation of ecological theory is essential to the correct prediction and interpretation of species distribution models. He notes that the ecological underpinnings of the statistics are, perhaps, more important the statistical method itself. [@Elith:2009gj] further suggest that additional improvements in species distribution modeling will come from the incorporation of additional, ecological relevant information in the statistical model itself, and the covariates used to fit it. Indeed, "further advances in SDM are more likely to come from better integration of theory, concepts, and practice than from improved methods per se" [@Elith:2009gj].  

Given the results presented here, modelers should clearly focus their effort not only on incorporating ecological realism into complex statistical models, but on optimizing these models to take advantage of parallelism, high performance libraries, and cloud computing environments.  While ecological datasets may not have been large in the recent past, they can now truly be considered Big Data.  Therefore, new model developments should be undertaken to develop models that more effectively leverage advanced computing infrastructure, including multiple cores and more effective memory management. While incorporation of additional ecological relevant information will help to strengthen the the ecological inference made from the model, incorporation of algorithm design techniques that facilitate the inclusion of Big Data will facilitate the inclusion of information from far more data points. More training data is nearly always associated with a better model fit, suggesting that the ability to better leverage large collections of data will improve the robustness of SDM models and their interpretations.

Once the models are fit, prediction, even for large datasets, is not a particularly large problem, rather, it is the model fitting process that must be differently designed.  My results indicate that prediction takes only a fraction of the fitting time, even when a high spatial resolution is specified, and many thousands (or even millions) of points must be predicted. In the case of additive tree models, this is only a matter of evaluating the predictor set at each splitting point in the internal nodes of the tree, and averaging the predicted response of each tree in the ensemble. So even with very large datasets, this term is relatively small.  The more important term is the fitting time. Random forests are a model that is considered embarrassingly parallel.  Each tree can be grown in isolation, and the only step that must be done sequentially is when the trees are combined into the ensemble.  Conversely, additive regression trees (GBM-BRT) are exceptionally difficult to parallelize, since each tree must be built in sequence, so it can develop a model that reduces residual variance.  The overhead of cross-core communication is prohibitively high if the trees are built in parallel.  While it is difficult, some attempts to build these models in parallel have been successful [@Tyree:2011bx].  This study used very large datasets, and novel implementations of the regression tree approach to parallelize it.

##### Framework Extensibility
Though these results are specific to choice of SDM classes and parameterizations chosen here, the framework could, theoretically, be applied to any class of predictive modeling workflow.  My modeling approach relies on algorithm inputs and hardware capabilities and has no intrinsic relationship to species distribution modeling.  To extend the framework to additional classes and scenarios, it would first be necessary to consider the set of factors that could affect accuracy.  If the model can fairly be characterized in terms of the number of observations and the number of features (covariates) in each training examples, this framework would likely be a good fit. If there are many tuning parameters that are hypothesized to significantly control accuracy, the framework could still be used, but may need some modification to optimize accuracy in more than two dimensions. If, on the other hand, a particular predictive model's accuracy is solely controlled by parameter settings, or the problem domain does not support different training set dimensions, this framework is likely not suitable without heavy modification.

One of the main limitation of utility of the results discussed here is that they will quickly become outdated as the computing landscape changes.  Hardware capabilities continue to improve, yielding faster processors, better cores, and higher performance memory cells.  Furthermore, prices for computing resources continue to decrease, particularly as cloud computing becomes mainstream.  This framework should be relatively robust to both changing hardware capabilities and cost surfaces.  To adapt to more performant hardware, additional data can be collected using similar methods, the execution time models refit, and predictions re-run.  If the clock rate on the processor is the same, this can be done directly.  If not, as is likely, the new data would have to be corrected, or the new data may bias the results.  To incorporate a new pricing scheme, the optimality prediction and clustering could simply be re-run using the new price surface.  The existing data and models would not need to be altered.  The optimization that balances time and cost would be reassessed to determine a new optimal cluster given the updated pricing information.

My results suggest fairly short runtimes and very low costs, for all models.  The longest observations of runtime are only several hours, far short of the several weeks suggested by @Elith:2006vt.  However, as data volumes grow and modeling approaches become more common, even these short runtimes become formidable.  Many studies now model hundred or even thousands of species (e.g., [@Rezende:2015do]). Some large scale projects attempt to model entire biotas in a single exercise. @Candela:2013bl describe a processes for routinely modeling and publishing maps of distributions over 11,000 marine species. Nearly all contemporary SDM studies in the literature use multiple model classes, predict to multiple time periods, and/or fit with several climate model scenarios.  When doing large scale biotic modeling with multiple models to be fit for each taxon, spending several hours for each model is no longer feasible.  Even if models fit in tens of seconds, this may add up to a large time sink and big computing bill.  Therefore, while the individual results presented here may seem trivial, they add up quickly when considering modeling activities typical in contemporary SDM literature.

### Conclusion
In this thesis, I presented a framework for approaching the tradeoff between accuracy, time, and money when considering the provisioning of computing resources for species distribution models.  A theoretical model for assessing the time to reach a user's goal when using a model was developed using a hypothetical user and scenario.  Empirical observations of the run time and accuracy of over 30,000 simulations for four SDM classes were collected under systematically varied inputs, yielding a robust experimental dataset.  The dataset was used to fit a Bayesian regression tree model for time and accuracy, and these models were used to (1) evaluate the drivers of SDM accuracy and execution time and (2) optimize algorithm inputs and hardware capabilities in order to maximize accuracy while jointly minimizing execution time and cost. The unconstrained optimal in the presence of no limit on data or time for all model classes was determined.  Finally, demonstrative examples of optimizing the SDM configuration under a time limit and data limit were worked out to show the ability to find optimal solutions under various constraints.  The posterior distributions of execution time and cost from the Bayesian prediction models were used, in conjunction with a hierarchical clustering method, to determine a set of statistically similar configurations that had low prediction uncertainty.

The models and method show considerable skill in their predictive ability, and can be used to improve researchers allocation of time and money.  My findings suggest that contemporary models are unable to handle large datasets or effectively leverage high performance computing resources, and are likely to struggle under the massive influx of data into global biodiversity databases. Models that can be parallelized show promising ability to balance accuracy and cost while using multiple cores.  Future model development should be directly towards enabling state-of-the-art SDM algorithms to effectively run in the cloud by modifying current implementations to support very large datasets and split workloads across multiple computing cores.
