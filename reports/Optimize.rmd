---
title: "Optimization Routines"
output:
  html_document:
    highlight: tango
    theme: spacelab
    toc: no
  pdf_document:
    toc: yes
---


Load libraries, set constants, define SDM class.
```{r setup}
knitr::opts_chunk$set(cache=F, echo=F, warning=F, error = F, message=F, results = F)
knitr::opts_knit$set(root.dir = "/users/scottsfarley/documents")
setwd("/users/scottsfarley/documents")
library(parallel)
library(doParallel)
library(akima)
library(ggplot2)
options(java.parameters = "-Xmx1500m")
library(bartMachine)
bartMachine::set_bart_machine_num_cores(3)
library(reshape2)
library(ggdendro)
library(plyr)
library(knitr)
library(matrixStats)
library(NbClust)

threshold.time <- 20 ##seconds
threshold.cost <- Inf ##cents
threshold.numTex <- 45

numKeep = 287
dissimilarityCut = 0.5

class = "GBM-BRT"

prices <- read.csv("/users/scottsfarley/documents/thesis-scripts/data/costs2.csv")
```


1. Unconstrained Optimization

Define a set of potential configurations from which to optimize. Only include model parameters, hardware will be optimized separately.
```{r, subset}
trainingExamples <- seq(0, 10000, by=1000)
numPredictors <- seq(1, 5)
cells <- 100000

## make the hypergrid
scenario <- expand.grid(numPredictors = numPredictors,
                        trainingExamples = trainingExamples,
                        cells = cells,
                        ConfigurationNumber = unique(prices$ConfigurationNumber)) ## include config number so we can link to the hardware

scenario <- join(scenario, prices, by = "ConfigurationNumber", type='full')
names(scenario) <- c("numPredictors", "trainingExamples", "cells", "config", "cores", "GBMemory", "RatePerCPU", "RatePerGB", "Rate", "TotalRate")
```

Fit a BART model. Generic for all SDM classes.
```{r fitModel}
if (class == "GBM-BRT"){
  res <- read.csv("thesis-scripts/data/GBM_ALL.csv")
}else if (class == "MARS"){
  res <- read.csv("thesis-scripts/data/mars_full.csv")
}else if (class == "RF"){
  res <- read.csv("thesis-scripts/data/rf_full.csv")
}else if (class == "GAM"){
  res <- read.csv("thesis-scripts/data/gam_full.csv")
}


# ## stratified random sample to make sure all have the same number of replicates
# sp <- split(res, list(res$numPredictors, res$cores, res$GBMemory, res$trainingExamples, res$cells))
# samples <- lapply(sp, function(x) x[sample(1:nrow(x), 3, TRUE),])
# sampled_res <- do.call(rbind, samples)
# sampled_res <- na.omit(sampled_res)

sampled_res = res

predictors <- sampled_res[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
predictors <- data.frame(predictors)

response <- log(sampled_res[[c("totalTime")]]) ## take the log for prediction
rf <- bartMachine(predictors, response, serialize=T, verbose = T, run_in_sample = F)

predictors.acc <- sampled_res[c("numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
predictors.acc <- data.frame(predictors.acc)
response.acc <- sampled_res[[c("testingAUC")]] 

acc.rf <- bartMachine(predictors.acc, response.acc, serialize=T, verbose=T, run_in_sample = F)
```


Predict the values of the configurations in the subset, then interpolate between them for accuracy at all potential configurations.

```{r predict}
scenario.acc <- scenario[c("numPredictors", "trainingExamples", "cells", 
                           "cores", "GBMemory")]
p.acc <- predict(acc.rf, scenario.acc)


scenario.time <- scenario[c("numPredictors", "trainingExamples", "cells", "cores", "GBMemory")]
## split here because my computer is stupid and runs out of heap space
p.time <- predict(rf, scenario.time)
scenario$accuracy <- p.acc
scenario$seconds <- exp(p.time)
scenario$cost <- scenario$seconds * scenario$TotalRate
```

```{r interpUnconstrained}
i.acc <- interp(x = scenario$numPredictors, 
                y = scenario$trainingExamples, 
                z = scenario$accuracy, 
                xo = seq(1, 5),
                yo = seq(1, 10000),
                duplicate=T)

i.acc <- interp2xyz(i.acc, data.frame = T)
```


Find the accuracy-maximizing point.
```{r plotAccMax}

sortedScenario <- i.acc[order(i.acc$y, i.acc$x),]
maxID <- which.max(sortedScenario$z)
theMax <- sortedScenario[maxID, ]
print(paste("Accuracy is maximized at", theMax$y, "training examples and", theMax$x, "predictors."))

theMax.trainingExamples <- theMax$y
theMax.numPredictors <- theMax$x
theMax.expectedAccuracy <- theMax$z
theMaxSet <- scenario[scenario$trainingExamples == theMax.trainingExamples &
                              scenario$numPredictors == theMax.numPredictors, ]


names(i.acc) <- c("numPredictors", "trainingExamples", "accuracy")
## plot the max onto the surface from before
pdf(paste("thesis-scripts/img/AccMax_", class, "_unconstrained.pdf", sep=""))
ggplot(i.acc, aes(numPredictors, trainingExamples, z = accuracy)) +
  geom_tile(aes(fill=accuracy), height=1, width=0.5) +
  stat_contour(binwidth=0.01, col='black') +
  ggtitle(paste(class, "Unconstrained Accuracy Surface")) +
  xlab("Number of Covariates") +
  ylab("Number of Training Examples") +
  scale_fill_continuous(low='pink', high='forestgreen') +
  geom_point(data=theMaxSet, aes(x = numPredictors , y = trainingExamples), shape=25, fill='red', size=3)
dev.off()
```


Calculate the distance from the origin and get the posterior samples.
```{r calcDists}
# origin <- rep(0, length(theMaxSet))
# pointSet <- theMaxSet[c("seconds", "cost")]
# candidates <- rbind(pointSet, origin)
# d <- as.matrix(dist(candidates))
# fromOrigin <- d[,nrow(candidates)]
# fromOrigin <- fromOrigin[fromOrigin > 0]
# fromOrigin <- as.numeric(fromOrigin)
# minDistIdx <- which.min(fromOrigin)
# optimal <- theMaxSet[minDistIdx, ]
# 
# optimalDist <- min(fromOrigin)
# 
# xend = optimal$seconds
# yend = optimal$cost
# fromOrigin <- data.frame(fromOrigin)
# 
theMaxSet.time <- theMaxSet[c("numPredictors", "trainingExamples", "cells", "cores", "GBMemory")]


time.post <- bart_machine_get_posterior(rf, theMaxSet.time)$y_hat_posterior_samples
time.post <- data.frame(time.post)
time.post <- exp(time.post)
time.post$cores <- theMaxSet$cores
time.post$GBMemory <- theMaxSet$GBMemory
time.post$config <- theMaxSet$config
time.post$TotalRate <- theMaxSet$TotalRate

time.post$postMeanTime <- rowMeans(time.post[, 1:1000])
# time.post$postMeanTime <- exp(time.post$postMeanTime)
X <- transform(time.post[, 1:1000], SD=apply(time.post[, 1:1000],1, sd, na.rm = TRUE))
time.post$postSDTime <- X$SD
time.post$postMeanCost <- time.post$postMeanTime * time.post$TotalRate
time.post$postSDCost <- time.post$postSDTime * time.post$TotalRate
```

Plot the distribution of posterior samples on time-cost.
```{r plotDists}
# c.d <- melt(time.post, id.vars = c("cores", "GBMemory", "config", "TotalRate", "postMeanTime", "postMeanCost", "postSDTime", "postSDCost"))
# c.d <- na.omit(c.d)
# c.d$cores <- as.numeric(c.d$cores)
# c.d$GBMemory <- as.numeric(c.d$GBMemory)
# c.d$value <- exp(c.d$value)
# c.d$CostSample <- c.d$value * c.d$TotalRate
# c.d$config <- as.factor(c.d$config)
# pdf(paste("thesis-scripts/img/", class, "_Posteriors_Unconstrained.pdf", sep=""))
# ggplot(c.d) +
#   geom_point(aes(x = CostSample, y = value, col=config), alpha=0.05) +
#   ##scale_color_discrete(guide=F) +
#   geom_point(data=time.post, aes(x = postMeanCost, y = postMeanTime), shape=20, size=2) +
#   ggtitle(paste(class, "Posterior Estimates")) +
#   xlab("Cost") +
#   ylab("Time")
# dev.off()
```

```{r clustCut}
# 
# method = "complete"
# index = "all"
# 
# hardwareVars <- time.post[c("cores", "GBMemory")]
# clusterVars <- time.post[c("postMeanTime", "postMeanCost", "postSDTime")]
# clusterVars$uncertainty <- (time.post$postSDTime / time.post$postMeanTime) * 100
# hardwareVars$uncertainty <- (time.post$postSDTime / time.post$postMeanTime) * 100
# clusterScale <- scale(clusterVars)
# 
# clust <- NbClust(clusterScale, method = method, index=index, max.nc=10, min.nc=3)
# 
# nc = max(clust$Best.partition)
# 
# clusterScale <- data.frame(clusterScale)
# rownames(clusterScale) <- c(1:nrow(clusterScale))
# hc       <- hclust(dist(clusterScale), "ave")           # heirarchal clustering
# dendr    <- dendro_data(hc, type="rectangle") # convert for ggplot
# clust    <- cutree(hc,k=nc)                    # find 2 clusters
# clust.df <- data.frame(label=names(clust), cluster=factor(clust))
# 
# # dendr[["labels"]] has the labels, merge with clust.df based on label column
# dendr[["labels"]] <- merge(dendr[["labels"]],clust.df, by="label")
# # plot the dendrogram; note use of color=cluster in geom_text(...)
# 
# 
# ggplot() + 
#   geom_segment(data=segment(dendr), aes(x=x, y=y, xend=xend, yend=yend)) + 
#   geom_text(data=label(dendr), aes(x, y, label=label, hjust=0, color=cluster), 
#            size=3) +
#   coord_flip() + scale_y_reverse(expand=c(0.2, 0)) + 
#   theme(axis.line.y=element_blank(),
#         axis.ticks.y=element_blank(),
#         axis.text.y=element_blank(),
#         axis.title.y=element_blank(),
#         panel.background=element_rect(fill="white"),
#         panel.grid=element_blank())
# 
# 
# clusterScale$cluster <- clust.df$cluster
# hardwareVars$cluster <- clust.df$cluster
# 
# ggplot(data.frame(clusterScale)) + 
#   geom_point(aes(x = postMeanTime, y = postMeanCost, col=as.numeric(cluster), alpha=uncertainty)) +
#     scale_color_continuous(low = 'lightgreen', high='darkred', name='Suitability') +
#   ggtitle(paste(class, "Clustered"))
# 
# 
# 
# ggplot(hardwareVars) + geom_tile(aes(x = cores, y = GBMemory, fill=as.numeric(cluster), alpha=uncertainty)) +
#     scale_fill_continuous(low = 'lightgreen', high='darkred', name='Suitability') +
#   scale_alpha_continuous(trans="reverse") +
#   ggtitle(paste(class, " Cluster by ", index)) +
#   geom_text(aes(x = cores, y = GBMemory, label=cluster), size=3.5)
# 
# 
# time.post$cluster <- hardwareVars$cluster
write.csv(time.post, paste("thesis-scripts/", class, "srs.csv", sep=""))
```
