

```{r, setup, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
library(akima)
library(devtools)
library(rgbif)
library(earthlife)
library(neotoma)
library(paleobioDB)
library(lubridate)
library(reshape2)
library(ggplot2)
library(gbm)
library(randomForest)
prices <- read.csv("/users/scottsfarley/documents/thesis-scripts/data/costs.csv")
prices <- prices[-which(prices$TotalRate > 10),]
meta <- read.csv("/Users/scottsfarley/documents/thesis-scripts/data/meta_analysis.csv")
```


### Figure 1
<img src="/Users/scottsfarley/documents/thesis-scripts/theThesis/img/Neotoma_ER.jpg"/>

*Figure 1* shows the complex set of relationships between the data in the Neotoma Paleoecological Database. The development of a dedicated database to manage these relationshios implicates that the complexity of the data exceeded the ability of traditional data management techniques.

### Figure 2
#### A.
```{r, fig2, message=F, error=F,warning=F, echo=F}
neotoma_datasets <- get_dataset()


neotoma_sub_dates = vector()
neotoma_sub_names = vector()
neotoma_sub_types = vector()
neotoma_site_names = vector()
neotoma_PIs <- vector()
for (idx in 1:length(neotoma_datasets)){
  thisDS = neotoma_datasets[[idx]]
  subdates = thisDS$submission$submission.date
  if(!is.null(subdates)){
    ##first submission date
    thisDate = subdates[[1]]
    neotoma_sub_dates[idx] = as.character(thisDate)
    thisName = thisDS$dataset.meta$collection.handle
    thisType = thisDS$dataset.meta$dataset.type
    neotoma_sub_names[idx] = thisName
    neotoma_sub_types[idx] = thisType
    neotoma_site_names[idx] = thisDS$site.data$site.name
    PIList <- thisDS$pi.data$ContactName
    for (i in 1:length(PIList)){
      thisPI <- as.character(PIList[i])
      if (length(thisPI > 0)){
        neotoma_PIs[idx] <- thisPI
        print(thisPI)
      }
    }
  }
}


##PLOT NUMBER OF DATASETS
## convert from character to dates and round to the nearest month
df = data.frame(subDate=neotoma_sub_dates, name=neotoma_sub_names, type=neotoma_sub_types, siteNames = neotoma_site_names)
df$subDate <- as.character(df$subDate)
df$subDate <- as.Date(df$subDate)
df$subDate <- round_date(df$subDate, unit='month')

## aggregate by type and date
caster <- dcast(df, formula = type ~ subDate, fun.aggregate=length)

## aggregate over all types
caster[, 1] <- as.character(caster[,1]) ## covert the type names to character (I think they were factor before)
caster$type[nrow(caster)] <- "Neotoma" # bottom row (all records)
caster[nrow(caster),2:ncol(caster)] <- colSums(caster[1:(nrow(caster)-1),2:ncol(caster)]) ## aggregate over all dates, but ignore the first column, which has type names in it


## apply the cumulative sum over all rows and all columns using some nifty indexing
tc <- caster
tc[, 7] <- rowSums(tc[, 2:7])
tc[nrow(tc), 8] <- tc[nrow(tc), 7] + tc[nrow(tc), 8]
tc[,8:ncol(tc)] <- t(apply(tc[,8:ncol(tc)], 1, cumsum))

## get ready to plot
toPlot <- melt(tc[,c(1, 8:ncol(tc))]) ## only aggregate on the rows that contain dates gets tricky because we went from rows to columns and back
toPlot$date <- as.Date(as.character(toPlot$variable))

totalCum <- toPlot[which(toPlot$type == "Neotoma"),]


neotomaplot <- ggplot(totalCum, aes(x = date, y = value, group = type)) +
  geom_line(aes(color = type)) +
  theme_bw() +
  xlab('Date') +
  ylab('Number of Datasets') +
  ggtitle("Neotoma Dataset Submissions") + theme(legend.position="none")
neotomaplot
```
*Figure 2A* shows the steady increase in datasets in the Neotoma Paleoecological Database. 

#### B.
```{r, echo=F, warning=F, error=F, message=F}
##GBIF
gbif_occs = occ_count(type='year')
years = vector()
counts <- vector()
idx = 1
for (i in names(gbif_occs)){
 years[idx] = i
 counts[idx] <- gbif_occs[[idx]]
  idx = idx  + 1
}

gbif_all <- data.frame(value=counts, year=years)

gbif_all$cumsum <- cumsum(gbif_all$value)
gbif_all$year <- as.numeric(as.character(gbif_all$year))

ggplot(gbif_all, aes(x=year, y=cumsum)) + 
  geom_line() +
  xlab("Year") +
  ylab("Number of Occurrences") +
  ggtitle("GBIF Occurrences")
```
*Figure 2A* shows the massive influx of occurrence records in the Global Biodiversity Information Facility. Note that digitization of existing records allows GBIF's holdings to preceed it's organization in 2001.  

### Figure 3
#### A.
```{r, fig3a, ech=F, message=F, warning=F, error=F}
df$type <- as.factor(df$type)
ggplot(df, aes(df$type)) + geom_bar() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Dataset Types in Neotoma") +
  xlab("") + ylab("Count")
```
*Figure 3A* shows the relative proportion of each of the 23 dataset types in the Neotoma Paleoecological Database.


#### B.
```{r, fig3b, echo=F, message=F, warning=F, error=F}
gbif_occ_types <- occ_count(type="basisOfRecord")
gbif_occ_types <- melt(gbif_occ_types, data.frame)
names(gbif_occ_types) <- c("numOfType", "Type")
ggplot(gbif_occ_types, aes(x=Type, y=numOfType)) + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("GBIF Record Types") +
  xlab("") + ylab("Number of Records")
```
*Figure 3B* shows the relative proportion of each of the eight record types in the GBIF dataset.


#### Figure 4
```{r, fig4, echo=F, message=F, warning=F, error=F}

## plot the nuber of articles per year
articles <- read.csv("/users/scottsfarley/documents/thesis-scripts/data/SDM_Trends.csv")
names(articles) <- c("Year", "Articles", "Pct", "Growth")
articles <- articles[-1,] ## rm header
articles$Year <- as.numeric(as.character(articles$Year))
articles$Articles <- as.numeric(as.character(articles$Articles))
articles$Pct <- as.numeric(as.character(articles$Pct))
articles$Growth <- as.numeric(as.character(articles$Growth))
nsf_totalRate <- 2.8
a <- articles[which(articles$Year > 1995),]
a <- a[which(a$Year < 2015),]

z <- vector('numeric', length=18)
z[1] <- 752
for (y in 2:19){
  z[y] <- z[y-1] * 1.07
}

a$NSF <- rev(z)



ggplot(a) +
  geom_line(aes(x=Year, y=Articles, col='SDM Citation Growth')) +
  geom_point(aes(x=Year, y=Articles)) +
  geom_line(aes(x=Year, y=NSF, col='Average Citation Growth')) +
  ylab("Number of SDM Citations") +
  ggtitle("Web of Science Citations") +
  theme(legend.position="right", legend.title=element_blank()) +
  scale_colour_manual(values=c("SDM Citation Growth" = 'blue', "Average Citation Growth" = 'black'))
```
*Figure 4* shows that the recent growth in citations for ecological forecasting models far outpaces the average citation growth in all of STEM fields. SDM citation growth was established from a Web of Science query for ("Ecolgical Niche Model" OR "Species Distribution Model" OR "Habitat Suitability Model") and average citation growth was derived from the National Science Board report on Science and Engineering indicators (2014).


#### Figure 5
```{r, fig5, echo=F, message=F, warning=F, error=F}
mar.default <- c(5,3,3,2) + 0.1
par(oma=c(0,0,2,0))
par(mfrow=c(1, 2), mar = mar.default + c(0, 7, 0, 0))
plot(as.factor(meta$NameFixed),las=2, horiz=T, xlab="Instances", cex.lab=1,
     cex.names=0.67)
par(mar=mar.default)
l <- c("NA", "Model-Driven", "Data-Driven", "Bayesian")
f <- factor(as.character(meta$Tier), labels=l)
plot(f, horiz=T, xlab="Instances")
title("Algorithms Used in SDM Literature", outer=T)

```
*Figure 5* reports the relative proportions of algorithms used in 100 randomly sampled modeling studies.  Instances were classified according to their classification in the data-driven/model-drive/Bayesian framework. In total, 203 model instances were reviewed in 100 papers. 42 unique algorithms were employed.

#### Figure 6
```{r computingCost, echo=F, message=F, warning=F, error=F}
## interp to linear grid
par(mar=c(4, 4, 3, 3))
i <- interp(prices$CPUs,prices$GBsMem,prices$TotalRate, xo=c(1:22), yo=c(1:22))
filled.contour(i, xlab='CPU', ylab='Memory (GB)', main="Computing Hourly Rate",
               col=rev(heat.colors(n=30, alpha=0.7)))

```
*Figure 6* demonstrates the cost surface faced by consumers of Google's Cloud Computing Engine. Rates are in $/hr. Note the tradeoff in relative increases in one of the computing components for the same total rate.


***Figures from this point onwards are not referenced in the thesis yet***

#### Figure 7
```{r gbmPerfModel, echo=F, message=F, warning=F, error=F}
res <- read.csv("thesis-scripts/data/gbm_all.csv")
res <- res[c("totalTime", "cores", "GBMemory", "trainingExamples", "numPredictors", "cells", "treeComplexity", "learningRate")]

gbm.testingInd <- sample(nrow(res), nrow(res) * 0.2)
gbm.testing <- res[gbm.testingInd,]
gbm.training <- res[-gbm.testingInd,]
gbm.training.predictors <- gbm.training[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells', "treeComplexity", "learningRate")]
gbm.training.predictors <- data.frame(gbm.training.predictors)
gbm.training.response <- log(gbm.training[[c("totalTime")]]) ## take the log for prediction
gbm.rf <- randomForest(gbm.training.predictors, gbm.training.response, ntree=100, mtry=3)


## do prediction
gbm.testing.predictors <- gbm.testing[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells', "treeComplexity", "learningRate")]
gbm.testing.predictors <- data.frame(gbm.testing.predictors)
gbm.prediction <- predict(gbm.rf, gbm.testing.predictors)

## get statistics
gbm.mdCor <- cor(gbm.prediction, gbm.testing[['totalTime']])
gbm.mdDelta <- gbm.prediction - gbm.testing$totalTime
gbm.mdDelta.mean <- mean(gbm.mdDelta)
gbm.mdDelta.sd <- sd(gbm.mdDelta)
gbm.mdDelta.RSS <- sum((gbm.prediction - gbm.testing$totalTime)^2)

## Plot
plot(gbm.prediction ~ log(gbm.testing[['totalTime']]), xlab="Observed", ylab="Predicted", main="Observed-Predicted Execution Time")
abline(0, 1)

print(paste("Runtime Model Mean Squared Error: ", gbm.rf$mse[length(gbm.rf$mse)]))
print(paste("Runtime Model Percent Variance Explained: ", gbm.rf$rsq[length(gbm.rf$rsq)], "%"))

```



#### Figure 8
```{res, gbmAccModel, echo=F, message=T, warning=F, error=F}
res <- read.csv("thesis-scripts/data/gbm_all.csv")

gbm.testingInd.acc <- sample(nrow(res), nrow(res) * 0.2)
gbm.testing.acc <- res[gbm.testingInd.acc,]
gbm.training.acc <- res[-gbm.testingInd.acc,]

gbm.training.predictors.acc <- gbm.training.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells',  "learningRate", "treeComplexity")]
gbm.training.predictors.acc <- data.frame(gbm.training.predictors.acc)
gbm.training.response.acc <- gbm.training.acc[[c("testingAUC")]] 

gbm.acc.rf <- randomForest(gbm.training.predictors.acc, gbm.training.response.acc, ntree=100, mtry=3, localImp = T)

## do prediction
gbm.testing.predictors.acc <- gbm.testing.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells',  "learningRate", "treeComplexity")]
gbm.testing.predictors.acc <- data.frame(gbm.testing.predictors.acc)
gbm.prediction.acc <- predict(gbm.acc.rf, gbm.testing.predictors.acc)

## get statistics
gbm.mdCor.acc <- cor(gbm.prediction.acc, gbm.testing.acc[['testingAUC']])
gbm.mdDelta.acc <- gbm.prediction.acc - gbm.testing.acc$testingAUC
gbm.mdDelta.mean.acc <- mean(gbm.mdDelta.acc)
gbm.mdDelta.sd.acc <- sd(gbm.mdDelta.acc)
gbm.mdDelta.RSS.acc <- sum((gbm.prediction.acc - gbm.testing.acc$testingAUC)^2)

## Plot
plot(gbm.prediction.acc ~ gbm.testing.acc[['testingAUC']], xlab="Observed AUC", ylab="Predicted AUC", main="Observed-Predicted AUC")
abline(0, 1)

print(paste("Accuracy Model Mean Squared Error: ", gbm.acc.rf$mse[length(gbm.acc.rf$mse)]))
print(paste("Accuracy Model Percent Variance Explained: ", gbm.acc.rf$rsq[length(gbm.acc.rf$rsq)], "%"))
      
```

#### Figure 9
```{r gbmImp, echo=F, error=F, warning=F}
timingImp <- data.frame(importance(gbm.rf))
timingImp$predictor <- rownames(timingImp)
ggplot(timingImp) + geom_bar(aes(y = IncNodePurity, x=predictor), stat="identity") +
  ggtitle("Variable Importance in GBM Runtime Model")

accImp <- data.frame(importance(gbm.acc.rf))
accImp$predictor <- rownames(accImp)
ggplot(accImp) + geom_bar(aes(y = IncNodePurity, x=predictor), stat="identity") +
  ggtitle("Variable Importance in GBM Accuracy Model")

```

#### Figure 10
```{r gbmPerfModel, echo=F, message=F, warning=F, error=F}
res <- read.csv("thesis-scripts/data/gam_full.csv")
res <- res[c("totalTime", "fittingTime", "cores", "GBMemory", "trainingExamples", "numPredictors", "cells")]

gam.testingInd <- sample(nrow(res), nrow(res) * 0.2)
gam.testing <- res[gam.testingInd,]
gam.training <- res[-gam.testingInd,]
gam.training.predictors <- gam.training[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
gam.training.predictors <- data.frame(gam.training.predictors)
gam.training.response <- log(gam.training[[c("totalTime")]]) ## take the log for prediction
gam.rf <- randomForest(gam.training.predictors, gam.training.response, ntree=100, mtry=3)


## do prediction
gam.testing.predictors <- gam.testing[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
gam.testing.predictors <- data.frame(gam.testing.predictors)
gam.prediction <- predict(gam.rf, gam.testing.predictors)

## get statistics
gam.mdCor <- cor(gam.prediction, gam.testing[['totalTime']])
gam.mdDelta <- gam.prediction - gam.testing$totalTime
gam.mdDelta.mean <- mean(gam.mdDelta)
gam.mdDelta.sd <- sd(gam.mdDelta)
gam.mdDelta.RSS <- sum((gam.prediction - gam.testing$totalTime)^2)

## Plot
plot(gam.prediction ~ log(gam.testing[['totalTime']]), xlab="Observed", ylab="Predicted", main="Observed-Predicted Execution Time")
abline(0, 1)

print(paste("Runtime Model Mean Squared Error: ", gam.rf$mse[length(gam.rf$mse)]))
print(paste("Runtime Model Percent Variance Explained: ", gam.rf$rsq[length(gam.rf$rsq)], "%"))

```



#### Figure 11
```{res, gbmAccModel, echo=F, message=T, warning=F, error=F}
res <- read.csv("thesis-scripts/data/gbm_all.csv")

gam.testingInd.acc <- sample(nrow(res), nrow(res) * 0.2)
gam.testing.acc <- res[gam.testingInd.acc,]
gam.training.acc <- res[-gam.testingInd.acc,]

gam.training.predictors.acc <- gam.training.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
gam.training.predictors.acc <- data.frame(gam.training.predictors.acc)
gam.training.response.acc <- gam.training.acc[[c("testingAUC")]] 

gam.acc.rf <- randomForest(gam.training.predictors.acc, gam.training.response.acc, ntree=100, mtry=3, localImp = T)

## do prediction
gam.testing.predictors.acc <- gam.testing.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
gam.testing.predictors.acc <- data.frame(gam.testing.predictors.acc)
gam.prediction.acc <- predict(gam.acc.rf, gam.testing.predictors.acc)

## get statistics
gam.mdCor.acc <- cor(gam.prediction.acc, gam.testing.acc[['testingAUC']])
gam.mdDelta.acc <- gam.prediction.acc - gam.testing.acc$testingAUC
gam.mdDelta.mean.acc <- mean(gam.mdDelta.acc)
gbm.mdDelta.sd.acc <- sd(gam.mdDelta.acc)
gbm.mdDelta.RSS.acc <- sum((gam.prediction.acc - gam.testing.acc$testingAUC)^2)

## Plot
plot(gam.prediction.acc ~ gam.testing.acc[['testingAUC']], xlab="Observed AUC", ylab="Predicted AUC", main="Observed-Predicted AUC")
abline(0, 1)

print(paste("Accuracy Model Mean Squared Error: ", gam.acc.rf$mse[length(gam.acc.rf$mse)]))
print(paste("Accuracy Model Percent Variance Explained: ", gam.acc.rf$rsq[length(gam.acc.rf$rsq)], "%"))
      
```

#### Figure 12
```{r gbmImp, echo=F, error=F, warning=F}
timingImp.gam <- data.frame(importance(gam.rf))
timingImp.gam$predictor <- rownames(timingImp.gam)
ggplot(timingImp.gam) + geom_bar(aes(y = IncNodePurity, x=predictor), stat="identity") +
  ggtitle("Variable Importance in GAM Runtime Model")

accImp.gam <- data.frame(importance(gam.acc.rf))
accImp.gam$predictor <- rownames(accImp.gam)
ggplot(accImp.gam) + geom_bar(aes(y = IncNodePurity, x=predictor), stat="identity") +
  ggtitle("Variable Importance in GAM Accuracy Model")

```

#### Figure 13
```{r gbmPerfModel, echo=F, message=F, warning=F, error=F}
res <- read.csv("thesis-scripts/data/mars_full.csv")
res <- res[c("totalTime", "fittingTime", "cores", "GBMemory", "trainingExamples", "numPredictors", "cells")]

mars.testingInd <- sample(nrow(res), nrow(res) * 0.2)
mars.testing <- res[mars.testingInd,]
mars.training <- res[-mars.testingInd,]
mars.training.predictors <- mars.training[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
mars.training.predictors <- data.frame(mars.training.predictors)
mars.training.response <- log(mars.training[[c("totalTime")]]) ## take the log for prediction
mars.rf <- randomForest(mars.training.predictors, mars.training.response, ntree=100, mtry=3)


## do prediction
mars.testing.predictors <- mars.testing[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
mars.testing.predictors <- data.frame(mars.testing.predictors)
mars.prediction <- predict(mars.rf, mars.testing.predictors)

## get statistics
mars.mdCor <- cor(mars.prediction, mars.testing[['totalTime']])
mars.mdDelta <- mars.prediction - mars.testing$totalTime
mars.mdDelta.mean <- mean(mars.mdDelta)
mars.mdDelta.sd <- sd(mars.mdDelta)
mars.mdDelta.RSS <- sum((mars.prediction - mars.testing$totalTime)^2)

## Plot
plot(mars.prediction ~ log(mars.testing[['totalTime']]), xlab="Observed", ylab="Predicted", main="Observed-Predicted Execution Time")
abline(0, 1)

print(paste("Runtime Model Mean Squared Error: ", mars.rf$mse[length(mars.rf$mse)]))
print(paste("Runtime Model Percent Variance Explained: ", mars.rf$rsq[length(mars.rf$rsq)], "%"))

```



#### Figure 14
```{res, gbmAccModel, echo=F, message=T, warning=F, error=F}
res <- read.csv("thesis-scripts/data/mars_full.csv")

mars.testingInd.acc <- sample(nrow(res), nrow(res) * 0.2)
mars.testing.acc <- res[mars.testingInd.acc,]
mars.training.acc <- res[-mars.testingInd.acc,]

mars.training.predictors.acc <- mars.training.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
mars.training.predictors.acc <- data.frame(mars.training.predictors.acc)
mars.training.response.acc <- mars.training.acc[[c("testingAUC")]] 

mars.acc.rf <- randomForest(gam.training.predictors.acc, gam.training.response.acc, ntree=100, mtry=3, localImp = T)

## do prediction
mars.testing.predictors.acc <- mars.testing.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
mars.testing.predictors.acc <- data.frame(mars.testing.predictors.acc)
mars.prediction.acc <- predict(mars.acc.rf, mars.testing.predictors.acc)

## get statistics
mars.mdCor.acc <- cor(mars.prediction.acc, mars.testing.acc[['testingAUC']])
mars.mdDelta.acc <- mars.prediction.acc - mars.testing.acc$testingAUC
mars.mdDelta.mean.acc <- mean(mars.mdDelta.acc)
mars.mdDelta.sd.acc <- sd(mars.mdDelta.acc)
mars.mdDelta.RSS.acc <- sum((mars.prediction.acc - mars.testing.acc$testingAUC)^2)

## Plot
plot(mars.prediction.acc ~ mars.testing.acc[['testingAUC']], xlab="Observed AUC", ylab="Predicted AUC", main="Observed-Predicted AUC")
abline(0, 1)

print(paste("Accuracy Model Mean Squared Error: ", mars.acc.rf$mse[length(mars.acc.rf$mse)]))
print(paste("Accuracy Model Percent Variance Explained: ", mars.acc.rf$rsq[length(mars.acc.rf$rsq)], "%"))
      
```

#### Figure 15
```{r gbmImp, echo=F, error=F, warning=F}
timingImp.mars <- data.frame(importance(mars.rf))
timingImp.mars$predictor <- rownames(timingImp.mars)
ggplot(timingImp.mars) + geom_bar(aes(y = IncNodePurity, x=predictor), stat="identity") +
  ggtitle("Variable Importance in MARS Runtime Model")

accImp.mars <- data.frame(importance(mars.acc.rf))
accImp.mars$predictor <- rownames(accImp.mars)
ggplot(accImp.mars) + geom_bar(aes(y = IncNodePurity, x=predictor), stat="identity") +
  ggtitle("Variable Importance in MARS Accuracy Model")

```

#### Figure 16
```{r gbmPerfModel, echo=F, message=F, warning=F, error=F}
res <- read.csv("thesis-scripts/data/rf_full.csv")
res <- res[c("totalTime", "fittingTime", "cores", "GBMemory", "trainingExamples", "numPredictors", "cells", "method")]
# dummy variables for method factor
res$seq<- 0
res$seq[res$method == 'SERIAL'] <- 1
res$par <- 0
res$par[res$method == "PARALLEL"] <- 1
rf.testingInd <- sample(nrow(res), nrow(res) * 0.2)
rf.testing <- res[rf.testingInd,]
rf.training <- res[-rf.testingInd,]
rf.training.predictors <- rf.training[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells', "par", "seq")]
rf.training.predictors <- data.frame(rf.training.predictors)
rf.training.response <- log(rf.training[[c("totalTime")]]) ## take the log for prediction
rf.rf <- randomForest(rf.training.predictors, rf.training.response, ntree=100, mtry=3)


## do prediction
rf.testing.predictors <- rf.testing[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells', "par", "seq")]
rf.testing.predictors <- data.frame(rf.testing.predictors)
rf.prediction <- predict(rf.rf, rf.testing.predictors)

## get statistics
rf.mdCor <- cor(rf.prediction, rf.testing[['totalTime']])
rf.mdDelta <- rf.prediction - rf.testing$totalTime
rf.mdDelta.mean <- mean(rf.mdDelta)
rf.mdDelta.sd <- sd(rf.mdDelta)
rf.mdDelta.RSS <- sum((rf.prediction - rf.testing$totalTime)^2)

## Plot
plot(rf.prediction ~ log(rf.testing[['totalTime']]), xlab="Observed", ylab="Predicted", main="Observed-Predicted Execution Time")
abline(0, 1)

print(paste("Runtime Model Mean Squared Error: ", rf.rf$mse[length(rf.rf$mse)]))
print(paste("Runtime Model Percent Variance Explained: ", rf.rf$rsq[length(rf.rf$rsq)], "%"))

```

#### Figure 17
```{res, gbmAccModel, echo=F, message=T, warning=F, error=F}
res <- read.csv("thesis-scripts/data/rf_full.csv")

rf.testingInd.acc <- sample(nrow(res), nrow(res) * 0.2)
rf.testing.acc <- res[rf.testingInd.acc,]
rf.training.acc <- res[-rf.testingInd.acc,]

rf.training.predictors.acc <- rf.training.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
rf.training.predictors.acc <- data.frame(rf.training.predictors.acc)
rf.training.response.acc <- rf.training.acc[[c("testingAUC")]] 

rf.acc.rf <- randomForest(gam.training.predictors.acc, gam.training.response.acc, ntree=100, mtry=3, localImp = T)

## do prediction
rf.testing.predictors.acc <- rf.testing.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
rf.testing.predictors.acc <- data.frame(rf.testing.predictors.acc)
rf.prediction.acc <- predict(rf.acc.rf, rf.testing.predictors.acc)

## get statistics
rf.mdCor.acc <- cor(rf.prediction.acc, rf.testing.acc[['testingAUC']])
rf.mdDelta.acc <- rf.prediction.acc - rf.testing.acc$testingAUC
rf.mdDelta.mean.acc <- mean(rf.mdDelta.acc)
rf.mdDelta.sd.acc <- sd(rf.mdDelta.acc)
rf.mdDelta.RSS.acc <- sum((rf.prediction.acc - rf.testing.acc$testingAUC)^2)

## Plot
plot(rf.prediction.acc ~ rf.testing.acc[['testingAUC']], xlab="Observed AUC", ylab="Predicted AUC", main="Observed-Predicted AUC")
abline(0, 1)

print(paste("Accuracy Model Mean Squared Error: ", rf.acc.rf$mse[length(rf.acc.rf$mse)]))
print(paste("Accuracy Model Percent Variance Explained: ", rf.acc.rf$rsq[length(rf.acc.rf$rsq)], "%"))
      
```

#### Figure 18
```{r gbmImp, echo=F, error=F, warning=F}
timingImp.rf <- data.frame(importance(rf.rf))
timingImp.rf$predictor <- rownames(timingImp.rf)
ggplot(timingImp.rf) + geom_bar(aes(y = IncNodePurity, x=predictor), stat="identity") +
  ggtitle("Variable Importance in Random Forest Runtime Model")

accImp.rf <- data.frame(importance(rf.acc.rf))
accImp.rf$predictor <- rownames(accImp.rf)
ggplot(accImp.rf) + geom_bar(aes(y = IncNodePurity, x=predictor), stat="identity") +
  ggtitle("Variable Importance in Random Forest Accuracy Model")

```


#### Figure 19
```{r}
res <- read.csv("thesis-scripts/data/rf_full.csv")
library(plyr)
res$grp <- interaction(res$method, res$cores, res$trainingExamples, res$numTrees)
resSum <-ddply(res, .(cores, trainingExamples, numTrees, method), summarize, meanTotalTime = mean(totalTime)) 

resSum$grp <- as.factor(interaction(resSum$trainingExamples, resSum$numTrees, resSum$cores))

resSplit <- split(resSum, resSum$grp)

parResults <- data.frame(cores = vector('numeric', length=length(resSplit)),
                      trainingExamples = vector('numeric', length=length(resSplit)),
                      numTrees = vector('numeric', length=length(resSplit)),
                      speedup = vector('numeric', length=length(resSplit)),
                      efficiency = vector('numeric', length=length(resSplit)))
for (i in 1:length(resSplit)){
  item <- resSplit[[i]]
  par <- item[1,]
  ser <- item[2, ]
  ncores <- par$cores
  Tex <- par$trainingExamples
  nt <- par$numTrees
  speedup <- ser$meanTotalTime / par$meanTotalTime
  eff <- ser$meanTotalTime / par$meanTotalTime /  ncores
  v <- c(ncores, Tex, nt, speedup, eff)
  parResults[i, ] <- v
}


##plot speedup
ggplot(parResults, aes(x = cores, y=speedup, 
                       group=interaction(trainingExamples, numTrees),
                       col = interaction(trainingExamples, numTrees))) + 
  geom_line() + ggtitle("Parallel Speedup of Random Forests")
```
*Figure 19* shows that more expensive workloads benefit more from additional cores than simple modeling routines.

#### Figure 20
```{r}
## and efficiency
ggplot(parResults, aes(x = cores, y=efficiency, 
                       group=interaction(trainingExamples, numTrees),
                       col = interaction(trainingExamples, numTrees))) + 
  geom_line() + ggtitle("Parallel Efficiency of Random Forests")
```
*Figure 20* shows the diminishing marginal returns of using additional cores. Note that simple workflows, though benefiting from additional cores, drop off steeply, while complex workloads decline nearly linearly. 
