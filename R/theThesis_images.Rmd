---
title: "Scott's Thesis Images"
output:
  pdf_document:
    toc: yes
  html_document:
    highlight: tango
    theme: spacelab
    toc: no
---

```{r, setup, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
library(akima)
library(devtools)
library(rgbif)
library(earthlife)
library(neotoma)
library(paleobioDB)
library(lubridate)
library(reshape2)
library(ggplot2)
library(gbm)
library(randomForest)
library(knitr)
library(matrixStats)
options(java.parametesrs = "-Xmx2500m") ## change memory allotment to RJava
library(bartMachine)
bartMachine::set_bart_machine_num_cores(3)
prices <- read.csv("/users/scottsfarley/documents/thesis-scripts/data/costs.csv")
meta <- read.csv("/Users/scottsfarley/documents/thesis-scripts/data/meta_analysis.csv")
opts_knit$set(root.dir = '/users/scottsfarley/documents')
```


### Figure 1
<img src="/Users/scottsfarley/documents/thesis-scripts/theThesis/img/Neotoma_ER.jpg"/>

*Figure 1* shows the complex set of relationships between the data in the Neotoma Paleoecological Database. The development of a dedicated database to manage these relationshios implicates that the complexity of the data exceeded the ability of traditional data management techniques.

### Figure 2
#### A.
```{r, fig2, message=F, error=F,warning=F, echo=F}
neotoma_datasets <- get_dataset()


neotoma_sub_dates = vector()
neotoma_sub_names = vector()
neotoma_sub_types = vector()
neotoma_site_names = vector()
neotoma_PIs <- vector()
for (idx in 1:length(neotoma_datasets)){
  thisDS = neotoma_datasets[[idx]]
  subdates = thisDS$submission$submission.date
  if(!is.null(subdates)){
    ##first submission date
    thisDate = subdates[[1]]
    neotoma_sub_dates[idx] = as.character(thisDate)
    thisName = thisDS$dataset.meta$collection.handle
    thisType = thisDS$dataset.meta$dataset.type
    neotoma_sub_names[idx] = thisName
    neotoma_sub_types[idx] = thisType
    neotoma_site_names[idx] = thisDS$site.data$site.name
    PIList <- thisDS$pi.data$ContactName
    for (i in 1:length(PIList)){
      thisPI <- as.character(PIList[i])
      if (length(thisPI > 0)){
        neotoma_PIs[idx] <- thisPI
      }
    }
  }
}


##PLOT NUMBER OF DATASETS
## convert from character to dates and round to the nearest month
df = data.frame(subDate=neotoma_sub_dates, name=neotoma_sub_names, type=neotoma_sub_types, siteNames = neotoma_site_names)
df$subDate <- as.character(df$subDate)
df$subDate <- as.Date(df$subDate)
df$subDate <- round_date(df$subDate, unit='month')

## aggregate by type and date
caster <- dcast(df, formula = type ~ subDate, fun.aggregate=length)

## aggregate over all types
caster[, 1] <- as.character(caster[,1]) ## covert the type names to character (I think they were factor before)
caster$type[nrow(caster)] <- "Neotoma" # bottom row (all records)
caster[nrow(caster),2:ncol(caster)] <- colSums(caster[1:(nrow(caster)-1),2:ncol(caster)]) ## aggregate over all dates, but ignore the first column, which has type names in it


## apply the cumulative sum over all rows and all columns using some nifty indexing
tc <- caster
tc[, 7] <- rowSums(tc[, 2:7])
tc[nrow(tc), 8] <- tc[nrow(tc), 7] + tc[nrow(tc), 8]
tc[,8:ncol(tc)] <- t(apply(tc[,8:ncol(tc)], 1, cumsum))

## get ready to plot
toPlot <- melt(tc[,c(1, 8:ncol(tc))]) ## only aggregate on the rows that contain dates gets tricky because we went from rows to columns and back
toPlot$date <- as.Date(as.character(toPlot$variable))

totalCum <- toPlot[which(toPlot$type == "Neotoma"),]


neotomaplot <- ggplot(totalCum, aes(x = date, y = value, group = type)) +
  geom_line(aes(color = type)) +
  theme_bw() +
  xlab('Date') +
  ylab('Number of Datasets') +
  ggtitle("Neotoma Dataset Submissions") + theme(legend.position="none")
neotomaplot
```

*Figure 2A* shows the steady increase in datasets in the Neotoma Paleoecological Database. 

#### B.
```{r, echo=F, warning=F, error=F, message=F}
##GBIF
gbif_occs = occ_count(type='year')
years = vector()
counts <- vector()
idx = 1
for (i in names(gbif_occs)){
 years[idx] = i
 counts[idx] <- gbif_occs[[idx]]
  idx = idx  + 1
}

gbif_all <- data.frame(value=counts, year=years)

gbif_all$cumsum <- cumsum(gbif_all$value)
gbif_all$year <- as.numeric(as.character(gbif_all$year))

ggplot(gbif_all, aes(x=year, y=cumsum)) + 
  geom_line() +
  xlab("Year") +
  ylab("Number of Occurrences") +
  ggtitle("GBIF Occurrences")
```

*Figure 2A* shows the massive influx of occurrence records in the Global Biodiversity Information Facility. Note that digitization of existing records allows GBIF's holdings to preceed it's organization in 2001.  

### Figure 3
#### A.
```{r, fig3a, ech=F, message=F, warning=F, error=F}
df$type <- as.factor(df$type)
ggplot(df, aes(df$type)) + geom_bar() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Dataset Types in Neotoma") +
  xlab("") + ylab("Count")
```

*Figure 3A* shows the relative proportion of each of the 23 dataset types in the Neotoma Paleoecological Database.


#### B.
```{r, fig3b, echo=F, message=F, warning=F, error=F}
gbif_occ_types <- occ_count(type="basisOfRecord")
gbif_occ_types <- melt(gbif_occ_types, data.frame)
names(gbif_occ_types) <- c("numOfType", "Type")
ggplot(gbif_occ_types, aes(x=Type, y=numOfType)) + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("GBIF Record Types") +
  xlab("") + ylab("Number of Records")
```

*Figure 3B* shows the relative proportion of each of the eight record types in the GBIF dataset.


#### Figure 4
```{r, fig4, echo=F, message=F, warning=F, error=F}

## plot the nuber of articles per year
articles <- read.csv("/users/scottsfarley/documents/thesis-scripts/data/SDM_Trends.csv")
names(articles) <- c("Year", "Articles", "Pct", "Growth")
articles <- articles[-1,] ## rm header
articles$Year <- as.numeric(as.character(articles$Year))
articles$Articles <- as.numeric(as.character(articles$Articles))
articles$Pct <- as.numeric(as.character(articles$Pct))
articles$Growth <- as.numeric(as.character(articles$Growth))
nsf_totalRate <- 2.8
a <- articles[which(articles$Year > 1995),]
a <- a[which(a$Year < 2015),]

z <- vector('numeric', length=18)
z[1] <- 752
for (y in 2:19){
  z[y] <- z[y-1] * 1.07
}

a$NSF <- rev(z)



ggplot(a) +
  geom_line(aes(x=Year, y=Articles, col='SDM Citation Growth')) +
  geom_point(aes(x=Year, y=Articles)) +
  geom_line(aes(x=Year, y=NSF, col='Average Citation Growth')) +
  ylab("Number of SDM Citations") +
  ggtitle("Web of Science Citations") +
  theme(legend.position="right", legend.title=element_blank()) +
  scale_colour_manual(values=c("SDM Citation Growth" = 'blue', "Average Citation Growth" = 'black'))
```

*Figure 4* shows that the recent growth in citations for ecological forecasting models far outpaces the average citation growth in all of STEM fields. SDM citation growth was established from a Web of Science query for ("Ecolgical Niche Model" OR "Species Distribution Model" OR "Habitat Suitability Model") and average citation growth was derived from the National Science Board report on Science and Engineering indicators (2014).


#### Figure 5
```{r, fig5, echo=F, message=F, warning=F, error=F}
mar.default <- c(5,3,3,2) + 0.1
par(oma=c(0,0,2,0))
par(mfrow=c(1, 2), mar = mar.default + c(0, 7, 0, 0))
plot(as.factor(meta$NameFixed),las=2, horiz=T, xlab="Instances", cex.lab=1,
     cex.names=0.67)
par(mar=mar.default)
l <- c("NA", "Model-Driven", "Data-Driven", "Bayesian")
f <- factor(as.character(meta$Tier), labels=l)
plot(f, horiz=T, xlab="Instances")
title("Algorithms Used in SDM Literature", outer=T)

```

*Figure 5* reports the relative proportions of algorithms used in 100 randomly sampled modeling studies.  Instances were classified according to their classification in the data-driven/model-drive/Bayesian framework. In total, 203 model instances were reviewed in 100 papers. 42 unique algorithms were employed.

#### Figure 6
```{r computingCost, echo=F, message=F, warning=F, error=F}
prices <- read.csv("/users/scottsfarley/documents/thesis-scripts/data/costs.csv")
prices <- na.omit(prices)
par(mar=c(4, 4, 3, 3))
i <- interp(prices$CPUs,prices$GBsMem,prices$TotalRate, xo=c(1:22), yo=c(1:22))
filled.contour(i, xlab='CPU', ylab='Memory (GB)', main="Computing Hourly Rate",
               col=rev(heat.colors(n=30, alpha=0.7)))

```

*Figure 6* demonstrates the cost surface faced by consumers of Google's Cloud Computing Engine. Rates are in $/hr. Note the tradeoff in relative increases in one of the computing components for the same total rate.


***Figures from this point onwards are not referenced in the thesis yet***

#### Figure 7
```{r , echo=F, message=F, warning=F, error=F}
res <- read.csv("thesis-scripts/data/gbm_all.csv")
res <- res[c("totalTime", "cores", "GBMemory", "trainingExamples", "numPredictors", "cells", "treeComplexity", "learningRate")]

gbm.testingInd <- sample(nrow(res), nrow(res) * 0.2)
gbm.testing <- res[gbm.testingInd,]
gbm.training <- res[-gbm.testingInd,]
gbm.training.predictors <- gbm.training[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells', "treeComplexity", "learningRate")]
gbm.training.predictors <- data.frame(gbm.training.predictors)
gbm.training.response <- log(gbm.training[[c("totalTime")]]) ## take the log for prediction
gbm.rf <- bartMachine(gbm.training.predictors, gbm.training.response, serialize = T)


## do prediction
gbm.testing.predictors <- gbm.testing[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells', 
                                         "treeComplexity", "learningRate")]
gbm.testing.predictors <- data.frame(gbm.testing.predictors)
gbm.prediction <- predict(gbm.rf, gbm.testing.predictors)

## get statistics
gbm.mdCor <- cor(gbm.prediction, log(gbm.testing[['totalTime']]))
gbm.mdDelta <- gbm.prediction - log(gbm.testing$totalTime)
gbm.mdDelta.mean <- mean(gbm.mdDelta)
gbm.mdDelta.sd <- sd(gbm.mdDelta)
gbm.mdDelta.RSS <- sum((gbm.mdDelta)^2)
gbm.r2 <- gbm.mdCor^2
gbm.mse <- gbm.mdDelta.RSS / length(gbm.prediction)


## Plot
plot(gbm.prediction ~ log(gbm.testing[['totalTime']]), xlab="Observed", ylab="Predicted", main="Observed-Predicted Execution Time (GBM-BRT)")
abline(0, 1)

print(paste("Runtime Model Mean Squared Error: ", gbm.mdDelta.RSS/length(gbm.prediction)))
print(paste("Runtime Model Percent Variance Explained: ", gbm.r2, "%"))

gbm.post <- bart_machine_get_posterior(gbm.rf, gbm.testing.predictors)
gbm.post <- data.frame(gbm.post$y_hat_posterior_samples)
gbm.post$sd <- apply(gbm.post, 1, sd)

gbm.post.sdMean <- mean(gbm.post$sd)
print(paste("Runtime Model Posterior Mean Standard Deviation: ", gbm.post.sdMean))

```

*Figure 7* shows the model-data comparison for the GBM-BRT model running time as predicted by a random forest ensemble of 100 trees. 


#### Figure 8
```{r, echo=F, message=T, warning=F, error=F}
res <- read.csv("thesis-scripts/data/gbm_all.csv")

gbm.testingInd.acc <- sample(nrow(res), nrow(res) * 0.2)
gbm.testing.acc <- res[gbm.testingInd.acc,]
gbm.training.acc <- res[-gbm.testingInd.acc,]

gbm.training.predictors.acc <- gbm.training.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells',  "learningRate", "treeComplexity")]
gbm.training.predictors.acc <- data.frame(gbm.training.predictors.acc)
gbm.training.response.acc <- gbm.training.acc[[c("testingAUC")]] 

gbm.acc.rf <- bartMachine(gbm.training.predictors.acc, gbm.training.response.acc, serialize=T)

## do prediction
gbm.testing.predictors.acc <- gbm.testing.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells',  "learningRate", "treeComplexity")]
gbm.testing.predictors.acc <- data.frame(gbm.testing.predictors.acc)
gbm.prediction.acc <- predict(gbm.acc.rf, gbm.testing.predictors.acc)

## get statistics
## get statistics
gbm.mdCor.acc <- cor(gbm.prediction.acc, gbm.testing.acc[['testingAUC']])
gbm.mdDelta.acc <- gbm.prediction.acc - gbm.testing.acc[[c("testingAUC")]] 
gbm.mdDelta.mean.acc <- mean(gbm.mdDelta.acc)
gbm.mdDelta.sd.acc <- sd(gbm.mdDelta.acc)
gbm.mdDelta.RSS.acc <- sum((gbm.mdDelta.acc)^2)
gbm.r2.acc <- gbm.mdCor.acc^2
gbm.mse.acc <- gbm.mdDelta.RSS.acc / length(gbm.prediction.acc)

## Plot
plot(gbm.prediction.acc ~ gbm.testing.acc[['testingAUC']], xlab="Observed AUC", 
     ylab="Predicted AUC", main="Observed-Predicted AUC (GBM-BRT)")
abline(0, 1)

print(paste("Accuracy Model Mean Squared Error: ", gbm.mse.acc))
print(paste("Accuracy Model Percent Variance Explained: ", gbm.r2.acc, "%"))

gbm.post.acc <- bart_machine_get_posterior(gbm.acc.rf, gbm.testing.predictors.acc)
gbm.post.acc <- data.frame(gbm.post.acc$y_hat_posterior_samples)
gbm.post.acc$sd <- apply(gbm.post.acc, 1, sd)

gbm.post.sdMean.acc <- mean(gbm.post.acc$sd)
print(paste("Accuracy Model Posterior Mean Standard Deviation: ", gbm.post.sdMean.acc))
      
```

*Figure 8* shows the model-data comparison for the GBM-BRT accuracy as predicted by a random forest ensemble of 100 trees. 


#### Figure 9
```{r , echo=F, error=F, warning=F}
timingImp <- data.frame(investigate_var_importance(gbm.rf))
timingImp$predictor <- rownames(timingImp)
ggplot(timingImp) + geom_bar(aes(y = avg_var_props, x=predictor), stat="identity") +
  ggtitle("Variable Importance in GBM Runtime Model")

accImp <- data.frame(investigate_var_importance(gbm.acc.rf))
accImp$predictor <- rownames(accImp)
ggplot(accImp) + geom_bar(aes(y = avg_var_props, x=predictor), stat="identity") +
  ggtitle("Variable Importance in GBM Accuracy Model")

```

*Figure 9A* shows the relative importance of variables in the model of runtime for the GBM-BRT. *Figure 9B* shows the relative importance of variables in the model of accuracy for the GBM-BRT.

#### Figure 10
```{r , echo=F, message=F, warning=F, error=F}
res <- read.csv("thesis-scripts/data/gam_full.csv")
res <- res[c("totalTime", "fittingTime", "cores", "GBMemory", "trainingExamples", "numPredictors", "cells")]

gam.testingInd <- sample(nrow(res), nrow(res) * 0.2)
gam.testing <- res[gam.testingInd,]
gam.training <- res[-gam.testingInd,]
gam.training.predictors <- gam.training[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
gam.training.predictors <- data.frame(gam.training.predictors)
gam.training.response <- log(gam.training[[c("totalTime")]]) ## take the log for prediction
gam.rf <- bartMachine(gam.training.predictors, gam.training.response, serialize=T)


## do prediction
gam.testing.predictors <- gam.testing[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
gam.testing.predictors <- data.frame(gam.testing.predictors)
gam.prediction <- predict(gam.rf, gam.testing.predictors)

## get statistics
gam.mdCor <- cor(gam.prediction, log(gam.testing[['totalTime']]))
gam.mdDelta <- gam.prediction - log(gam.testing$totalTime)
gam.mdDelta.mean <- mean(gam.mdDelta)
gam.mdDelta.sd <- sd(gam.mdDelta)
gam.mdDelta.RSS <- sum((gam.mdDelta)^2)
gam.r2 <- gam.mdCor ^ 2
gam.mse <- gam.mdDelta.RSS / length(gam.prediction)

## Plot
plot(gam.prediction ~ log(gam.testing[['totalTime']]), xlab="Observed", 
     ylab="Predicted", main="Observed-Predicted Execution Time (GAM)")
abline(0, 1)

print(paste("Runtime Model Mean Squared Error: ",gam.mse))
print(paste("Runtime Model Percent Variance Explained: ", gam.r2, "%"))

gam.post <- bart_machine_get_posterior(gam.rf, gam.testing.predictors)
gam.post <- data.frame(gam.post$y_hat_posterior_samples)
gam.post$sd <- apply(gam.post, 1, sd)

gam.post.sdMean <- mean(gam.post$sd)
print(paste("Runtime Model Posterior Mean Standard Deviation: ", gam.post.sdMean))

```

*Figure 10* shows the model-data comparison for the runtime of the generalized additive model (GAM) SDM. 


#### Figure 11
```{r , echo=F, message=T, warning=F, error=F}
res <- read.csv("thesis-scripts/data/gam_all.csv")

gam.testingInd.acc <- sample(nrow(res), nrow(res) * 0.2)
gam.testing.acc <- res[gam.testingInd.acc,]
gam.training.acc <- res[-gam.testingInd.acc,]

gam.training.predictors.acc <- gam.training.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
gam.training.predictors.acc <- data.frame(gam.training.predictors.acc)
gam.training.response.acc <- gam.training.acc[[c("testingAUC")]] 

gam.acc.rf <- bartMachine(gam.training.predictors.acc, gam.training.response.acc, serialize=T)

## do prediction
gam.testing.predictors.acc <- gam.testing.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
gam.testing.predictors.acc <- data.frame(gam.testing.predictors.acc)
gam.prediction.acc <- predict(gam.acc.rf, gam.testing.predictors.acc)

## get statistics
gam.mdCor.acc <- cor(gam.prediction.acc, gam.testing.acc[['testingAUC']])
gam.mdDelta.acc <- gam.prediction.acc - gam.testing.acc$testingAUC
gam.mdDelta.mean.acc <- mean(gam.mdDelta.acc)
gam.mdDelta.sd.acc <- sd(gam.mdDelta.acc)
gam.mdDelta.RSS.acc <- sum((gam.mdDelta.acc)^2)
gam.acc.mse <- gam.mdDelta.RSS.acc / length(gam.prediction.acc)
gam.acc.r2 <- gam.mdCor.acc ^ 2

## Plot
plot(gam.prediction.acc ~ gam.testing.acc[['testingAUC']], 
     xlab="Observed AUC", ylab="Predicted AUC", main="Observed-Predicted AUC (GAM)")
abline(0, 1)

print(paste("Accuracy Model Mean Squared Error: ", gam.acc.mse))
print(paste("Accuracy Model Percent Variance Explained: ", gam.acc.r2, "%"))

gam.post.acc <- bart_machine_get_posterior(gam.acc.rf, gam.testing.predictors.acc)
gam.post.acc <- data.frame(gam.post.acc$y_hat_posterior_samples)
gam.post.acc$sd <- apply(gam.post.acc, 1, sd)

gam.post.sdMean.acc <- mean(gam.post.acc$sd)
print(paste("Accuracy Model Posterior Mean Standard Deviation: ", gam.post.sdMean.acc))
      
```

*Figure 11* shows the model-data comaprison for the model of GAM accuracy.

#### Figure 12
```{r gbmImp, echo=F, error=F, warning=F}
timingImp.gam <- data.frame(importance(gam.rf))
timingImp.gam$predictor <- rownames(timingImp.gam)
ggplot(timingImp.gam) + geom_bar(aes(y = avg_var_props, x=predictor), stat="identity") +
  ggtitle("Variable Importance in GAM Runtime Model")

accImp.gam <- data.frame(importance(gam.acc.rf))
accImp.gam$predictor <- rownames(accImp.gam)
ggplot(accImp.gam) + geom_bar(aes(y = avg_var_props, x=predictor), stat="identity") +
  ggtitle("Variable Importance in GAM Accuracy Model")

```

*Figure 12A* shows the relative importance of each variable in the GAM runtime model. *Figure 12B* shows the relative importance of each variable in the GAM accuracy model.


#### Figure 13
```{r gbmPerfModel, echo=F, message=F, warning=F, error=F}
res <- read.csv("thesis-scripts/data/mars_full.csv")
res <- res[c("totalTime", "fittingTime", "cores", "GBMemory", "trainingExamples", "numPredictors", "cells")]

mars.testingInd <- sample(nrow(res), nrow(res) * 0.2)
mars.testing <- res[mars.testingInd,]
mars.training <- res[-mars.testingInd,]
mars.training.predictors <- mars.training[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
mars.training.predictors <- data.frame(mars.training.predictors)
mars.training.response <- log(mars.training[[c("totalTime")]]) ## take the log for prediction
mars.rf <- bartMachine(mars.training.predictors, mars.training.response, serialize=T)


## do prediction
mars.testing.predictors <- mars.testing[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
mars.testing.predictors <- data.frame(mars.testing.predictors)
mars.prediction <- predict(mars.rf, mars.testing.predictors)

## get statistics
mars.mdCor <- cor(mars.prediction, log(mars.testing[['totalTime']]))
mars.mdDelta <- mars.prediction - log(mars.testing$totalTime)
mars.mdDelta.mean <- mean(mars.mdDelta)
mars.mdDelta.sd <- sd(mars.mdDelta)
mars.mdDelta.RSS <- sum((mars.mdDelta)^2)
mars.r2 <- mars.mdCor^2
mars.mse <- mars.mdDelta.RSS / length(mars.prediction)

## Plot
plot(mars.prediction ~ log(mars.testing[['totalTime']]), 
     xlab="Observed", ylab="Predicted", main="Observed-Predicted Execution Time (MARS)")
abline(0, 1)

print(paste("Runtime Model Mean Squared Error: ", mars.mse))
print(paste("Runtime Model Percent Variance Explained: ", mars.r2, "%"))


mars.post <- bart_machine_get_posterior(mars.rf, mars.testing.predictors)
mars.post <- data.frame(mars.post$y_hat_posterior_samples)
mars.post$sd <- apply(mars.post, 1, sd)

mars.post.sdMean <- mean(mars.post$sd)
print(paste("Runtime Model Posterior Mean Standard Deviation: ", mars.post.sdMean))


```

*Figure 13* shows the model-data comparison of the runtime model for the multivariate adaptive regression splines (MARS) SDM. 


#### Figure 14
```{r , echo=F, message=T, warning=F, error=F}
res <- read.csv("thesis-scripts/data/mars_full.csv")

mars.testingInd.acc <- sample(nrow(res), nrow(res) * 0.2)
mars.testing.acc <- res[mars.testingInd.acc,]
mars.training.acc <- res[-mars.testingInd.acc,]

mars.training.predictors.acc <- mars.training.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
mars.training.predictors.acc <- data.frame(mars.training.predictors.acc)
mars.training.response.acc <- mars.training.acc[[c("testingAUC")]] 

mars.acc.rf <- bartMachine(mars.training.predictors.acc, mars.training.response.acc, serialize = TRUE)

## do prediction
mars.testing.predictors.acc <- mars.testing.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
mars.testing.predictors.acc <- data.frame(mars.testing.predictors.acc)
mars.prediction.acc <- predict(mars.acc.rf, mars.testing.predictors.acc)

## get statistics
mars.mdCor.acc <- cor(mars.prediction.acc, mars.testing.acc[['testingAUC']])
mars.mdDelta.acc <- mars.prediction.acc - mars.testing.acc$testingAUC
mars.mdDelta.mean.acc <- mean(mars.mdDelta.acc)
mars.mdDelta.sd.acc <- sd(mars.mdDelta.acc)
mars.mdDelta.RSS.acc <- sum((mars.mdDelta.acc)^2)
mars.r2.acc <- mars.mdCor.acc^2
mars.mse.acc <- mars.mdDelta.RSS.acc / length(mars.prediction.acc)


## Plot
plot(mars.prediction.acc ~ mars.testing.acc[['testingAUC']], 
     xlab="Observed AUC", ylab="Predicted AUC", main="Observed-Predicted AUC (MARS)")
abline(0, 1)

print(paste("Accuracy Model Mean Squared Error: ", mars.mse.acc))
print(paste("Accuracy Model Percent Variance Explained: ", mars.r2.acc, "%"))

mars.post.acc <- bart_machine_get_posterior(mars.acc.rf, mars.testing.predictors.acc)
mars.post.acc <- data.frame(mars.post.acc$y_hat_posterior_samples)
mars.post.acc$sd <- apply(mars.post.acc, 1, sd)

mars.post.acc.sdMean <- mean(mars.post.acc$sd)
print(paste("Accuracy Model Posterior Mean Standard Deviation: ", mars.post.acc.sdMean))


```

*Figure 14* shows the model-data comparison for the model of MARS accuracy.


```{r}
additionalName = vector()
mse.acc.imp = vector()
r2.acc.imp = vector()

predNames = vector()
for (i in 1:length(names(mars.training.predictors.acc))){
  predName = names(mars.training.predictors.acc)[i]
  predSet <- mars.training.predictors.acc
  predSet[[predName]] <- NULL
  
  testSet <- mars.testing.predictors
  testSet[[predName]] <- NULL
  
  model <- bartMachine(predSet, mars.training.response.acc)
  
  p <- predict(model, testSet)
  
  pDelta <- p - mars.testing.acc$testingAUC
  
  
  RSS.acc <- sum((mars.mdDelta.acc)^2)
  
  r2 <- cor(p, mars.testing.acc$testingAUC)^2
  
  mse <- sum(RSS.acc) / length(p)
  
  mse.acc.imp[i] <- mse 
  r2.acc.imp[i] <- r2 
  additionalName[i] <- names(mars.training.predictors.acc)[i]
}



imp <- data.frame(additionalName, mse = mse.acc.imp, r2 = r2.acc.imp)
imp$mse = imp$mse
imp$r2 = imp$r2

ggplot(imp) + geom_bar(aes(x = additionalName, y = mse), stat='identity') + geom_abline(slope = 0, intercept=mars.mse.acc)

```

#### Figure 15
```{r , echo=F, error=F, warning=F}
timingImp.mars <- data.frame(investigate_var_importance(mars.rf))
timingImp.mars$predictor <- rownames(timingImp.mars)
ggplot(timingImp.mars) + geom_bar(aes(y = avg_var_props, x=predictor), stat="identity") +
  ggtitle("Variable Importance in MARS Runtime Model")

accImp.mars <- data.frame(investigate_var_importance(mars.acc.rf))
accImp.mars$predictor <- rownames(accImp.mars)
ggplot(accImp.mars) + geom_bar(aes(y = avg_var_props, x=predictor), stat="identity") +
  ggtitle("Variable Importance in MARS Accuracy Model")

```

*Figure 15A* shows the relative importance of variables in the MARS runtime model.
*Figure 15B* shows the relative importance of variables in the MARS accuracy model.

#### Figure 16
```{r , echo=F, message=F, warning=F, error=F}
res <- read.csv("thesis-scripts/data/rf_full.csv")
res <- res[c("totalTime", "fittingTime", "cores", "GBMemory", "trainingExamples", "numPredictors", "cells", "method")]
# dummy variables for method factor
res$seq<- 0
res$seq[res$method == 'SERIAL'] <- 1
res$par <- 0
res$par[res$method == "PARALLEL"] <- 1
rf.testingInd <- sample(nrow(res), nrow(res) * 0.2)
rf.testing <- res[rf.testingInd,]
rf.training <- res[-rf.testingInd,]
rf.training.predictors <- rf.training[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells', "par", "seq")]
rf.training.predictors <- data.frame(rf.training.predictors)
rf.training.response <- log(rf.training[[c("totalTime")]]) ## take the log for prediction
rf.rf <- bartMachine(rf.training.predictors, rf.training.response, serialize=T)


## do prediction
rf.testing.predictors <- rf.testing[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells', "par", "seq")]
rf.testing.predictors <- data.frame(rf.testing.predictors)
rf.prediction <- predict(rf.rf, rf.testing.predictors)

## get statistics
rf.mdCor <- cor(rf.prediction, log(rf.testing[['totalTime']]))
rf.mdDelta <- rf.prediction - log(rf.testing$totalTime)
rf.mdDelta.mean <- mean(rf.mdDelta)
rf.mdDelta.sd <- sd(rf.mdDelta)
rf.mdDelta.RSS <- sum((rf.mdDelta)^2)
rf.mse <- rf.mdDelta.RSS / length(rf.prediction)
rf.r2 <- rf.mdCor ^ 2

## Plot
plot(rf.prediction ~ log(rf.testing[['totalTime']]), 
     xlab="Observed", ylab="Predicted", main="Observed-Predicted Execution Time (RF)")
abline(0, 1)

print(paste("Runtime Model Mean Squared Error: ", rf.mse))
print(paste("Runtime Model Percent Variance Explained: ", rf.r2, "%"))

rf.post <- bart_machine_get_posterior(rf.rf, rf.testing.predictors)
rf.post <- data.frame(rf.post$y_hat_posterior_samples)
rf.post$sd <- apply(rf.post, 1, sd)
rf.post.sdMean <- mean(rf.post$sd)
print(paste("Runtime Model Posterior Mean Standard Deviation: ", rf.post.sdMean))



```

*Figure 16* shows the model-data comparison for the random forest (RF) model of runtime.

#### Figure 17
```{r , echo=F, message=T, warning=F, error=F}
res <- read.csv("thesis-scripts/data/rf_full.csv")

rf.testingInd.acc <- sample(nrow(res), nrow(res) * 0.2)
rf.testing.acc <- res[rf.testingInd.acc,]
rf.training.acc <- res[-rf.testingInd.acc,]

rf.training.predictors.acc <- rf.training.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
rf.training.predictors.acc <- data.frame(rf.training.predictors.acc)
rf.training.response.acc <- rf.training.acc[[c("testingAUC")]] 

rf.acc.rf <- bartMachine(mars.training.predictors.acc, mars.training.response.acc, serialize=T)

## do prediction
rf.testing.predictors.acc <- rf.testing.acc[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells')]
rf.testing.predictors.acc <- data.frame(rf.testing.predictors.acc)
rf.prediction.acc <- predict(rf.acc.rf, rf.testing.predictors.acc)

## get statistics
rf.mdCor.acc <- cor(rf.prediction.acc, rf.testing.acc[['testingAUC']])
rf.mdDelta.acc <- rf.prediction.acc - rf.testing.acc$testingAUC
rf.mdDelta.mean.acc <- mean(rf.mdDelta.acc)
rf.mdDelta.sd.acc <- sd(rf.mdDelta.acc)
rf.mdDelta.RSS.acc <- sum((rf.mdDelta.acc)^2)
rf.r2.acc <- rf.mdCor.acc ^ 2
rf.mse.acc <- rf.mdDelta.RSS.acc / length(rf.prediction.acc)


## Plot
plot(rf.prediction.acc ~ rf.testing.acc[['testingAUC']], 
     xlab="Observed AUC", ylab="Predicted AUC", main="Observed-Predicted AUC (RF)")
abline(0, 1)

print(paste("Accuracy Model Mean Squared Error: ", rf.mse.acc))
print(paste("Accuracy Model Percent Variance Explained: ", rf.r2.acc, "%"))

rf.post.acc <- bart_machine_get_posterior(rf.acc.rf, rf.testing.predictors.acc)
rf.post.acc <- data.frame(rf.post.acc$y_hat_posterior_samples)
rf.post.acc$sd <- apply(rf.post.acc, 1, sd)

rf.post.acc.sdMean <- mean(rf.post.acc$sd)
print(paste("Accuracy Model Posterior Mean Standard Deviation: ", rf.post.acc.sdMean))
      
```

*Figure 17* shows the model-data comparison for the RF model of accruacy.

#### Figure 18
```{r , echo=F, error=F, warning=F}
timingImp.rf <- data.frame(importance(rf.rf))
timingImp.rf$predictor <- rownames(timingImp.rf)
ggplot(timingImp.rf) + geom_bar(aes(y = avg_var_props, x=predictor), stat="identity") +
  ggtitle("Variable Importance in Random Forest Runtime Model")

accImp.rf <- data.frame(importance(rf.acc.rf))
accImp.rf$predictor <- rownames(accImp.rf)
ggplot(accImp.rf) + geom_bar(aes(yw = avg_var_props, x=predictor), stat="identity") +
  ggtitle("Variable Importance in Random Forest Accuracy Model")

```

*Figure 18A* shows the relative importance of each of the variables used in the RF runtime model. *Figure 18B* shows the relative importance of each of the variables used in the RF accuracy model.


#### Figure 19
```{r, echo=F}
res <- read.csv("thesis-scripts/data/rf_full.csv")
library(plyr)
res$grp <- interaction(res$method, res$cores, res$trainingExamples, res$numTrees)
resSum <-ddply(res, .(cores, trainingExamples, numTrees, method), summarize, meanTotalTime = mean(totalTime)) 

resSum$grp <- as.factor(interaction(resSum$trainingExamples, resSum$numTrees, resSum$cores))

resSplit <- split(resSum, resSum$grp)

parResults <- data.frame(cores = vector('numeric', length=length(resSplit)),
                      trainingExamples = vector('numeric', length=length(resSplit)),
                      numTrees = vector('numeric', length=length(resSplit)),
                      speedup = vector('numeric', length=length(resSplit)),
                      efficiency = vector('numeric', length=length(resSplit)))
for (i in 1:length(resSplit)){
  item <- resSplit[[i]]
  par <- item[1,]
  ser <- item[2, ]
  ncores <- par$cores
  Tex <- par$trainingExamples
  nt <- par$numTrees
  speedup <- ser$meanTotalTime / par$meanTotalTime
  eff <- ser$meanTotalTime / par$meanTotalTime /  ncores
  v <- c(ncores, Tex, nt, speedup, eff)
  parResults[i, ] <- v
}


##plot speedup
ggplot(parResults, aes(x = cores, y=speedup, 
                       group=interaction(trainingExamples, numTrees),
                       col = interaction(trainingExamples, numTrees))) + 
  geom_line() + ggtitle("Parallel Speedup of Random Forests")
```

*Figure 19* shows that more expensive workloads benefit more from additional cores than simple modeling routines.

#### Figure 20
```{r, echo=F}
## and efficiency
ggplot(parResults, aes(x = cores, y=efficiency, 
                       group=interaction(trainingExamples, numTrees),
                       col = interaction(trainingExamples, numTrees))) + 
  geom_line() + ggtitle("Parallel Efficiency of Random Forests")
```

*Figure 20* shows the diminishing marginal returns of using additional cores. Note that simple workflows, though benefiting from additional cores, drop off steeply, while complex workloads decline nearly linearly. 

#### Figure 22
```{r multiplotfunction, echo=F}
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```
```{r, echo=F}
library(reshape2)
prices <- read.csv("/users/scottsfarley/documents/thesis-scripts/data/costs.csv")
calcTimeCostIndex <- function(trainingExamples, cells, numPredictors,
                            model, prices, rfTrees = 100, 
                            gbm.learningRate=0.01, 
                            gbm.treeComplexity=5,
                            rf.method = "PARALLEL"){
  
  timeAndCost <- data.frame(
    cores = vector('numeric', length=nrow(prices)),
    GBMemory = vector('numeric', length=nrow(prices)),
    seconds = vector('numeric', length=nrow(prices)),
    cost = vector('numeric', length=nrow(prices)),
    accuracy = vector('numeric', length = nrow(prices)))
  
  for (i in 1:nrow(prices)){
    thisComp <- prices[i,]
    thisComp.cores <- thisComp$CPUs
    thisComp.memory <- thisComp$GBsMem
    scenario <- c(cores = thisComp.cores, GBMemory = thisComp.memory, 
                  trainingExamples=trainingExamples, 
                  numPredictors = numPredictors,
                  cells=cells,
                  method = rf.method
                  )
    scenario <- t(melt(scenario, data.frame))
    scenario <- as.data.frame(scenario)
    thisComp.price <- thisComp$TotalRate## this is rate per hour
    thisComp.pricePerSecond <- thisComp.price / 3600 ## this is rate per second
    thisComp.Conf <- thisComp$ConfigurationNumber
    if (model == 'GBM-BRT'){
      scenario$learningRate <- gbm.learningRate
      scenario$treeComplexity <- gbm.treeComplexity
      logTime <- predict(gbm.rf, scenario)
      acc <- predict(gbm.acc.rf, scenario)
      inverseAcc <- 1 - acc
    }
    if (model == 'RF'){
      ## dummy vars for categorical method
      if (scenario$method == "PARALLEL"){
        scenario$par <- 1
        scenario$seq <- 0
      }else{
        scenario$seq <- 1
        scenario$par <- 0
      }
      logTime <- predict(rf.rf, scenario)
      acc <- predict(rf.acc.rf, scenario)
      inverseAcc <- 1 - acc
    }
    if (model == 'GAM'){
      logTime <- predict(gam.rf, scenario)
      acc <- predict(gam.acc.rf, scenario)
      inverseAcc <- 1 - acc
    }
    if(model == 'MARS'){
      logTime <- predict(mars.rf, scenario)
      acc <- predict(mars.acc.rf, scenario)
      inverseAcc <- 1 - acc
    }
    timePred <- exp(logTime)
    scenarioCost <- timePred * thisComp.pricePerSecond
    v <- c(thisComp.cores, thisComp.memory, timePred, scenarioCost, inverseAcc)
    timeAndCost[i, ] <- v
  }
  return(timeAndCost)
}

estimateOptimal <- function(timeCostMatrix, plot.scene = T){
  r <- timeCostMatrix[c("cost", "seconds", "accuracy")]
  origin <- c(0, 0, 0)
  r <- rbind(r, origin)
  d <- as.matrix(dist(r, "manhattan"))
  fromOrigin <- d[,nrow(r)]
  fromOrigin <- fromOrigin[fromOrigin > 0]
  fromOrigin <- as.numeric(fromOrigin)
  minDistIdx <- which.min(fromOrigin)
  optimal <- timeCostMatrix[minDistIdx, ]
  if (plot.scene){
    xend = optimal$seconds
    yend = optimal$cost
    print(ggplot(timeCostMatrix) +
    geom_segment(aes(x=0, y=0, xend=seconds, yend=cost, alpha=0.1)) +
    geom_point(aes(x = seconds, y=cost, col=cores, size=GBMemory)) +
    xlab("Time Cost [seconds]") +
    ylab("Money Cost [$]") +
    ggtitle(paste("Estimated Costs for Experiment")) + 
    geom_segment(aes(x = 0, y=0, yend=yend, xend=xend), col='red'))
  }
  return(optimal)
}

res.gbm <- read.csv("/Users/scottsfarley/documents/thesis-scripts/data/gbm_all.csv")
res.rf <- read.csv("/Users/scottsfarley/documents/thesis-scripts/data/rf_full.csv")
res.gam <- read.csv("/Users/scottsfarley/documents/thesis-scripts/data/gam_full.csv")
res.mars <- read.csv("/Users/scottsfarley/documents/thesis-scripts/data/mars_full.csv")

nTest <- 1

test.gbm <- res.gbm[sample(nrow(res.gbm), nTest),]
test.gbm$method <- NA
test.rf <- res.rf[sample(nrow(res.rf), nTest),]
test.gam <- res.gam[sample(nrow(res.gam), nTest),]
test.gam$method <- NA
test.mars <- res.mars[sample(nrow(res.mars), nTest),]
test.mars$method <- NA

predictors.gbm <- test.gbm[c("trainingExamples", "cells", "numPredictors", "treeComplexity", "learningRate", "X")]
predictors.gbm$model <- "GBM-BRT"
predictors.gam <- test.gam[c("trainingExamples", "cells", "numPredictors", "X")]
predictors.gam$model <- "GAM"
predictors.rf <- test.rf[c("trainingExamples", "cells", "numPredictors", "method", "X")]
predictors.rf$model <- "RF"
predictors.mars <- test.mars[c("trainingExamples", "cells", "numPredictors", "X")]
predictors.mars$model <- "MARS"

toPredict <- list(predictors.gbm, predictors.gam, predictors.mars, predictors.rf)

timeCosts <- list()
optima <- list()
idx <- 1
for (m in 1:length(toPredict)){
  methodPredictors <- toPredict[[m]]
  for (i in 1:nTest){
    testCase <- methodPredictors[i,]
    print(paste("Model: ", testCase$model, 
                " using n=", trainingExamples=testCase$trainingExamples, 
                "and p =", testCase$numPredictors))
    timeCost <- calcTimeCostIndex(trainingExamples=testCase$trainingExamples, 
                    cells=testCase$cells, 
                    numPredictors = testCase$numPredictors,
                    model = testCase$model, prices=prices, 
                    gbm.learningRate = testCase$learningRate, 
                    gbm.treeComplexity = testCase$treeComplexity,
                    rf.method = testCase$method)
    timeCosts[[idx]] <- timeCost
    optimal <- estimateOptimal(timeCost, plot.scene=T)
    print(paste("Estimated Cost ($): ", optimal$cost))
    print(paste("Estimated Cost (seconds): ", optimal$seconds))
    print(paste("Optimal # Cores: ", optimal$cores))
    print(paste("Optimal RAM: ", optimal$GBMemory))
    optima[[idx]] <- optimal 
    idx = idx + 1
  }
}
```

*Figure 22* demonstrates calculating the optimal computing configuration for five randomly drawn experiments.  I first calculate the predicted execution time and cost under 200+ configuration types, using the Google Cloud Engine cost curves. Then, I plot them in cartesian space, with time cost and dollar cost on orthogonal axes. Then, I find the euclidean distance between each point and the origin (0,0).  Finally, I find the minimum distance between the origin and a point, and call that point the optimal.

#### Figure 21
Move from a machine-learning to a Bayesian approach.  Run a simple linear model, estimating coefficients with MCMC simulations, then predict the execution time for a testing set of n=50.

```{r, echo=F}
library(R2jags)
library(ggplot2)
res <- read.csv("/Users/scottsfarley/documents/thesis-scripts/data/gbm_all.csv")
res$logTime <- log(res$totalTime)
res$grp <- as.numeric(interaction(res$cores, res$GBMemory, res$trainingExamples, res$cells, res$numPredictors))
basicModel <- lm(logTime ~  trainingExamples + numPredictors + cores + GBMemory + cells, data = res)

bayesModel <- function(){
  beta0 ~ dnorm(0.001, 0.01) ## intercept prior
  tau ~ dgamma (0.001, 0.001)      ## model error prior
  beta1 ~ dnorm (0.001, 0.001) # prior for nT
  beta2 ~ dnorm (0.001, 0.001) # prior for nP
  beta3 ~ dnorm (0.001, 0.001) # prior for cores
  beta4 ~ dnorm (0.001, 0.001) # prior for memory
  beta5 ~ dnorm (0.001, 0.001) # prior for cells
  
  for (i in 1:N){
    totalTime[i] ~ dnorm(mu[i], tau) # model error
    mu[i] <- beta0 + beta1*nT[i] + beta2*nP[i] + beta3*cores[i] + beta4*mem[i] + beta5 * cells[i]
  }
  
}

out <- jags(data = list(totalTime = res$logTime,
                        nP = res$numPredictors,
                        nT = res$trainingExamples,
                        cells = res$cells,
                        cores = res$cores,
                        mem = res$GBMemory,
                        N = nrow(res)),
  parameters.to.save = c('beta0', 'beta1', 'beta2', 'beta3', 'beta4', 'beta5', 'tau'), 
  n.chains = 1,
  n.iter = 10000, 
  n.burnin = 1000, 
  n.thin = 10,
  model.file = bayesModel, 
  DIC = FALSE)
out.mcmc <- as.mcmc(out)[[1]]

beta0.mean <- mean(out.mcmc[, 1]) ## intercept
beta1.mean <- mean(out.mcmc[, 2]) ## nT
beta2.mean <- mean(out.mcmc[, 3]) ## nP
beta3.mean <- mean(out.mcmc[, 4]) ## cores 
beta4.mean <- mean(out.mcmc[, 5]) ## memory
beta5.mean <- mean(out.mcmc[, 6]) ## cells
tau.mean <- mean(out.mcmc[, 7])

beta0.sd <- sd(out.mcmc[, 1])
beta1.sd <- sd(out.mcmc[, 2])
beta2.sd <- sd(out.mcmc[, 3])
beta3.sd <- sd(out.mcmc[, 4])
beta4.sd <- sd(out.mcmc[, 5])
beta5.sd <- sd(out.mcmc[, 6])
tau.sd <- sd(out.mcmc[, 7])


beta0.lm <- basicModel$coefficients['(Intercept)']
beta1.lm <- basicModel$coefficients['trainingExamples']
beta2.lm <- basicModel$coefficients['numPredictors']
beta3.lm <- basicModel$coefficients['cores']
beta4.lm <- basicModel$coefficients['GBMemory']
beta5.lm <- basicModel$coefficients['cells']

lmCoeffs <- c(beta0.lm, beta1.lm, beta2.lm, beta3.lm, beta4.lm, beta5.lm, NA)
bayesCoeffs <- c(beta0.mean, beta1.mean, beta2.mean, beta3.mean, beta4.mean, beta5.mean, tau.mean)
bayesMin <- c(beta0.mean - beta0.sd, beta1.mean - beta1.sd, 
              beta2.mean - beta2.sd, beta3.mean - beta3.sd, 
              beta4.mean - beta4.sd, beta5.mean - beta5.sd, 
              tau.mean - tau.sd)

bayesMax <- c(beta0.mean + beta0.sd, beta1.mean + beta1.sd, 
              beta2.mean + beta2.sd, beta3.mean + beta3.sd, 
              beta4.mean + beta4.sd, beta5.mean + beta5.sd, 
              tau.mean + tau.sd)

coeffName <- c("beta0", "beta1", "beta2", "beta3", "beta4", "beta5", "tau")

basicMat <- data.frame(lmCoeffs, bayesCoeffs, bayesMax, bayesMin, coeffName)

##plot the coefficients
ggplot(basicMat, aes(x = coeffName)) + geom_point(aes(y = bayesCoeffs, col='Bayes')) + 
  geom_errorbar(aes(ymin = bayesMin, ymax = bayesMax, col='Bayes')) + 
  geom_point(aes(y = lmCoeffs, col='LM')) +
  ggtitle("Linear Model Coefficients") +
  xlab("Coefficient Name") +
  ylab("Coefficient Value")

test <- res[sample(nrow(res), 50, replace=F),]

simplePrediction <- function(){
  beta0 ~ dnorm(beta0.mean, 1/(beta0.sd)^2) ## intercept prior
  tau ~ dnorm (tau.mean, 1/(tau.sd)^2)      ## model error prior
  beta1 ~ dnorm(beta1.mean, 1/(beta1.sd)^2) # prior for nT
  beta2 ~ dnorm(beta2.mean, 1/(beta2.sd)^2) # prior for nP
  beta3 ~ dnorm(beta3.mean, 1/(beta3.sd)^2) # prior for cores
  beta4 ~ dnorm(beta4.mean, 1/(beta4.sd)^2) # prior for memory
  beta5 ~ dnorm(beta5.mean, 1/(beta5.sd)^2) # prior for cells
  
  for (i in 1:N){
    totalTime[i] ~ dnorm(mu[i], tau) # model error
    mu[i] <- beta0 + beta1*nT[i] + beta2*nP[i] + beta3*cores[i] + beta4*mem[i] + beta5 * cells[i]
  }
}

simplePredictionMCMC <- jags(data = list(
                        nP = test$numPredictors,
                        nT = test$trainingExamples,
                        cells =test$cells,
                        cores = test$cores,
                        mem = test$GBMemory,
                        N = nrow(test),
                        beta0.mean = beta0.mean, beta0.sd = beta0.sd,
                        beta1.mean = beta1.mean, beta1.sd = beta1.sd,
                        beta2.mean = beta2.mean, beta2.sd= beta2.sd,
                        beta3.mean = beta3.mean, beta3.sd = beta3.sd,
                        beta4.mean = beta4.mean, beta4.sd = beta4.sd,
                        beta5.mean = beta5.mean, beta5.sd = beta5.sd,
                        tau.mean = tau.mean, tau.sd = tau.sd),
  parameters.to.save = c('totalTime'), 
  n.chains = 3,
  n.iter = 10000, 
  model.file = simplePrediction, 
  DIC = FALSE)
preds <- data.frame(simplePredictionMCMC$BUGSoutput$summary)

lmPred <- predict(basicModel, test)

predMat <- data.frame(obs = test$logTime, bayesPred = preds$mean, 
                      bayesMin = preds$X2.5., bayesMax = preds$X97.5.)

ggplot(predMat, aes(x = obs)) +
  xlab("Observed [log-seconds]") +
  ylab("Predicted [log-seconds]") +
  geom_errorbar(aes(ymax = bayesMax, ymin = bayesMin, col='bayes')) + 
  geom_point(aes(y = lmPred, col='LM'))  +
  geom_abline(slope=1, intercept=0, col='black')   +
  geom_point(aes(y = bayesPred, x=obs), col='red') + 
  ggtitle("Bayesian Fitting GBM-BRT Model-Data Comparison")


ggplot(predMat, aes(x = lmPred, y = bayesPred)) + 
  geom_errorbar(aes(ymin = bayesMin, ymax = bayesMax, col='Uncertainty'))+
  geom_point(aes(col='Point')) + 
  geom_abline(slope=1,intercept=0) + 
  xlab("OLS [log-seconds]") +
  ylab("Bayesian [log-seconds]") +
  ggtitle("Bayesian Estimate vs. OLS Fit")
  

r <- cor(predMat$obs, predMat$bayesPred)
r2 <- r*r
print(paste("Model Explains ", r2, "% of data variance."))

```

*Figure 21* shows the results of fitting a model using draws from an MCMC instead of OLS.  Notice the close agreement between OLS and posterior means.  Also, we now have uncertainty estimates on all of our results.


### Figure 23
```{r, echo=F, message=F}
library(R2jags)
library(bartMachine)
res <- read.csv("/users/scottsfarley/documents/thesis-scripts/data/gbm_all.csv")

res <- res[c("totalTime", "cores", "GBMemory", "trainingExamples", "numPredictors", "cells", "treeComplexity", "learningRate")]

gbm.testingInd <- sample(nrow(res), 100)
gbm.testing <- res[gbm.testingInd,]
gbm.training <- res[-gbm.testingInd,]
gbm.training.predictors <- gbm.training[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells', "treeComplexity", "learningRate")]
gbm.training.predictors <- data.frame(gbm.training.predictors)
gbm.training.response <- log(gbm.training[[c("totalTime")]]) ## take the log for prediction


gbm.testing.predictors <- gbm.testing[c( "numPredictors", "cores", "GBMemory", "trainingExamples", 'cells', 
                                         "treeComplexity", "learningRate")]
gbm.testing.predictors <- data.frame(gbm.testing.predictors)

gbm.model <- bartMachine(X = gbm.training.predictors, y = gbm.training.response, serialize = T)

```


```{r, echo=F, message=F}



costs.mean <- vector()
times.mean <- vector()
CPUS <- vector()
Mem <- vector()

cost.dist <- list()
time.dist <- list()

long <- res[res$trainingExamples > 1000,]
testCase <- long[sample(nrow(long), 1),]

for (i in 1:50){
    thisComp <- prices[i,]
    thisComp.cores <- thisComp$CPUs
    thisComp.memory <- thisComp$GBsMem
    scenario <- c(trainingExamples=testCase$trainingExamples,
                  numPredictors = testCase$numPredictors,
                  cells=testCase$cells,
                  learningRate = testCase$learningRate,
                  treeComplexity = testCase$treeComplexity,
                  cores = thisComp.cores, 
                  GBMemory = thisComp.memory)
    scenario <- t(melt(scenario, data.frame))
    scenario <- as.data.frame(scenario)
    p <- predict(gbm.model,  scenario)
    time <- exp(p)
    times.mean[[i]] <- time
    thisComp.price <- thisComp$TotalRate## this is rate per hour
    thisComp.pricePerSecond <- thisComp.price / 3600 ## this is rate per second
    cost <- time*thisComp.pricePerSecond
    costs.mean[[i]] <- cost
    timeDist <- data.frame(t(bart_machine_get_posterior(gbm.model, scenario)$y_hat_posterior_samples))
    names(timeDist) <- c("value")
    timeDist$value <- exp(timeDist$value)
    timeMean <- mean(timeDist$value)
    timeSD <- sd(timeDist$value)
    costDist <- timeDist$value * thisComp.pricePerSecond
    costMean <- mean(costDist)
    costSD <- sd(costDist)
    time.dist[[i]] <- timeDist$value
    cost.dist[[i]] <- costDist
}

```

```{r, echo=F}
p <- melt(time.dist)

p.test <- p
p.test$L1 <- as.factor(p.test$L1)
ggplot(p.test) + 
  geom_density(aes(x=value, group=L1, col=L1), alpha=0.2) +
  scale_fill_brewer('Dark2', guide=FALSE) + guides(colour=FALSE, L1=F) +
  ggtitle(paste("Posterior Density of Execution Time of GBM-BRT SDM #", 1)) +
  xlab("Log Seconds") +
  ylab("Posterior Density")
  
```

```{r, echo=F}
p <- melt(cost.dist)
p <- na.omit(p)
p.test <- p
p.test$L1 <- as.factor(p.test$L1)
ggplot(p.test) + 
  geom_density(aes(x=value, group=L1, col=L1), alpha=0.2) +
  scale_fill_brewer('Dark2', guide=FALSE) + guides(colour=FALSE, L1=F) +
  ggtitle(paste("Posterior Density of Cost of GBM-BRT SDM #", 1)) +
  xlab("Dollars ($)") +
  ylab("Posterior Density") + xlim(0, 0.2)

```

```{r}
c.d <- melt(cost.dist)
c.d <- na.omit(c.d)
c.d$L1 <- as.factor(c.d$L1)
t.d <- melt(time.dist)
t.d <- na.omit(t.d)
t.d$L1 <- as.factor(t.d$L1)
pots <- data.frame(time = t.d, cost = c.d)
ggplot(pots) + 
  # geom_point(aes(x = cost.value, y = time.value, group=cost.L1, col=cost.L1), alpha=0.25) +
  scale_fill_gradient() +
  guides(col=F, group=F) +
  stat_density2d(aes(y = time.value, x = cost.value, group=cost.L1, col=cost.L1, fill=cost.L1), 
                 geom="polygon", bins=5) +
  xlab("Dollar Cost ($)") + 
  ylab("Time Cost (seconds)") +
  scale_fill_discrete() +
  ggtitle("Posterior Distributions of Time-Cost")
```

```{r}
# c.d <- melt(cost.dist)
# c.d <- na.omit(c.d)
# c.d$L1 <- as.factor(c.d$L1)
# t.d <- melt(time.dist)
# t.d <- na.omit(t.d)
# t.d$L1 <- as.factor(t.d$L1)
# pots <- data.frame(time = t.d, cost = c.d)
# ggplot(pots) + 
#   # geom_point(aes(x = cost.value, y = time.value, group=cost.L1, col=cost.L1), alpha=0.25) +
#   geom_density(aes(y = time.value, group=cost.L1, col=cost.L1)) +
#   geom_density(aes(x = cost.value, group=cost.L1, col-cost.L1)) + 
#   scale_x_continuous()
```

```{r, echo=F}
cost.sd <- sapply(cost.dist, function(x){
  return(sd(x))
})
time.sd <- sapply(cost.dist, function(x){
  return(sd(x))
})
cost.mean <- sapply(cost.dist, function(x){
  return(mean(x))
})
time.mean <- sapply(cost.dist, function(x){
  return(mean(x))
})


timeCostMatrix = data.frame(time = time.mean, cost =cost.mean,
                            timeSD = time.sd, cost = cost.sd)

compDists <- list()
for (i in 1:length(time.dist)){
  compDist <- vector()
  for (j in 1:1000){
    t.i <- time.dist[[i]][[j]]
    c.i <- cost.dist[[i]][[j]]
    pt <- c(t.i, c.i)
    orig <- c(0, 0)
    m <- rbind(pt, orig)
    d <- dist(m)
    compDist[j] <- d[[1]]
  }
  compDists[[i]] <- compDist
}
allDistances <- melt(compDists)

p <- na.omit(allDistances)
p$value <- as.numeric(p$value)
p$L1 <- as.factor(p$L1)
ggplot(p) + 
  geom_density(aes(x=value, group=L1, col=L1), alpha=0.2) +
  scale_fill_brewer('Dark2', guide=FALSE) + guides(colour=FALSE, L1=F) +
  xlab("Euclidiean Distance From Origin") +
  ggtitle("Density of Euclidean Distance From Origin") +
  geom_rug(aes(x =value , group=L1, col=L1))
```

*Figure 23* is something I'm still in the middle of working through, where I use a bayesian framework to fit the execution time model, then propagate the uncertainty through the optimal prediction process. First, I fit an additive tree model (a la GBM), but using the Bayesian framework, with priors. I used the ```bartMachine``` package in R to do this.  I computed a 1000 iteration MCMC, where each iteration was a full tree model.  Then using the posterior draws, I calculated the uncertainty on the execution time prediction.  Then, for each time, I calculated the corresponding dollar cost.  Then, I calculate the euclidean distance between all posterior draws of execution time/dollar cost and the origin for all possible computing configurations.  As before, I then take the minimum euclidean distance, but this time, with a range of possible solutions.


```{r}
learningRateOpts <- seq(0.001, 0.1, by=0.1)
treeComplexityOpts <- seq(1, 5)
nTexOpts <- seq(10, 25000, by=50)
cellOpts <- seq(10000, 1000000, by=100000)
nPOpts <- seq(1, 5)
prices <- read.csv("/users/scottsfarley/documents/thesis-scripts/data/costs.csv")

n = length(learningRateOpts) * 
  length(treeComplexityOpts) *
  length(nTexOpts) *
  length(cellOpts) *
  length(nPOpts) *
  nrow(prices)

timeAndCost <- data.frame(
  cores = vector('numeric', length=n),
  GBMemory = vector('numeric', length=n),
  seconds = vector('numeric', length=n),
  cost = vector('numeric', length=n),
  accuracy = vector('numeric', length = n))

print(paste("N is ", n))

for (lr in learningRateOpts){
  for (tc in treeComplexityOpts){
    for(ntex in nTexOpts){
      for (cell in cellOpts){
        for(np in nPOpts){
          for (cID in 1:nrow(prices)){
            thisComp <- prices[cID,]
            thisComp.cores <- thisComp$CPUs
            thisComp.memory <- thisComp$GBsMem
            scenario <- c(cores = thisComp.cores, 
                          GBMemory = thisComp.memory, 
                          trainingExamples=ntex, 
                          numPredictors = np,
                          cells=cell,
                          learningRate = lr,
                          treeComplexity = tc
                        )
            scenario <- t(melt(scenario, data.frame))
            scenario <- as.data.frame(scenario)
            p <- predict(gbm.model,  scenario)
            time <- exp(p)
            acc <- predict(gbm.acc.rf, scenario)
            timePred <- exp(logTime)
            scenarioCost <- timePred * thisComp.pricePerSecond
            v <- c(thisComp.cores, thisComp.memory, timePred, scenarioCost, acc)
            timeAndCost[i, ] <- v
          }
        }
      }
    }
  }
}

```
